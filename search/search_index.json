{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Argo Events What is Argo Events? Argo Events is an event-based dependency manager for Kubernetes which helps you define multiple dependencies from a variety of event sources like webhook, s3, schedules, streams etc. and trigger Kubernetes objects after successful event dependencies resolution. Features Manage dependencies from 20+ event sources. Ability to customize business-level constraint logic for event dependencies resolution. Manage everything from simple, linear, real-time dependencies to complex, multi-source, batch job dependencies. Supports AWS Lambda and OpenFaas as triggers. Supports integration of existing API servers with 20+ event sources. CloudEvents compliant. Event Sources AMQP AWS SNS AWS SQS Cron Schedules GCP PubSub GitHub GitLab HDFS File Based Events Kafka Minio NATS MQTT K8s Resources Slack NetApp StorageGrid Webhooks Stripe NSQ Emitter Redis Azure Events Hub Installation Follow the setup to install Argo Events. Quick Start Check out the quick start guide to trigger Argo workflows on webhook events. Deep Dive Explore the tutorial to dive deep into Argo Events features.","title":"Overview"},{"location":"#argo-events","text":"","title":"Argo Events"},{"location":"#what-is-argo-events","text":"Argo Events is an event-based dependency manager for Kubernetes which helps you define multiple dependencies from a variety of event sources like webhook, s3, schedules, streams etc. and trigger Kubernetes objects after successful event dependencies resolution.","title":"What is Argo Events?"},{"location":"#features","text":"Manage dependencies from 20+ event sources. Ability to customize business-level constraint logic for event dependencies resolution. Manage everything from simple, linear, real-time dependencies to complex, multi-source, batch job dependencies. Supports AWS Lambda and OpenFaas as triggers. Supports integration of existing API servers with 20+ event sources. CloudEvents compliant.","title":"Features"},{"location":"#event-sources","text":"AMQP AWS SNS AWS SQS Cron Schedules GCP PubSub GitHub GitLab HDFS File Based Events Kafka Minio NATS MQTT K8s Resources Slack NetApp StorageGrid Webhooks Stripe NSQ Emitter Redis Azure Events Hub","title":"Event Sources"},{"location":"#installation","text":"Follow the setup to install Argo Events.","title":"Installation"},{"location":"#quick-start","text":"Check out the quick start guide to trigger Argo workflows on webhook events.","title":"Quick Start"},{"location":"#deep-dive","text":"Explore the tutorial to dive deep into Argo Events features.","title":"Deep Dive"},{"location":"FAQ/","text":"FAQs Q. How to get started with Argo Events? **A Recommended way to get started with Argo Events is, 1. Read the basic concepts about Gateway , Sensor and Event Source . 2. Install the setup as outlined here . 3. Read the tutorials available here . Q. Can I deploy gateway and sensor in a namespace different that argo-events ? A . Yes. If you want to deploy the gateway in a different namespace that argo-events , then please update the gateway definition with desired namespace and service account. Make sure to grant the service account the necessary roles. Also note that the gateway and sensor controllers are configured to process the gateway and sensor resources in argo-events namespace with instance-id argo-events . You can change the configuration by updating the appropriate controller configmap. Q. Gateway is receiving the events but nothing happens. A . First, check the sensor resource is deployed and a pod is created for the resource. If sensor pod is running, check the subscribers list in the gateway resource. The sensor service url must be registered as a subscriber in order to receive events from gateway. The gateway-client container should also log an error related to this situation. Second, if the gateway was able to send an event to sensor, then check the sensor logs, either the sensor event resolution circuitry has rejected the event or the sensor failed to execute the trigger due to an error. Q. Helm chart installation does not work. A. The helm chart for argo events is maintained by the community and can be out of sync with latest release version. The official installation file is available here . If you notice the helm chart is outdated, we encourage you to contribute to the argo-helm . Q. Kustomization file doesn't have a X resource. A. The kustomization.yaml file is maintained by the community. If you notice that it is out of sync with the official installation file, please raise a PR. Q. Can I use Minio gateway for AWS S3 notifications? A. No. Minio gateway is exclusive for the Minio server. If you want to trigger workloads on AWS S3 bucket notification, then set up the AWS SNS gateway. Q. If I have multiple event dependencies and triggers in a single sensor, can I execute a specific trigger upon a specific event? A. Yes, this is precisely the functionality the sensor event resolution circuitry offers. Please take a look at the Circuit and Switch . Q. The latest image tag does not point to latest release tag? A. When it comes to image tags, the golden rule is do not trust the latest tag. Always use the pinned version of the images. We will try to keep the latest in sync with the latest release version. Q. Where can I find the event structure for a particular gateway? A. Please refer https://github.com/argoproj/argo-events/blob/master/pkg/apis/events/event-data.go to understand the structure of different types of events dispatched by gateways.","title":"FAQs"},{"location":"FAQ/#faqs","text":"Q. How to get started with Argo Events? **A Recommended way to get started with Argo Events is, 1. Read the basic concepts about Gateway , Sensor and Event Source . 2. Install the setup as outlined here . 3. Read the tutorials available here . Q. Can I deploy gateway and sensor in a namespace different that argo-events ? A . Yes. If you want to deploy the gateway in a different namespace that argo-events , then please update the gateway definition with desired namespace and service account. Make sure to grant the service account the necessary roles. Also note that the gateway and sensor controllers are configured to process the gateway and sensor resources in argo-events namespace with instance-id argo-events . You can change the configuration by updating the appropriate controller configmap. Q. Gateway is receiving the events but nothing happens. A . First, check the sensor resource is deployed and a pod is created for the resource. If sensor pod is running, check the subscribers list in the gateway resource. The sensor service url must be registered as a subscriber in order to receive events from gateway. The gateway-client container should also log an error related to this situation. Second, if the gateway was able to send an event to sensor, then check the sensor logs, either the sensor event resolution circuitry has rejected the event or the sensor failed to execute the trigger due to an error. Q. Helm chart installation does not work. A. The helm chart for argo events is maintained by the community and can be out of sync with latest release version. The official installation file is available here . If you notice the helm chart is outdated, we encourage you to contribute to the argo-helm . Q. Kustomization file doesn't have a X resource. A. The kustomization.yaml file is maintained by the community. If you notice that it is out of sync with the official installation file, please raise a PR. Q. Can I use Minio gateway for AWS S3 notifications? A. No. Minio gateway is exclusive for the Minio server. If you want to trigger workloads on AWS S3 bucket notification, then set up the AWS SNS gateway. Q. If I have multiple event dependencies and triggers in a single sensor, can I execute a specific trigger upon a specific event? A. Yes, this is precisely the functionality the sensor event resolution circuitry offers. Please take a look at the Circuit and Switch . Q. The latest image tag does not point to latest release tag? A. When it comes to image tags, the golden rule is do not trust the latest tag. Always use the pinned version of the images. We will try to keep the latest in sync with the latest release version. Q. Where can I find the event structure for a particular gateway? A. Please refer https://github.com/argoproj/argo-events/blob/master/pkg/apis/events/event-data.go to understand the structure of different types of events dispatched by gateways.","title":"FAQs"},{"location":"controllers/","text":"Controllers Sensor and Gateway controllers are the components which manage Sensor and Gateway objects respectively. Sensor and Gateway are Kubernetes Custom Resources. For more information on K8 CRDs visit here. Controller Configmap Defines the instance-id and the namespace for the controller. e.g. # The gateway-controller configmap includes configuration information for the gateway-controller apiVersion : v1 kind : ConfigMap metadata : name : gateway-controller-configmap data : config : | instanceID: argo-events # mandatory namespace: my-custom-namespace # optional namespace : If you don't provide namespace, controller will watch all namespaces for gateway resource. instanceID : it is used to map a gateway or sensor object to a controller. e.g. when you create a gateway with label gateways.argoproj.io/gateway-controller-instanceid: argo-events , a controller with label argo-events will process that gateway. instanceID is used to horizontally scale controllers, so you won't end up overwhelming a single controller with large number of gateways or sensors. Also keep in mind that instanceID has nothing to do with namespace where you are deploying controllers and gateways/sensors objects.","title":"Controllers"},{"location":"controllers/#controllers","text":"Sensor and Gateway controllers are the components which manage Sensor and Gateway objects respectively. Sensor and Gateway are Kubernetes Custom Resources. For more information on K8 CRDs visit here.","title":"Controllers"},{"location":"controllers/#controller-configmap","text":"Defines the instance-id and the namespace for the controller. e.g. # The gateway-controller configmap includes configuration information for the gateway-controller apiVersion : v1 kind : ConfigMap metadata : name : gateway-controller-configmap data : config : | instanceID: argo-events # mandatory namespace: my-custom-namespace # optional namespace : If you don't provide namespace, controller will watch all namespaces for gateway resource. instanceID : it is used to map a gateway or sensor object to a controller. e.g. when you create a gateway with label gateways.argoproj.io/gateway-controller-instanceid: argo-events , a controller with label argo-events will process that gateway. instanceID is used to horizontally scale controllers, so you won't end up overwhelming a single controller with large number of gateways or sensors. Also keep in mind that instanceID has nothing to do with namespace where you are deploying controllers and gateways/sensors objects.","title":"Controller Configmap"},{"location":"developer_guide/","text":"Developer Guide Setup your DEV environment Argo Events is native to Kubernetes so you'll need a running Kubernetes cluster. This guide includes steps for Minikube for local development, but if you have another cluster you can ignore the Minikube specific step 3. Requirements Golang 1.11 Docker dep Installation Setup 1. Get the project go get github.com/argoproj/argo-events cd $GOPATH/src/github.com/argoproj/argo-events 2. Vendor dependencies dep ensure -vendor-only 3. Start Minikube and point Docker Client to Minikube's Docker Daemon minikube start eval $(minikube docker-env) 5. Build the project make all Changing Types If you're making a change to the pkg/apis package, please ensure you re-run the K8 code-generator scripts found in the /hack folder. First, ensure you have the generate-groups.sh script at the path: vendor/k8s.io/code-generator/ . Next run the following commands in order: $ make codegen How to write a custom gateway? To implement a custom gateway, you need to create a gRPC server and implement the service defined below. The framework code acts as a gRPC client consuming event stream from gateway server. Proto Definition The proto file is located here If you choose to implement the gateway in Go , then you can find generated client stubs here To create stubs in other languages, head over to gRPC website Service, /** * Service for handling event sources. */ service Eventing { // StartEventSource starts an event source and returns stream of events . rpc StartEventSource ( EventSource ) returns ( stream Event ) ; // ValidateEventSource validates an event source . rpc ValidateEventSource ( EventSource ) returns ( ValidEventSource ) ; } Available Environment Variables to Server Field Description GATEWAY_NAMESPACE K8s namespace of the gateway GATEWAY_EVENT_SOURCE_CONFIG_MAP K8s configmap containing event source GATEWAY_NAME name of the gateway GATEWAY_CONTROLLER_INSTANCE_ID gateway controller instance id GATEWAY_CONTROLLER_NAME gateway controller name GATEWAY_SERVER_PORT Port on which the gateway gRPC server should run","title":"Developer Guide"},{"location":"developer_guide/#developer-guide","text":"","title":"Developer Guide"},{"location":"developer_guide/#setup-your-dev-environment","text":"Argo Events is native to Kubernetes so you'll need a running Kubernetes cluster. This guide includes steps for Minikube for local development, but if you have another cluster you can ignore the Minikube specific step 3.","title":"Setup your DEV environment"},{"location":"developer_guide/#requirements","text":"Golang 1.11 Docker dep","title":"Requirements"},{"location":"developer_guide/#installation-setup","text":"","title":"Installation &amp; Setup"},{"location":"developer_guide/#1-get-the-project","text":"go get github.com/argoproj/argo-events cd $GOPATH/src/github.com/argoproj/argo-events","title":"1. Get the project"},{"location":"developer_guide/#2-vendor-dependencies","text":"dep ensure -vendor-only","title":"2. Vendor dependencies"},{"location":"developer_guide/#3-start-minikube-and-point-docker-client-to-minikubes-docker-daemon","text":"minikube start eval $(minikube docker-env)","title":"3. Start Minikube and point Docker Client to Minikube's Docker Daemon"},{"location":"developer_guide/#5-build-the-project","text":"make all","title":"5. Build the project"},{"location":"developer_guide/#changing-types","text":"If you're making a change to the pkg/apis package, please ensure you re-run the K8 code-generator scripts found in the /hack folder. First, ensure you have the generate-groups.sh script at the path: vendor/k8s.io/code-generator/ . Next run the following commands in order: $ make codegen","title":"Changing Types"},{"location":"developer_guide/#how-to-write-a-custom-gateway","text":"To implement a custom gateway, you need to create a gRPC server and implement the service defined below. The framework code acts as a gRPC client consuming event stream from gateway server.","title":"How to write a custom gateway?"},{"location":"developer_guide/#proto-definition","text":"The proto file is located here If you choose to implement the gateway in Go , then you can find generated client stubs here To create stubs in other languages, head over to gRPC website Service, /** * Service for handling event sources. */ service Eventing { // StartEventSource starts an event source and returns stream of events . rpc StartEventSource ( EventSource ) returns ( stream Event ) ; // ValidateEventSource validates an event source . rpc ValidateEventSource ( EventSource ) returns ( ValidEventSource ) ; }","title":"Proto Definition"},{"location":"developer_guide/#available-environment-variables-to-server","text":"Field Description GATEWAY_NAMESPACE K8s namespace of the gateway GATEWAY_EVENT_SOURCE_CONFIG_MAP K8s configmap containing event source GATEWAY_NAME name of the gateway GATEWAY_CONTROLLER_INSTANCE_ID gateway controller instance id GATEWAY_CONTROLLER_NAME gateway controller name GATEWAY_SERVER_PORT Port on which the gateway gRPC server should run","title":"Available Environment Variables to Server"},{"location":"installation/","text":"Installation Requirements Kubernetes cluster v1.9 Installed the kubectl command-line tool v1.9.0 Using Helm Chart Note: This method does not work with Helm 3, only Helm 2. Make sure you have helm client installed and Tiller server is running. To install helm, follow the link. Add argoproj repository helm repo add argo https://argoproj.github.io/argo-helm Install argo-events chart helm install argo-events argo/argo-events Using kubectl One Command Installation Deploy Argo Events SA, Roles, ConfigMap, Sensor Controller and Gateway Controller kubectl apply -f https://raw.githubusercontent.com/argoproj/argo-events/master/hack/k8s/manifests/installation.yaml Step-by-Step Installation Create the namespace kubectl create namespace argo-events Create the service account kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/master/hack/k8s/manifests/argo-events-sa.yaml Create the cluster roles kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/master/hack/k8s/manifests/argo-events-cluster-roles.yaml Install the sensor custom resource definition kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/master/hack/k8s/manifests/sensor-crd.yaml Install the gateway custom resource definition kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/master/hack/k8s/manifests/gateway-crd.yaml Install the event source custom resource definition kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/master/hack/k8s/manifests/event-source-crd.yaml Create the confimap for sensor controller kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/master/hack/k8s/manifests/sensor-controller-configmap.yaml Create the configmap for gateway controller kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/master/hack/k8s/manifests/gateway-controller-configmap.yaml Deploy the sensor controller kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/master/hack/k8s/manifests/sensor-controller-deployment.yaml Deploy the gateway controller kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/master/hack/k8s/manifests/gateway-controller-deployment.yaml Deploy at cluster level To deploy Argo-Events controllers at cluster level where the controllers will be able to process gateway and sensor objects created in any namespace, Make sure to apply cluster role and binding to the service account, kubectl apply -f https://raw.githubusercontent.com/argoproj/argo-events/master/hack/k8s/manifests/argo-events-cluster-roles.yaml Update the configmap for both gateway and sensor and remove the namespace key from it. Deploy both gateway and sensor controllers and watch the magic.","title":"Installation"},{"location":"installation/#installation","text":"","title":"Installation"},{"location":"installation/#requirements","text":"Kubernetes cluster v1.9 Installed the kubectl command-line tool v1.9.0","title":"Requirements"},{"location":"installation/#using-helm-chart","text":"Note: This method does not work with Helm 3, only Helm 2. Make sure you have helm client installed and Tiller server is running. To install helm, follow the link. Add argoproj repository helm repo add argo https://argoproj.github.io/argo-helm Install argo-events chart helm install argo-events argo/argo-events","title":"Using Helm Chart"},{"location":"installation/#using-kubectl","text":"","title":"Using kubectl"},{"location":"installation/#one-command-installation","text":"Deploy Argo Events SA, Roles, ConfigMap, Sensor Controller and Gateway Controller kubectl apply -f https://raw.githubusercontent.com/argoproj/argo-events/master/hack/k8s/manifests/installation.yaml","title":"One Command Installation"},{"location":"installation/#step-by-step-installation","text":"Create the namespace kubectl create namespace argo-events Create the service account kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/master/hack/k8s/manifests/argo-events-sa.yaml Create the cluster roles kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/master/hack/k8s/manifests/argo-events-cluster-roles.yaml Install the sensor custom resource definition kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/master/hack/k8s/manifests/sensor-crd.yaml Install the gateway custom resource definition kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/master/hack/k8s/manifests/gateway-crd.yaml Install the event source custom resource definition kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/master/hack/k8s/manifests/event-source-crd.yaml Create the confimap for sensor controller kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/master/hack/k8s/manifests/sensor-controller-configmap.yaml Create the configmap for gateway controller kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/master/hack/k8s/manifests/gateway-controller-configmap.yaml Deploy the sensor controller kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/master/hack/k8s/manifests/sensor-controller-deployment.yaml Deploy the gateway controller kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/master/hack/k8s/manifests/gateway-controller-deployment.yaml","title":"Step-by-Step Installation"},{"location":"installation/#deploy-at-cluster-level","text":"To deploy Argo-Events controllers at cluster level where the controllers will be able to process gateway and sensor objects created in any namespace, Make sure to apply cluster role and binding to the service account, kubectl apply -f https://raw.githubusercontent.com/argoproj/argo-events/master/hack/k8s/manifests/argo-events-cluster-roles.yaml Update the configmap for both gateway and sensor and remove the namespace key from it. Deploy both gateway and sensor controllers and watch the magic.","title":"Deploy at cluster level"},{"location":"quick_start/","text":"Getting Started We are going to set up a gateway, sensor and event-source for webhook. The goal is to trigger an Argo workflow upon a HTTP Post request. Note: You will need to have Argo Workflows installed for this tutorial. First, we need to setup event sources for gateway to listen. kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/master/examples/event-sources/webhook.yaml The event-source drives the configuration required for a gateway to consume events from external sources. Create webhook gateway, kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/master/examples/gateways/webhook.yaml After running above command, gateway controller will create corresponding a pod and service. Create webhook sensor, kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/master/examples/sensors/webhook.yaml Once sensor object is created, sensor controller will create corresponding pod and service. Once the gateway and sensor pods are running, dispatch a HTTP POST request to /example endpoint. Note: the WEBHOOK_SERVICE_URL will differ based on the Kubernetes cluster. export WEBHOOK_SERVICE_URL=$(minikube service -n argo-events --url webhook-gateway-svc) echo $WEBHOOK_SERVICE_URL curl -d { message : this is my first webhook } -H Content-Type: application/json -X POST $WEBHOOK_SERVICE_URL/example Note : If you are facing an issue getting service url by running minikube service -n argo-events --url webhook-gateway-svc You can use port forwarding to access the service as well, kubectl port-forward Open another terminal window and enter kubectl port-forward -n argo-events name_of_the_webhook_gateway_pod 12000:12000 You can now use localhost:12000 to query webhook gateway Verify that an Argo workflow was triggered. kubectl -n argo-events get workflows | grep webhook","title":"Getting Started"},{"location":"quick_start/#getting-started","text":"We are going to set up a gateway, sensor and event-source for webhook. The goal is to trigger an Argo workflow upon a HTTP Post request. Note: You will need to have Argo Workflows installed for this tutorial. First, we need to setup event sources for gateway to listen. kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/master/examples/event-sources/webhook.yaml The event-source drives the configuration required for a gateway to consume events from external sources. Create webhook gateway, kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/master/examples/gateways/webhook.yaml After running above command, gateway controller will create corresponding a pod and service. Create webhook sensor, kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/master/examples/sensors/webhook.yaml Once sensor object is created, sensor controller will create corresponding pod and service. Once the gateway and sensor pods are running, dispatch a HTTP POST request to /example endpoint. Note: the WEBHOOK_SERVICE_URL will differ based on the Kubernetes cluster. export WEBHOOK_SERVICE_URL=$(minikube service -n argo-events --url webhook-gateway-svc) echo $WEBHOOK_SERVICE_URL curl -d { message : this is my first webhook } -H Content-Type: application/json -X POST $WEBHOOK_SERVICE_URL/example Note : If you are facing an issue getting service url by running minikube service -n argo-events --url webhook-gateway-svc You can use port forwarding to access the service as well, kubectl port-forward Open another terminal window and enter kubectl port-forward -n argo-events name_of_the_webhook_gateway_pod 12000:12000 You can now use localhost:12000 to query webhook gateway Verify that an Argo workflow was triggered. kubectl -n argo-events get workflows | grep webhook","title":"Getting Started"},{"location":"concepts/event_source/","text":"Event Source Event Sources are the configuration store for gateways. The configuration stored in an Event Source is used by a gateway to consume events from external entities like AWS SNS, SQS, GCP PubSub, Webhooks etc. Specification Complete specification is available here . Examples Examples are located under examples/event-sources .","title":"Event Source"},{"location":"concepts/event_source/#event-source","text":"Event Sources are the configuration store for gateways. The configuration stored in an Event Source is used by a gateway to consume events from external entities like AWS SNS, SQS, GCP PubSub, Webhooks etc.","title":"Event Source"},{"location":"concepts/event_source/#specification","text":"Complete specification is available here .","title":"Specification"},{"location":"concepts/event_source/#examples","text":"Examples are located under examples/event-sources .","title":"Examples"},{"location":"concepts/gateway/","text":"Gateway What is a gateway? A gateway consumes events from outside entities, transforms them into the cloudevents specification compliant events and dispatches them to sensors. There are two components for a gateway, Gateway Client Gateway client manages the event source for the gateway. Its responsibilities are, 1. Monitor and manage the event sources. 2. Monitor and manage the subscribers. 3. Convert the events received from the gateway server into CloudEvents. 4. Dispatch the cloudevents to subscribers. Gateway Server Gateway server listens to events from event sources. Its responsibilities are, 1. Validate an event source. 2. Implement the logic for consuming events from an event source. 3. Dispatch events to gateway client. Gateway Event Source Event Source are event configuration store for a gateway. The configuration stored in an Event Source is used by a gateway to consume events from external entities like AWS SNS, SQS, GCP PubSub, Webhooks etc. Specification Complete specification is available here . Examples Examples are located under examples/gateways .","title":"Gateway"},{"location":"concepts/gateway/#gateway","text":"","title":"Gateway"},{"location":"concepts/gateway/#what-is-a-gateway","text":"A gateway consumes events from outside entities, transforms them into the cloudevents specification compliant events and dispatches them to sensors. There are two components for a gateway,","title":"What is a gateway?"},{"location":"concepts/gateway/#gateway-client","text":"Gateway client manages the event source for the gateway. Its responsibilities are, 1. Monitor and manage the event sources. 2. Monitor and manage the subscribers. 3. Convert the events received from the gateway server into CloudEvents. 4. Dispatch the cloudevents to subscribers.","title":"Gateway Client"},{"location":"concepts/gateway/#gateway-server","text":"Gateway server listens to events from event sources. Its responsibilities are, 1. Validate an event source. 2. Implement the logic for consuming events from an event source. 3. Dispatch events to gateway client.","title":"Gateway Server"},{"location":"concepts/gateway/#gateway-event-source","text":"Event Source are event configuration store for a gateway. The configuration stored in an Event Source is used by a gateway to consume events from external entities like AWS SNS, SQS, GCP PubSub, Webhooks etc.","title":"Gateway &amp; Event Source"},{"location":"concepts/gateway/#specification","text":"Complete specification is available here .","title":"Specification"},{"location":"concepts/gateway/#examples","text":"Examples are located under examples/gateways .","title":"Examples"},{"location":"concepts/sensor/","text":"Sensor Sensor defines a set of event dependencies (inputs) and triggers (outputs). It listens to events from one or more gateways and act as an event dependency manager. Event dependency A dependency is an event the sensor is waiting to happen. Trigger A Trigger is the resource executed by sensor once the event dependencies are resolved.. Specification Complete specification is available here . Examples Examples are located under examples/sensors .","title":"Sensor"},{"location":"concepts/sensor/#sensor","text":"Sensor defines a set of event dependencies (inputs) and triggers (outputs). It listens to events from one or more gateways and act as an event dependency manager.","title":"Sensor"},{"location":"concepts/sensor/#event-dependency","text":"A dependency is an event the sensor is waiting to happen.","title":"Event dependency"},{"location":"concepts/sensor/#trigger","text":"A Trigger is the resource executed by sensor once the event dependencies are resolved..","title":"Trigger"},{"location":"concepts/sensor/#specification","text":"Complete specification is available here .","title":"Specification"},{"location":"concepts/sensor/#examples","text":"Examples are located under examples/sensors .","title":"Examples"},{"location":"setup/calendar/","text":"Calendar Gateway Sensor Calendar gateway generates events on either a cron schedule or an interval and help trigger workloads. Event Structure The structure of an event dispatched by the gateway to the sensor looks like following, { context : { type : type_of_gateway , specVersion : cloud_events_version , source : name_of_the_gateway , eventID : unique_event_id , time : event_time , dataContentType : type_of_data , subject : name_of_the_event_within_event_source }, data : { eventTime : {/* UTC time of the event */}, userPayload : { /* static payload available in the event source */}, } } Setup Install gateway in the argo-events namespace using following command, kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/master/examples/gateways/calendar.yaml Once the gateway resource is created, the gateway controller will process it and create a pod. If you don't see the pod in argo-events namespace, check the gateway controller logs for errors. If you inspect the gateway resource definition, you will notice it points to the event source called calendar-event-source . Lets install event source in the argo-events namespace, kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/master/examples/event-sources/calendar.yaml Check the gateway logs to make sure the gateway has processed the event source. The gateway will generate events at every 10 seconds. Lets create the sensor, kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/master/examples/sensors/calendar.yaml Once the sensor pod is in running state, wait for next interval to occur. Troubleshoot Please read the FAQ .","title":"Calendar Gateway & Sensor"},{"location":"setup/calendar/#calendar-gateway-sensor","text":"Calendar gateway generates events on either a cron schedule or an interval and help trigger workloads.","title":"Calendar Gateway &amp; Sensor"},{"location":"setup/calendar/#event-structure","text":"The structure of an event dispatched by the gateway to the sensor looks like following, { context : { type : type_of_gateway , specVersion : cloud_events_version , source : name_of_the_gateway , eventID : unique_event_id , time : event_time , dataContentType : type_of_data , subject : name_of_the_event_within_event_source }, data : { eventTime : {/* UTC time of the event */}, userPayload : { /* static payload available in the event source */}, } }","title":"Event Structure"},{"location":"setup/calendar/#setup","text":"Install gateway in the argo-events namespace using following command, kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/master/examples/gateways/calendar.yaml Once the gateway resource is created, the gateway controller will process it and create a pod. If you don't see the pod in argo-events namespace, check the gateway controller logs for errors. If you inspect the gateway resource definition, you will notice it points to the event source called calendar-event-source . Lets install event source in the argo-events namespace, kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/master/examples/event-sources/calendar.yaml Check the gateway logs to make sure the gateway has processed the event source. The gateway will generate events at every 10 seconds. Lets create the sensor, kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/master/examples/sensors/calendar.yaml Once the sensor pod is in running state, wait for next interval to occur.","title":"Setup"},{"location":"setup/calendar/#troubleshoot","text":"Please read the FAQ .","title":"Troubleshoot"},{"location":"setup/kafka/","text":"Kafka Gateway Sensor Event Structure The structure of an event dispatched by the gateway to the sensor looks like following, { context : { type : type_of_gateway , specVersion : cloud_events_version , source : name_of_the_gateway , eventID : unique_event_id , time : event_time , dataContentType : type_of_data , subject : name_of_the_event_within_event_source }, data : { topic : kafka_topic , partition : partition_number , body : message_body , timestamp : timestamp_of_the_message } } Setup Make sure to setup the Kafka cluster in Kubernetes if you don't already have one. You can refer to https://github.com/Yolean/kubernetes-kafka for installation instructions. Create the event source by running the following command. Make sure to update the appropriate fields. kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/master/examples/event-sources/kafka.yaml Create the gateway by running the following command, kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/master/examples/gateways/kafka.yaml Inspect the gateway pod logs to make sure the gateway was able to subscribe to the topic specified in the event source to consume messages. Create the sensor by running the following command, kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/master/examples/sensors/kafka.yaml Send message by using Kafka client. More info on how to send message at https://kafka.apache.org/quickstart Once a message is published, an argo workflow will be triggered. Run argo list to find the workflow. Troubleshoot Please read the FAQ .","title":"Kafka Gateway & Sensor"},{"location":"setup/kafka/#kafka-gateway-sensor","text":"","title":"Kafka Gateway &amp; Sensor"},{"location":"setup/kafka/#event-structure","text":"The structure of an event dispatched by the gateway to the sensor looks like following, { context : { type : type_of_gateway , specVersion : cloud_events_version , source : name_of_the_gateway , eventID : unique_event_id , time : event_time , dataContentType : type_of_data , subject : name_of_the_event_within_event_source }, data : { topic : kafka_topic , partition : partition_number , body : message_body , timestamp : timestamp_of_the_message } }","title":"Event Structure"},{"location":"setup/kafka/#setup","text":"Make sure to setup the Kafka cluster in Kubernetes if you don't already have one. You can refer to https://github.com/Yolean/kubernetes-kafka for installation instructions. Create the event source by running the following command. Make sure to update the appropriate fields. kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/master/examples/event-sources/kafka.yaml Create the gateway by running the following command, kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/master/examples/gateways/kafka.yaml Inspect the gateway pod logs to make sure the gateway was able to subscribe to the topic specified in the event source to consume messages. Create the sensor by running the following command, kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/master/examples/sensors/kafka.yaml Send message by using Kafka client. More info on how to send message at https://kafka.apache.org/quickstart Once a message is published, an argo workflow will be triggered. Run argo list to find the workflow.","title":"Setup"},{"location":"setup/kafka/#troubleshoot","text":"Please read the FAQ .","title":"Troubleshoot"},{"location":"setup/minio/","text":"Minio Gateway Sensor Minio gateway listens to minio bucket notifications and helps sensor trigger the workloads. Note: Minio gateway is exclusive for the Minio server. If you want to trigger workloads on AWS S3 bucket notification, set up the AWS SNS gateway. Event Structure The structure of an event dispatched by the gateway to the sensor looks like following, { context : { type : type_of_gateway , specVersion : cloud_events_version , source : name_of_the_gateway , eventID : unique_event_id , time : event_time , dataContentType : type_of_data , subject : name_of_the_event_within_event_source }, data : { notification: [ { /* Minio notification. More info is available at https://docs.min.io/docs/minio-bucket-notification-guide.html } ] } } Setup Make sure to have minio server deployed and reachable from the gateway. More info on minio server setup is available at https://github.com/minio/minio/blob/master/docs/orchestration/kubernetes/k8s-yaml.md. Create a K8s secret that holds the access and secret key. This secret will be referred in the minio event source definition that we are going to install in a later step. apiVersion : v1 data : # base64 of minio accesskey : bWluaW8 = # base64 of minio123 secretkey : bWluaW8xMjM = kind : Secret metadata : name : artifacts - minio namespace : argo - events Install gateway in the argo-events namespace using following command, kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/master/examples/gateways/minio.yaml Once the gateway resource is created, the gateway controller will process it and create a pod. If you don't see the pod in argo-events namespace, check the gateway controller logs for errors. Make sure to create the bucket called input . If you don't have the bucket created, then the gateway will mark the event source as failure. If the minio service name differs from the event source, then make sure to update the event source. If you inspect the gateway resource definition, you will notice that it refers to the event source minio-event-source . Lets install event source in the argo-events namespace, kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/master/examples/event-sources/minio.yaml Check the gateway logs to make sure the gateway has processed the event source. Lets create the sensor, kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/master/examples/sensors/minio.yaml Create a file named and hello-world.txt and upload it onto to the bucket. This will trigger the argo workflow. Run argo list to find the workflow. Troubleshoot Please read the FAQ .","title":"Minio Gateway & Sensor"},{"location":"setup/minio/#minio-gateway-sensor","text":"Minio gateway listens to minio bucket notifications and helps sensor trigger the workloads. Note: Minio gateway is exclusive for the Minio server. If you want to trigger workloads on AWS S3 bucket notification, set up the AWS SNS gateway.","title":"Minio Gateway &amp; Sensor"},{"location":"setup/minio/#event-structure","text":"The structure of an event dispatched by the gateway to the sensor looks like following, { context : { type : type_of_gateway , specVersion : cloud_events_version , source : name_of_the_gateway , eventID : unique_event_id , time : event_time , dataContentType : type_of_data , subject : name_of_the_event_within_event_source }, data : { notification: [ { /* Minio notification. More info is available at https://docs.min.io/docs/minio-bucket-notification-guide.html } ] } }","title":"Event Structure"},{"location":"setup/minio/#setup","text":"Make sure to have minio server deployed and reachable from the gateway. More info on minio server setup is available at https://github.com/minio/minio/blob/master/docs/orchestration/kubernetes/k8s-yaml.md. Create a K8s secret that holds the access and secret key. This secret will be referred in the minio event source definition that we are going to install in a later step. apiVersion : v1 data : # base64 of minio accesskey : bWluaW8 = # base64 of minio123 secretkey : bWluaW8xMjM = kind : Secret metadata : name : artifacts - minio namespace : argo - events Install gateway in the argo-events namespace using following command, kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/master/examples/gateways/minio.yaml Once the gateway resource is created, the gateway controller will process it and create a pod. If you don't see the pod in argo-events namespace, check the gateway controller logs for errors. Make sure to create the bucket called input . If you don't have the bucket created, then the gateway will mark the event source as failure. If the minio service name differs from the event source, then make sure to update the event source. If you inspect the gateway resource definition, you will notice that it refers to the event source minio-event-source . Lets install event source in the argo-events namespace, kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/master/examples/event-sources/minio.yaml Check the gateway logs to make sure the gateway has processed the event source. Lets create the sensor, kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/master/examples/sensors/minio.yaml Create a file named and hello-world.txt and upload it onto to the bucket. This will trigger the argo workflow. Run argo list to find the workflow.","title":"Setup"},{"location":"setup/minio/#troubleshoot","text":"Please read the FAQ .","title":"Troubleshoot"},{"location":"setup/nats/","text":"NATS Gateway Sensor NATS gateway listens to NATS subject notifications and helps sensor trigger the workloads. Event Structure The structure of an event dispatched by the gateway to the sensor looks like following, { context : { type : type_of_gateway , specVersion : cloud_events_version , source : name_of_the_gateway , eventID : unique_event_id , time : event_time , dataContentType : type_of_data , subject : name_of_the_event_within_event_source }, data : { subject : name_of_the_nats_subject , body : message_payload } } Setup Make sure to have NATS cluster deployed in the Kubernetes. If you don't have one already installed, please refer https://github.com/nats-io/nats-operator for details. NATS cluster setup for test purposes, apiVersion: v1 kind: Service metadata: name: nats namespace: argo-events labels: component: nats spec: selector: component: nats type: ClusterIP ports: - name: client port: 4222 - name: cluster port: 6222 - name: monitor port: 8222 --- apiVersion: apps/v1beta1 kind: StatefulSet metadata: name: nats namespace: argo-events labels: component: nats spec: serviceName: nats replicas: 1 template: metadata: labels: component: nats spec: serviceAccountName: argo-events-sa containers: - name: nats image: nats:latest ports: - containerPort: 4222 name: client - containerPort: 6222 name: cluster - containerPort: 8222 name: monitor livenessProbe: httpGet: path: / port: 8222 initialDelaySeconds: 10 timeoutSeconds: 5 Create the event source by running the following command. Make sure to update the appropriate fields. kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/master/examples/event-sources/nats.yaml Create the gateway by running the following command, kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/master/examples/gateways/nats.yaml Inspect the gateway pod logs to make sure the gateway was able to subscribe to the subject specified in the event source to consume messages. Create the sensor by running the following command, kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/master/examples/sensors/nats.yaml Its time to publish a message for the subject specified in the event source. Refer the nats example to publish a message to the subject https://github.com/nats-io/go-nats-examples/tree/master/patterns/publish-subscribe. Once a message is published, an argo workflow will be triggered. Run argo list to find the workflow. Troubleshoot Please read the FAQ .","title":"NATS Gateway & Sensor"},{"location":"setup/nats/#nats-gateway-sensor","text":"NATS gateway listens to NATS subject notifications and helps sensor trigger the workloads.","title":"NATS Gateway &amp; Sensor"},{"location":"setup/nats/#event-structure","text":"The structure of an event dispatched by the gateway to the sensor looks like following, { context : { type : type_of_gateway , specVersion : cloud_events_version , source : name_of_the_gateway , eventID : unique_event_id , time : event_time , dataContentType : type_of_data , subject : name_of_the_event_within_event_source }, data : { subject : name_of_the_nats_subject , body : message_payload } }","title":"Event Structure"},{"location":"setup/nats/#setup","text":"Make sure to have NATS cluster deployed in the Kubernetes. If you don't have one already installed, please refer https://github.com/nats-io/nats-operator for details. NATS cluster setup for test purposes, apiVersion: v1 kind: Service metadata: name: nats namespace: argo-events labels: component: nats spec: selector: component: nats type: ClusterIP ports: - name: client port: 4222 - name: cluster port: 6222 - name: monitor port: 8222 --- apiVersion: apps/v1beta1 kind: StatefulSet metadata: name: nats namespace: argo-events labels: component: nats spec: serviceName: nats replicas: 1 template: metadata: labels: component: nats spec: serviceAccountName: argo-events-sa containers: - name: nats image: nats:latest ports: - containerPort: 4222 name: client - containerPort: 6222 name: cluster - containerPort: 8222 name: monitor livenessProbe: httpGet: path: / port: 8222 initialDelaySeconds: 10 timeoutSeconds: 5 Create the event source by running the following command. Make sure to update the appropriate fields. kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/master/examples/event-sources/nats.yaml Create the gateway by running the following command, kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/master/examples/gateways/nats.yaml Inspect the gateway pod logs to make sure the gateway was able to subscribe to the subject specified in the event source to consume messages. Create the sensor by running the following command, kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/master/examples/sensors/nats.yaml Its time to publish a message for the subject specified in the event source. Refer the nats example to publish a message to the subject https://github.com/nats-io/go-nats-examples/tree/master/patterns/publish-subscribe. Once a message is published, an argo workflow will be triggered. Run argo list to find the workflow.","title":"Setup"},{"location":"setup/nats/#troubleshoot","text":"Please read the FAQ .","title":"Troubleshoot"},{"location":"setup/resource/","text":"Resource Gateway Sensor Resource gateway watches change notifications for K8s object and helps sensor trigger the workloads. Event Structure The structure of an event dispatched by the gateway to the sensor looks like following, { context : { type : type_of_gateway , specVersion : cloud_events_version , source : name_of_the_gateway , eventID : unique_event_id , time : event_time , dataContentType : type_of_data , subject : name_of_the_event_within_event_source }, data : { type : type_of_the_event , // CREATE, UPDATE or DELETE body : resource_body , group : resource_group_name , version : resource_version_name , resource : resource_name } } Setup Create the event source by running the following command. The event source has multiple configurations that you may not be interested in. Make sure to update the appropriate fields. kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/master/examples/event-sources/resource.yaml Create the gateway by running the following command, kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/master/examples/gateways/resource.yaml Create the sensor by running the following command, kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/master/examples/sensors/resource.yaml If you create an argo workflow called my-workflow , once it transitions into success state, the sensor will trigger another argo workflow. Troubleshoot Please read the FAQ .","title":"Resource Gateway & Sensor"},{"location":"setup/resource/#resource-gateway-sensor","text":"Resource gateway watches change notifications for K8s object and helps sensor trigger the workloads.","title":"Resource Gateway &amp; Sensor"},{"location":"setup/resource/#event-structure","text":"The structure of an event dispatched by the gateway to the sensor looks like following, { context : { type : type_of_gateway , specVersion : cloud_events_version , source : name_of_the_gateway , eventID : unique_event_id , time : event_time , dataContentType : type_of_data , subject : name_of_the_event_within_event_source }, data : { type : type_of_the_event , // CREATE, UPDATE or DELETE body : resource_body , group : resource_group_name , version : resource_version_name , resource : resource_name } }","title":"Event Structure"},{"location":"setup/resource/#setup","text":"Create the event source by running the following command. The event source has multiple configurations that you may not be interested in. Make sure to update the appropriate fields. kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/master/examples/event-sources/resource.yaml Create the gateway by running the following command, kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/master/examples/gateways/resource.yaml Create the sensor by running the following command, kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/master/examples/sensors/resource.yaml If you create an argo workflow called my-workflow , once it transitions into success state, the sensor will trigger another argo workflow.","title":"Setup"},{"location":"setup/resource/#troubleshoot","text":"Please read the FAQ .","title":"Troubleshoot"},{"location":"setup/wehbook/","text":"Webhook Gateway Sensor Webhook gateway exposes a http server and allows external entities to trigger workloads via http requests. Event Structure The structure of an event dispatched by the gateway to the sensor looks like following, { context : { type : type_of_gateway , specVersion : cloud_events_version , source : name_of_the_gateway , eventID : unique_event_id , time : event_time , dataContentType : type_of_data , subject : name_of_the_event_within_event_source }, data : { header : {/* the headers from the request received by the gateway from the external entity */}, body : { /* the payload of the request received by the gateway from the external entity */}, } } Setup Install gateway in the argo-events namespace using following command, kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/master/examples/gateways/webhook.yaml Once the gateway resource is created, the gateway controller will process it and create a pod and a service. If you don't see the pod and service in argo-events namespace, check the gateway controller logs for errors. If you inspect the gateway resource definition, you will notice it points to the event source called webhook-event-source . Lets install event source in the argo-events namespace, kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/master/examples/event-sources/webhook.yaml Check the gateway logs to make sure the gateway has processed the event source. The gateway is now listening for HTTP requests on port 12000 and endpoint /example . Its time to create the sensor, kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/master/examples/sensors/webhook.yaml Once the sensor pod is in running state, test the setup by sending a POST request to gateway service. Troubleshoot Please read the FAQ .","title":"Webhook Gateway & Sensor"},{"location":"setup/wehbook/#webhook-gateway-sensor","text":"Webhook gateway exposes a http server and allows external entities to trigger workloads via http requests.","title":"Webhook Gateway &amp; Sensor"},{"location":"setup/wehbook/#event-structure","text":"The structure of an event dispatched by the gateway to the sensor looks like following, { context : { type : type_of_gateway , specVersion : cloud_events_version , source : name_of_the_gateway , eventID : unique_event_id , time : event_time , dataContentType : type_of_data , subject : name_of_the_event_within_event_source }, data : { header : {/* the headers from the request received by the gateway from the external entity */}, body : { /* the payload of the request received by the gateway from the external entity */}, } }","title":"Event Structure"},{"location":"setup/wehbook/#setup","text":"Install gateway in the argo-events namespace using following command, kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/master/examples/gateways/webhook.yaml Once the gateway resource is created, the gateway controller will process it and create a pod and a service. If you don't see the pod and service in argo-events namespace, check the gateway controller logs for errors. If you inspect the gateway resource definition, you will notice it points to the event source called webhook-event-source . Lets install event source in the argo-events namespace, kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/master/examples/event-sources/webhook.yaml Check the gateway logs to make sure the gateway has processed the event source. The gateway is now listening for HTTP requests on port 12000 and endpoint /example . Its time to create the sensor, kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/master/examples/sensors/webhook.yaml Once the sensor pod is in running state, test the setup by sending a POST request to gateway service.","title":"Setup"},{"location":"setup/wehbook/#troubleshoot","text":"Please read the FAQ .","title":"Troubleshoot"},{"location":"tutorials/01-introduction/","text":"Introduction In the tutorials, we will cover every aspect of Argo Events and demonstrate how you can leverage these features to build an event driven workflow pipeline. All the concepts you will learn in this tutorial and subsequent ones can be applied to any type of gateway. Prerequisites Follow the installation guide to set up the Argo Events. Make sure to configure Argo Workflow controller to listen to workflow objects created in argo-events namespace. Make sure to read the concepts behind gateway , sensor , event source . Get Started Lets set up a basic webhook gateway and sensor that listens to events over HTTP and executes an Argo workflow. Create the webhook event source. kubectl -n argo-events create -f https://raw.githubusercontent.com/argoproj/argo-events/master/examples/event-sources/webhook.yaml Create the webhook gateway. kubectl -n argo-events create -f https://raw.githubusercontent.com/argoproj/argo-events/master/examples/gateways/webhook.yaml Create the webhook sensor. kubectl -n argo-events create -f https://github.com/argoproj/argo-events/tree/master/examples/sensors/webhook.yaml If the commands are executed successfully, the gateway and sensor pods will get created. You will also notice that a service is created for both the gateway and sensor. Expose the gateway pod via Ingress, OpenShift Route or port forward to consume requests over HTTP. kubectl -n port-forward gateway-pod-name 12000:12000 Use either Curl or Postman to send a post request to the http://localhost:12000/example curl -d { message : this is my first webhook } -H Content-Type: application/json -X POST http://localhost:12000/example Now, you should see an Argo workflow being created. kubectl -n argo-events get wf Make sure the workflow pod ran successfully. _________________________________________ / { context :{ type : webhook , specVersi \\ | on : 0.3 , source : webhook-gateway , e | | ventID : 38376665363064642d343336352d34 | | 3035372d393766662d366234326130656232343 | | 337 , time : 2020-01-11T16:55:42.996636 | | Z , dataContentType : application/json | | , subject : example }, data : eyJoZWFkZ | | XIiOnsiQWNjZXB0IjpbIiovKiJdLCJDb250ZW50 | | LUxlbmd0aCI6WyIzOCJdLCJDb250ZW50LVR5cGU | | iOlsiYXBwbGljYXRpb24vanNvbiJdLCJVc2VyLU | | FnZW50IjpbImN1cmwvNy41NC4wIl19LCJib2R5I | | jp7Im1lc3NhZ2UiOiJ0aGlzIGlzIG15IGZpcnN0 | \\ IHdlYmhvb2sifX0= } / ----------------------------------------- \\ \\ \\ ## . ## ## ## == ## ## ## ## === / ___/ === ~~~ {~~ ~~~~ ~~~ ~~~~ ~~ ~ / ===- ~~~ \\______ o __/ \\ \\ __/ \\____\\______/ Note: You will see the message printed in the workflow logs contains both the event context and data, with data being base64 encoded. In later sections, we will see how to extract particular key-value from event context or data and pass it to the workflow as arguments. Troubleshoot If you don't see the gateway and sensor pod in argo-events namespace, Make sure the correct Role and RoleBindings are applied to the service account and there are no errors in both gateway and sensor controller. Make sure gateway and sensor controller configmap has namespace set to argo-events . Check the logs of gateway and sensor controller. Make sure the controllers have processed the gateway and sensor objects and there are no errors. Look for any error in gateway or sensor pod. Inspect the gateway, kubectl -n argo-event gateway-object-name -o yaml Inspect the sensor, kubectl -n argo-events sensor-object-name -o yaml and look for any errors within the Status . Raise an issue on GitHub or post a question on argo-events slack channel.","title":"Introduction"},{"location":"tutorials/01-introduction/#introduction","text":"In the tutorials, we will cover every aspect of Argo Events and demonstrate how you can leverage these features to build an event driven workflow pipeline. All the concepts you will learn in this tutorial and subsequent ones can be applied to any type of gateway.","title":"Introduction"},{"location":"tutorials/01-introduction/#prerequisites","text":"Follow the installation guide to set up the Argo Events. Make sure to configure Argo Workflow controller to listen to workflow objects created in argo-events namespace. Make sure to read the concepts behind gateway , sensor , event source .","title":"Prerequisites"},{"location":"tutorials/01-introduction/#get-started","text":"Lets set up a basic webhook gateway and sensor that listens to events over HTTP and executes an Argo workflow. Create the webhook event source. kubectl -n argo-events create -f https://raw.githubusercontent.com/argoproj/argo-events/master/examples/event-sources/webhook.yaml Create the webhook gateway. kubectl -n argo-events create -f https://raw.githubusercontent.com/argoproj/argo-events/master/examples/gateways/webhook.yaml Create the webhook sensor. kubectl -n argo-events create -f https://github.com/argoproj/argo-events/tree/master/examples/sensors/webhook.yaml If the commands are executed successfully, the gateway and sensor pods will get created. You will also notice that a service is created for both the gateway and sensor. Expose the gateway pod via Ingress, OpenShift Route or port forward to consume requests over HTTP. kubectl -n port-forward gateway-pod-name 12000:12000 Use either Curl or Postman to send a post request to the http://localhost:12000/example curl -d { message : this is my first webhook } -H Content-Type: application/json -X POST http://localhost:12000/example Now, you should see an Argo workflow being created. kubectl -n argo-events get wf Make sure the workflow pod ran successfully. _________________________________________ / { context :{ type : webhook , specVersi \\ | on : 0.3 , source : webhook-gateway , e | | ventID : 38376665363064642d343336352d34 | | 3035372d393766662d366234326130656232343 | | 337 , time : 2020-01-11T16:55:42.996636 | | Z , dataContentType : application/json | | , subject : example }, data : eyJoZWFkZ | | XIiOnsiQWNjZXB0IjpbIiovKiJdLCJDb250ZW50 | | LUxlbmd0aCI6WyIzOCJdLCJDb250ZW50LVR5cGU | | iOlsiYXBwbGljYXRpb24vanNvbiJdLCJVc2VyLU | | FnZW50IjpbImN1cmwvNy41NC4wIl19LCJib2R5I | | jp7Im1lc3NhZ2UiOiJ0aGlzIGlzIG15IGZpcnN0 | \\ IHdlYmhvb2sifX0= } / ----------------------------------------- \\ \\ \\ ## . ## ## ## == ## ## ## ## === / ___/ === ~~~ {~~ ~~~~ ~~~ ~~~~ ~~ ~ / ===- ~~~ \\______ o __/ \\ \\ __/ \\____\\______/ Note: You will see the message printed in the workflow logs contains both the event context and data, with data being base64 encoded. In later sections, we will see how to extract particular key-value from event context or data and pass it to the workflow as arguments.","title":"Get Started"},{"location":"tutorials/01-introduction/#troubleshoot","text":"If you don't see the gateway and sensor pod in argo-events namespace, Make sure the correct Role and RoleBindings are applied to the service account and there are no errors in both gateway and sensor controller. Make sure gateway and sensor controller configmap has namespace set to argo-events . Check the logs of gateway and sensor controller. Make sure the controllers have processed the gateway and sensor objects and there are no errors. Look for any error in gateway or sensor pod. Inspect the gateway, kubectl -n argo-event gateway-object-name -o yaml Inspect the sensor, kubectl -n argo-events sensor-object-name -o yaml and look for any errors within the Status . Raise an issue on GitHub or post a question on argo-events slack channel.","title":"Troubleshoot"},{"location":"tutorials/02-parameterization/","text":"Parameterization In previous section, we saw how to set up a basic webhook gateway and sensor, and trigger an Argo workflow. The trigger template had parameters set in the sensor obejct and the workflow was able to print the event payload. In this tutorial, we will dig deeper into different types of parameterization, how to extract particular key-value from event payload and how to use default values if certain key is not available within event payload. Trigger Resource Parameterization If you take a closer look at the Sensor object, you will notice it contains a list of triggers. Each Trigger contains the template that defines the context of the trigger and actual resource that we expect the sensor to execute. In previous section, the resource within the trigger template was an Argo workflow. This subsection deals with how to parameterize the resource within trigger template with the event payload. Prerequisites Make sure to have the basic webhook gateway and sensor set up. Follow the introduction tutorial if haven't done already. Webhook Event Payload Webhook gateway consumes events through HTTP requests and transforms them into CloudEvents. The structure of the event the Webhook sensor receives from the gateway looks like following, { context : { type : type_of_gateway , specVersion : cloud_events_version , source : name_of_the_gateway , eventID : unique_event_id , time : event_time , dataContentType : type_of_data , subject : name_of_the_event_within_event_source }, data : { header : {}, body : {}, } } Context : This is the CloudEvent context and it is populated by the gateway regardless of type of HTTP request. Data : Data contains following fields, Header : The header within event data contains the headers in the HTTP request that was dispatched to the gateway. The gateway extracts the headers from the request and put it in the the header within event data . Body : This is the request payload from the HTTP request. Event Context Now that we have an understanding of the structure of the event the webhook sensor receives from the gateway, lets see how we can use the event context to parameterize the Argo workflow. Update the Webhook Sensor and add the contextKey for the parameter at index 0. kubectl -n argo-events apply -f https://raw.githubusercontent.com/argoproj/argo-events/master/examples/tutorials/02-parameterization/sensor-01.yaml Send a HTTP request to the gateway. curl -d { message : this is my first webhook } -H Content-Type: application/json -X POST http://localhost:12000/example Inspect the output of the Argo workflow that was created. argo logs name_of_the_workflow You will see the following output, _________ webhook --------- \\ \\ \\ ## . ## ## ## == ## ## ## ## === / ___/ === ~~~ {~~ ~~~~ ~~~ ~~~~ ~~ ~ / ===- ~~~ \\______ o __/ \\ \\ __/ \\____\\______/ We have successfully extracted the type key within the event context and parameterized the workflow to print the value of the type . Event Data Now, it is time to use the event data and parameterize the Argo workflow trigger. We will extract the message from request payload and get the Argo workflow to print the message. Update the Webhook Sensor and add the dataKey in the parameter at index 0. kubectl -n argo-events apply -f https://raw.githubusercontent.com/argoproj/argo-events/master/examples/tutorials/02-parameterization/sensor-02.yaml Send a HTTP request to the gateway. curl -d { message : this is my first webhook } -H Content-Type: application/json -X POST http://localhost:12000/example Inspect the output of the Argo workflow that was created. argo logs name_of_the_workflow You will see the following output, __________________________ this is my first webhook -------------------------- \\ \\ \\ ## . ## ## ## == ## ## ## ## === / ___/ === ~~~ {~~ ~~~~ ~~~ ~~~~ ~~ ~ / ===- ~~~ \\______ o __/ \\ \\ __/ \\____\\______/ Yay!! The Argo workflow printed the message. You can add however many number of parameters to update the trigger resource on the fly. Note : If you define both the contextKey and dataKey within a parameter, then the dataKey takes the precedence. Default Values Each parameter comes with an option to configure the default value. This is specially important when the key you defined in the parameter doesn't exist in the event. Update the Webhook Sensor and add the value for the parameter at index 0. We will also update the dataKey to an unknown event key. kubectl -n argo-events apply -f https://raw.githubusercontent.com/argoproj/argo-events/master/examples/tutorials/02-parameterization/sensor-03.yaml Send a HTTP request to the gateway. curl -d { message : this is my first webhook } -H Content-Type: application/json -X POST http://localhost:12000/example Inspect the output of the Argo workflow that was created. argo logs name_of_the_workflow You will see the following output, _______________________ wow! a default value. ----------------------- \\ \\ \\ ## . ## ## ## == ## ## ## ## === / ___/ === ~~~ {~~ ~~~~ ~~~ ~~~~ ~~ ~ / ===- ~~~ \\______ o __/ \\ \\ __/ \\____\\______/ Operations Sometimes you need the ability to append or prepend a parameter value to an existing value in trigger resource. This is where the operation field within a parameter comes handy. Update the Webhook Sensor and add the operation in the parameter at index 0. We will prepend the message to an existing value. kubectl -n argo-events apply -f https://raw.githubusercontent.com/argoproj/argo-events/master/examples/tutorials/02-parameterization/sensor-04.yaml Send a HTTP request to the gateway. curl -d { message : hey!! } -H Content-Type: application/json -X POST http://localhost:12000/example Inspect the output of the Argo workflow that was created. argo logs name_of_the_workflow You will see the following output, __________________ hey!!hello world ------------------ \\ \\ \\ ## . ## ## ## == ## ## ## ## === / ___/ === ~~~ {~~ ~~~~ ~~~ ~~~~ ~~ ~ / ===- ~~~ \\______ o __/ \\ \\ __/ \\____\\______/ Trigger Template Parameterization The parameterization you saw above deals with the trigger resource, but sometimes you need to parameterize the trigger template itself. This comes handy when you have the trigger resource stored on some external source like S3, Git, etc. and you need to replace the url of the source on the fly in trigger template. Imagine a scenario where you want to parameterize the parameters of trigger to parameterize the trigger resource. What?... The sensor you have been using in this tutorial has one parameter defined in the trigger resource under k8s . We will parameterize that parameter by applying a parameter at the trigger template level. Update the Webhook Sensor and add parameters at trigger level. kubectl -n argo-events apply -f https://raw.githubusercontent.com/argoproj/argo-events/master/examples/tutorials/02-parameterization/sensor-05.yaml Send a HTTP request to the gateway. curl -d { dependencyName : test-dep , dataKey : body.message , dest : spec.arguments.parameters.0.value , message : amazing!! } -H Content-Type: application/json -X POST http://localhost:12000/example Inspect the output of the Argo workflow that was created. argo logs name_of_the_workflow You will see the following output, ___________ amazing!! ----------- \\ \\ \\ ## . ## ## ## == ## ## ## ## === / ___/ === ~~~ {~~ ~~~~ ~~~ ~~~~ ~~ ~ / ===- ~~~ \\______ o __/ \\ \\ __/ \\____\\______/ Great!! You have now learned how to apply parameters at trigger resource and template level. Keep in mind that you can apply default values and operations like prepend and append for trigger template parameters as well.","title":"Parameterization"},{"location":"tutorials/02-parameterization/#parameterization","text":"In previous section, we saw how to set up a basic webhook gateway and sensor, and trigger an Argo workflow. The trigger template had parameters set in the sensor obejct and the workflow was able to print the event payload. In this tutorial, we will dig deeper into different types of parameterization, how to extract particular key-value from event payload and how to use default values if certain key is not available within event payload.","title":"Parameterization"},{"location":"tutorials/02-parameterization/#trigger-resource-parameterization","text":"If you take a closer look at the Sensor object, you will notice it contains a list of triggers. Each Trigger contains the template that defines the context of the trigger and actual resource that we expect the sensor to execute. In previous section, the resource within the trigger template was an Argo workflow. This subsection deals with how to parameterize the resource within trigger template with the event payload.","title":"Trigger Resource Parameterization"},{"location":"tutorials/02-parameterization/#prerequisites","text":"Make sure to have the basic webhook gateway and sensor set up. Follow the introduction tutorial if haven't done already.","title":"Prerequisites"},{"location":"tutorials/02-parameterization/#webhook-event-payload","text":"Webhook gateway consumes events through HTTP requests and transforms them into CloudEvents. The structure of the event the Webhook sensor receives from the gateway looks like following, { context : { type : type_of_gateway , specVersion : cloud_events_version , source : name_of_the_gateway , eventID : unique_event_id , time : event_time , dataContentType : type_of_data , subject : name_of_the_event_within_event_source }, data : { header : {}, body : {}, } } Context : This is the CloudEvent context and it is populated by the gateway regardless of type of HTTP request. Data : Data contains following fields, Header : The header within event data contains the headers in the HTTP request that was dispatched to the gateway. The gateway extracts the headers from the request and put it in the the header within event data . Body : This is the request payload from the HTTP request.","title":"Webhook Event Payload"},{"location":"tutorials/02-parameterization/#event-context","text":"Now that we have an understanding of the structure of the event the webhook sensor receives from the gateway, lets see how we can use the event context to parameterize the Argo workflow. Update the Webhook Sensor and add the contextKey for the parameter at index 0. kubectl -n argo-events apply -f https://raw.githubusercontent.com/argoproj/argo-events/master/examples/tutorials/02-parameterization/sensor-01.yaml Send a HTTP request to the gateway. curl -d { message : this is my first webhook } -H Content-Type: application/json -X POST http://localhost:12000/example Inspect the output of the Argo workflow that was created. argo logs name_of_the_workflow You will see the following output, _________ webhook --------- \\ \\ \\ ## . ## ## ## == ## ## ## ## === / ___/ === ~~~ {~~ ~~~~ ~~~ ~~~~ ~~ ~ / ===- ~~~ \\______ o __/ \\ \\ __/ \\____\\______/ We have successfully extracted the type key within the event context and parameterized the workflow to print the value of the type .","title":"Event Context"},{"location":"tutorials/02-parameterization/#event-data","text":"Now, it is time to use the event data and parameterize the Argo workflow trigger. We will extract the message from request payload and get the Argo workflow to print the message. Update the Webhook Sensor and add the dataKey in the parameter at index 0. kubectl -n argo-events apply -f https://raw.githubusercontent.com/argoproj/argo-events/master/examples/tutorials/02-parameterization/sensor-02.yaml Send a HTTP request to the gateway. curl -d { message : this is my first webhook } -H Content-Type: application/json -X POST http://localhost:12000/example Inspect the output of the Argo workflow that was created. argo logs name_of_the_workflow You will see the following output, __________________________ this is my first webhook -------------------------- \\ \\ \\ ## . ## ## ## == ## ## ## ## === / ___/ === ~~~ {~~ ~~~~ ~~~ ~~~~ ~~ ~ / ===- ~~~ \\______ o __/ \\ \\ __/ \\____\\______/ Yay!! The Argo workflow printed the message. You can add however many number of parameters to update the trigger resource on the fly. Note : If you define both the contextKey and dataKey within a parameter, then the dataKey takes the precedence.","title":"Event Data"},{"location":"tutorials/02-parameterization/#default-values","text":"Each parameter comes with an option to configure the default value. This is specially important when the key you defined in the parameter doesn't exist in the event. Update the Webhook Sensor and add the value for the parameter at index 0. We will also update the dataKey to an unknown event key. kubectl -n argo-events apply -f https://raw.githubusercontent.com/argoproj/argo-events/master/examples/tutorials/02-parameterization/sensor-03.yaml Send a HTTP request to the gateway. curl -d { message : this is my first webhook } -H Content-Type: application/json -X POST http://localhost:12000/example Inspect the output of the Argo workflow that was created. argo logs name_of_the_workflow You will see the following output, _______________________ wow! a default value. ----------------------- \\ \\ \\ ## . ## ## ## == ## ## ## ## === / ___/ === ~~~ {~~ ~~~~ ~~~ ~~~~ ~~ ~ / ===- ~~~ \\______ o __/ \\ \\ __/ \\____\\______/","title":"Default Values"},{"location":"tutorials/02-parameterization/#operations","text":"Sometimes you need the ability to append or prepend a parameter value to an existing value in trigger resource. This is where the operation field within a parameter comes handy. Update the Webhook Sensor and add the operation in the parameter at index 0. We will prepend the message to an existing value. kubectl -n argo-events apply -f https://raw.githubusercontent.com/argoproj/argo-events/master/examples/tutorials/02-parameterization/sensor-04.yaml Send a HTTP request to the gateway. curl -d { message : hey!! } -H Content-Type: application/json -X POST http://localhost:12000/example Inspect the output of the Argo workflow that was created. argo logs name_of_the_workflow You will see the following output, __________________ hey!!hello world ------------------ \\ \\ \\ ## . ## ## ## == ## ## ## ## === / ___/ === ~~~ {~~ ~~~~ ~~~ ~~~~ ~~ ~ / ===- ~~~ \\______ o __/ \\ \\ __/ \\____\\______/","title":"Operations"},{"location":"tutorials/02-parameterization/#trigger-template-parameterization","text":"The parameterization you saw above deals with the trigger resource, but sometimes you need to parameterize the trigger template itself. This comes handy when you have the trigger resource stored on some external source like S3, Git, etc. and you need to replace the url of the source on the fly in trigger template. Imagine a scenario where you want to parameterize the parameters of trigger to parameterize the trigger resource. What?... The sensor you have been using in this tutorial has one parameter defined in the trigger resource under k8s . We will parameterize that parameter by applying a parameter at the trigger template level. Update the Webhook Sensor and add parameters at trigger level. kubectl -n argo-events apply -f https://raw.githubusercontent.com/argoproj/argo-events/master/examples/tutorials/02-parameterization/sensor-05.yaml Send a HTTP request to the gateway. curl -d { dependencyName : test-dep , dataKey : body.message , dest : spec.arguments.parameters.0.value , message : amazing!! } -H Content-Type: application/json -X POST http://localhost:12000/example Inspect the output of the Argo workflow that was created. argo logs name_of_the_workflow You will see the following output, ___________ amazing!! ----------- \\ \\ \\ ## . ## ## ## == ## ## ## ## === / ___/ === ~~~ {~~ ~~~~ ~~~ ~~~~ ~~ ~ / ===- ~~~ \\______ o __/ \\ \\ __/ \\____\\______/ Great!! You have now learned how to apply parameters at trigger resource and template level. Keep in mind that you can apply default values and operations like prepend and append for trigger template parameters as well.","title":"Trigger Template Parameterization"},{"location":"tutorials/03-trigger-sources/","text":"Trigger Sources A trigger source is the source of trigger resource. It can be either external source such as Git , S3 , K8s Configmap , File , any valid URL that hosts the resource or an internal resource which is defined in the sensor object itself like Inline or Resource . In the previous sections, you have been dealing with the Resource trigger source. In this tutorial, we will explore other trigger sources. Prerequisites The Webhook gateway is already set up. Git Git trigger source refers to K8s trigger refers to the K8s resource stored in Git. The specification for the Git source is available here . In order to fetch data from git, you need to set up the private SSH key in sensor. If you don't have ssh keys available, create them following this guide Create a K8s secret that holds the SSH keys kubectl -n argo-events create secret generic git-ssh --from-file=key=.ssh/ YOUR_SSH_KEY_FILE_NAME Create a K8s secret that holds known hosts. kubectl -n argo-events create secret generic git-known-hosts --from-file=ssh_known_hosts=.ssh/known_hosts Create a sensor with the git trigger source and refer it to the hello world worklfow stored on the Argo Git project kubectl -n argo-events apply -f https://raw.githubusercontent.com/argoproj/argo-events/master/examples/tutorials/03-trigger-sources/sensor-git.yaml Use either Curl or Postman to send a post request to the http://localhost:12000/example curl -d { message : ok } -H Content-Type: application/json -X POST http://localhost:12000/example Now, you should see an Argo workflow being created. kubectl -n argo-events get wf S3 You can refer to the K8s resource stored on S3 complaint store as the trigger source. For this tutorial, lets set up a minio server which is S3 compliant store. Create a K8s secret called artifacts-minio that holds your minio access key and secret key. The access key must be stored under accesskey key and secret key must be stored under secretkey . Follow steps described here to set up the minio server. Make sure a service is available to expose the minio server. Create a bucket called workflows and store a basic hello world Argo workflow with key name hello-world.yaml . Create the sensor with trigger source as S3. kubectl -n argo-events apply -f https://raw.githubusercontent.com/argoproj/argo-events/master/examples/tutorials/03-trigger-sources/sensor-minio.yaml Use either Curl or Postman to send a post request to the http://localhost:12000/example curl -d { message : ok } -H Content-Type: application/json -X POST http://localhost:12000/example Now, you should see an Argo workflow being created. kubectl -n argo-events get wf K8s Configmap K8s configmap can be treated as trigger source if needed. Lets create a configmap called trigger-store . kubectl -n argo-events apply -f https://raw.githubusercontent.com/argoproj/argo-events/master/examples/tutorials/03-trigger-sources/trigger-store.yaml Create a sensor with trigger source as configmap and refer it to the trigger-store . kubectl -n argo-events apply -f https://raw.githubusercontent.com/argoproj/argo-events/master/examples/tutorials/03-trigger-sources/sensor-cm.yaml Use either Curl or Postman to send a post request to the http://localhost:12000/example curl -d { message : ok } -H Content-Type: application/json -X POST http://localhost:12000/example Now, you should see an Argo workflow being created. kubectl -n argo-events get wf File URL File and URL trigger sources are pretty self explanatory. The example sensors are available under examples/sensors folder.","title":"Trigger Sources"},{"location":"tutorials/03-trigger-sources/#trigger-sources","text":"A trigger source is the source of trigger resource. It can be either external source such as Git , S3 , K8s Configmap , File , any valid URL that hosts the resource or an internal resource which is defined in the sensor object itself like Inline or Resource . In the previous sections, you have been dealing with the Resource trigger source. In this tutorial, we will explore other trigger sources.","title":"Trigger Sources"},{"location":"tutorials/03-trigger-sources/#prerequisites","text":"The Webhook gateway is already set up.","title":"Prerequisites"},{"location":"tutorials/03-trigger-sources/#git","text":"Git trigger source refers to K8s trigger refers to the K8s resource stored in Git. The specification for the Git source is available here . In order to fetch data from git, you need to set up the private SSH key in sensor. If you don't have ssh keys available, create them following this guide Create a K8s secret that holds the SSH keys kubectl -n argo-events create secret generic git-ssh --from-file=key=.ssh/ YOUR_SSH_KEY_FILE_NAME Create a K8s secret that holds known hosts. kubectl -n argo-events create secret generic git-known-hosts --from-file=ssh_known_hosts=.ssh/known_hosts Create a sensor with the git trigger source and refer it to the hello world worklfow stored on the Argo Git project kubectl -n argo-events apply -f https://raw.githubusercontent.com/argoproj/argo-events/master/examples/tutorials/03-trigger-sources/sensor-git.yaml Use either Curl or Postman to send a post request to the http://localhost:12000/example curl -d { message : ok } -H Content-Type: application/json -X POST http://localhost:12000/example Now, you should see an Argo workflow being created. kubectl -n argo-events get wf","title":"Git"},{"location":"tutorials/03-trigger-sources/#s3","text":"You can refer to the K8s resource stored on S3 complaint store as the trigger source. For this tutorial, lets set up a minio server which is S3 compliant store. Create a K8s secret called artifacts-minio that holds your minio access key and secret key. The access key must be stored under accesskey key and secret key must be stored under secretkey . Follow steps described here to set up the minio server. Make sure a service is available to expose the minio server. Create a bucket called workflows and store a basic hello world Argo workflow with key name hello-world.yaml . Create the sensor with trigger source as S3. kubectl -n argo-events apply -f https://raw.githubusercontent.com/argoproj/argo-events/master/examples/tutorials/03-trigger-sources/sensor-minio.yaml Use either Curl or Postman to send a post request to the http://localhost:12000/example curl -d { message : ok } -H Content-Type: application/json -X POST http://localhost:12000/example Now, you should see an Argo workflow being created. kubectl -n argo-events get wf","title":"S3"},{"location":"tutorials/03-trigger-sources/#k8s-configmap","text":"K8s configmap can be treated as trigger source if needed. Lets create a configmap called trigger-store . kubectl -n argo-events apply -f https://raw.githubusercontent.com/argoproj/argo-events/master/examples/tutorials/03-trigger-sources/trigger-store.yaml Create a sensor with trigger source as configmap and refer it to the trigger-store . kubectl -n argo-events apply -f https://raw.githubusercontent.com/argoproj/argo-events/master/examples/tutorials/03-trigger-sources/sensor-cm.yaml Use either Curl or Postman to send a post request to the http://localhost:12000/example curl -d { message : ok } -H Content-Type: application/json -X POST http://localhost:12000/example Now, you should see an Argo workflow being created. kubectl -n argo-events get wf","title":"K8s Configmap"},{"location":"tutorials/03-trigger-sources/#file-url","text":"File and URL trigger sources are pretty self explanatory. The example sensors are available under examples/sensors folder.","title":"File &amp; URL"},{"location":"tutorials/04-standard-k8s-resources/","text":"Standard K8s Resources In the previous sections, you saw how to trigger the Argo workflows. In this tutorial, you will see how to trigger Pod and Deployment. Similarly you can trigger any standard Kubernetes resources. Having the ability to trigger standard Kubernetes resources is quite powerful as provides an avenue to set up pipelines for existing workloads. Prerequisites Make sure that argo-events-sa service account has necessary permissions to create the Kubernetes resource of your choice. The Webhook gateway is already set up. Pod Create a sensor with K8s trigger. Pay close attention to the group , version and kind keys within the trigger resource. These keys determine the type of kubernetes object. You will notice that the group key is empty, that means we want to use core group. For any other groups, you need to specify the group key. kubectl -n argo-events apply -f https://raw.githubusercontent.com/argoproj/argo-events/master/examples/tutorials/03-standard-k8s-resources/sensor-pod.yaml Use either Curl or Postman to send a post request to the http://localhost:12000/example curl -d { message : ok } -H Content-Type: application/json -X POST http://localhost:12000/example Now, you should see a pod being created. kubectl -n argo-events get po Output _________________________________________ / { context :{ type : webhook , specVersi \\ | on : 0.3 , source : webhook-gateway , e | | ventID : 30306463666539362d346666642d34 | | 3336332d383861312d336538363333613564313 | | 932 , time : 2020-01-11T21:23:07.682961 | | Z , dataContentType : application/json | | , subject : example }, data : eyJoZWFkZ | | XIiOnsiQWNjZXB0IjpbIiovKiJdLCJDb250ZW50 | | LUxlbmd0aCI6WyIxOSJdLCJDb250ZW50LVR5cGU | | iOlsiYXBwbGljYXRpb24vanNvbiJdLCJVc2VyLU | | FnZW50IjpbImN1cmwvNy41NC4wIl19LCJib2R5I | \\ jp7Im1lc3NhZ2UiOiJoZXkhISJ9fQ== } / ----------------------------------------- \\ \\ \\ ## . ## ## ## == ## ## ## ## === / ___/ === ~~~ {~~ ~~~~ ~~~ ~~~~ ~~ ~ / ===- ~~~ \\______ o __/ \\ \\ __/ \\____\\______/ Deployment Lets create a sensor with a K8s deployment as trigger. kubectl -n argo-events apply -f https://raw.githubusercontent.com/argoproj/argo-events/master/examples/tutorials/03-standard-k8s-resources/sensor-deployment.yaml Use either Curl or Postman to send a post request to the http://localhost:12000/example curl -d { message : ok } -H Content-Type: application/json -X POST http://localhost:12000/example Now, you should see a deployment being created. Get the corresponding pod. kubectl -n argo-events get deployments Output _________________________________________ / { context :{ type : webhook , specVersi \\ | on : 0.3 , source : webhook-gateway , e | | ventID : 30306463666539362d346666642d34 | | 3336332d383861312d336538363333613564313 | | 932 , time : 2020-01-11T21:23:07.682961 | | Z , dataContentType : application/json | | , subject : example }, data : eyJoZWFkZ | | XIiOnsiQWNjZXB0IjpbIiovKiJdLCJDb250ZW50 | | LUxlbmd0aCI6WyIxOSJdLCJDb250ZW50LVR5cGU | | iOlsiYXBwbGljYXRpb24vanNvbiJdLCJVc2VyLU | | FnZW50IjpbImN1cmwvNy41NC4wIl19LCJib2R5I | \\ jp7Im1lc3NhZ2UiOiJoZXkhISJ9fQ== } / ----------------------------------------- \\ \\ \\ ## . ## ## ## == ## ## ## ## === / ___/ === ~~~ {~~ ~~~~ ~~~ ~~~~ ~~ ~ / ===- ~~~ \\______ o __/ \\ \\ __/ \\____\\______/","title":"Standard K8s Resources"},{"location":"tutorials/04-standard-k8s-resources/#standard-k8s-resources","text":"In the previous sections, you saw how to trigger the Argo workflows. In this tutorial, you will see how to trigger Pod and Deployment. Similarly you can trigger any standard Kubernetes resources. Having the ability to trigger standard Kubernetes resources is quite powerful as provides an avenue to set up pipelines for existing workloads.","title":"Standard K8s Resources"},{"location":"tutorials/04-standard-k8s-resources/#prerequisites","text":"Make sure that argo-events-sa service account has necessary permissions to create the Kubernetes resource of your choice. The Webhook gateway is already set up.","title":"Prerequisites"},{"location":"tutorials/04-standard-k8s-resources/#pod","text":"Create a sensor with K8s trigger. Pay close attention to the group , version and kind keys within the trigger resource. These keys determine the type of kubernetes object. You will notice that the group key is empty, that means we want to use core group. For any other groups, you need to specify the group key. kubectl -n argo-events apply -f https://raw.githubusercontent.com/argoproj/argo-events/master/examples/tutorials/03-standard-k8s-resources/sensor-pod.yaml Use either Curl or Postman to send a post request to the http://localhost:12000/example curl -d { message : ok } -H Content-Type: application/json -X POST http://localhost:12000/example Now, you should see a pod being created. kubectl -n argo-events get po Output _________________________________________ / { context :{ type : webhook , specVersi \\ | on : 0.3 , source : webhook-gateway , e | | ventID : 30306463666539362d346666642d34 | | 3336332d383861312d336538363333613564313 | | 932 , time : 2020-01-11T21:23:07.682961 | | Z , dataContentType : application/json | | , subject : example }, data : eyJoZWFkZ | | XIiOnsiQWNjZXB0IjpbIiovKiJdLCJDb250ZW50 | | LUxlbmd0aCI6WyIxOSJdLCJDb250ZW50LVR5cGU | | iOlsiYXBwbGljYXRpb24vanNvbiJdLCJVc2VyLU | | FnZW50IjpbImN1cmwvNy41NC4wIl19LCJib2R5I | \\ jp7Im1lc3NhZ2UiOiJoZXkhISJ9fQ== } / ----------------------------------------- \\ \\ \\ ## . ## ## ## == ## ## ## ## === / ___/ === ~~~ {~~ ~~~~ ~~~ ~~~~ ~~ ~ / ===- ~~~ \\______ o __/ \\ \\ __/ \\____\\______/","title":"Pod"},{"location":"tutorials/04-standard-k8s-resources/#deployment","text":"Lets create a sensor with a K8s deployment as trigger. kubectl -n argo-events apply -f https://raw.githubusercontent.com/argoproj/argo-events/master/examples/tutorials/03-standard-k8s-resources/sensor-deployment.yaml Use either Curl or Postman to send a post request to the http://localhost:12000/example curl -d { message : ok } -H Content-Type: application/json -X POST http://localhost:12000/example Now, you should see a deployment being created. Get the corresponding pod. kubectl -n argo-events get deployments Output _________________________________________ / { context :{ type : webhook , specVersi \\ | on : 0.3 , source : webhook-gateway , e | | ventID : 30306463666539362d346666642d34 | | 3336332d383861312d336538363333613564313 | | 932 , time : 2020-01-11T21:23:07.682961 | | Z , dataContentType : application/json | | , subject : example }, data : eyJoZWFkZ | | XIiOnsiQWNjZXB0IjpbIiovKiJdLCJDb250ZW50 | | LUxlbmd0aCI6WyIxOSJdLCJDb250ZW50LVR5cGU | | iOlsiYXBwbGljYXRpb24vanNvbiJdLCJVc2VyLU | | FnZW50IjpbImN1cmwvNy41NC4wIl19LCJib2R5I | \\ jp7Im1lc3NhZ2UiOiJoZXkhISJ9fQ== } / ----------------------------------------- \\ \\ \\ ## . ## ## ## == ## ## ## ## === / ___/ === ~~~ {~~ ~~~~ ~~~ ~~~~ ~~ ~ / ===- ~~~ \\______ o __/ \\ \\ __/ \\____\\______/","title":"Deployment"},{"location":"tutorials/05-trigger-custom-resources/","text":"Trigger Custom Resources Argo Events supports Argo workflows, standard K8s objects, Sensor and Gateway as K8s triggers. In order to add your custom resource as trigger to Argo Events, you will need to register the scheme of resource in Argo Events. If you feel Argo Events should support your custom resource out of box, create an issue on GitHub and provide the details. Steps to build sensor image with your CR Fork the Argo Events project. Go to store.go in store package. Import your custom resource api package. In init method, add the scheme to your custom resource api. Make sure there are no errors. Rebuild the sensor binary using make sensor To build the image, first change IMAGE_NAMESPACE in Makefile to your docker registry and then run make sensor-image.","title":"Trigger Custom Resources"},{"location":"tutorials/05-trigger-custom-resources/#trigger-custom-resources","text":"Argo Events supports Argo workflows, standard K8s objects, Sensor and Gateway as K8s triggers. In order to add your custom resource as trigger to Argo Events, you will need to register the scheme of resource in Argo Events. If you feel Argo Events should support your custom resource out of box, create an issue on GitHub and provide the details.","title":"Trigger Custom Resources"},{"location":"tutorials/05-trigger-custom-resources/#steps-to-build-sensor-image-with-your-cr","text":"Fork the Argo Events project. Go to store.go in store package. Import your custom resource api package. In init method, add the scheme to your custom resource api. Make sure there are no errors. Rebuild the sensor binary using make sensor To build the image, first change IMAGE_NAMESPACE in Makefile to your docker registry and then run make sensor-image.","title":"Steps to build sensor image with your CR"},{"location":"tutorials/06-circuit-and-switch/","text":"Circuit and Switch In previous sections, you have been dealing with just a single dependency. But in many cases, you want to wait for multiple events to occur and then trigger a resource which means you need a mechanism to determine which triggers to execute based on set of different event dependencies. This mechanism is supported through Circuit and Switch . Note : Whenever you define multiple dependencies in a sensor, the sensor applies a AND operation, meaning, it will wait for all dependencies to resolve before it executes triggers. Circuit and Switch can modify that behavior. Prerequisite Minio server must be set up in the argo-events namespace with a bucket called test and it should be available at minio-service.argo-events:9000 . Circuit A circuit is a boolean expression. To create a circuit, you just need to define event dependencies in groups and the sensor will apply the circuit logic on those groups. If the logic results in true value, the sensor will execute the triggers else it won't. Switch A switch is the conditional execution gate for a trigger. Consider a scenario where you have a Webhook and Minio gateway, and you want to trigger an Argo workflow if the sensor receives an event from the Webhook gateway, but, another workflow if it receives an event from the Minio gateway. Create the webhook event source and gateway. The gateway listens to HTTP requests on port 12000 kubectl -n argo-events apply -f https://raw.githubusercontent.com/argoproj/argo-events/master/examples/tutorials/05-circuit-and-switches/webhook-event-source.yaml kubectl -n argo-events apply -f https://raw.githubusercontent.com/argoproj/argo-events/master/examples/tutorials/05-circuit-and-switches/webhook-gateway.yaml Create the minio event source and gateway. The gateway listens to events of type PUT and DELETE for objects in bucket test . kubectl -n argo-events apply -f https://raw.githubusercontent.com/argoproj/argo-events/master/examples/tutorials/05-circuit-and-switches/minio-event-source.yaml kubectl -n argo-events apply -f https://raw.githubusercontent.com/argoproj/argo-events/master/examples/tutorials/05-circuit-and-switches/minio-gateway.yaml Make sure there are no errors in any of the gateways and all event sources are active. Lets create the sensor. If you take a closer look at the trigger templates, you will notice that it contains switch key with all condition, meaning, execute this trigger when every group defined in all is resolved. In the sensor definition, there is only one group under all in both trigger templates. So, as soon as the group is resolved, the corresponding trigger will be executed. kubectl -n argo-events apply -f https://raw.githubusercontent.com/argoproj/argo-events/master/examples/tutorials/06-circuit-and-switches/sensor-01.yaml Send a HTTP request to Webhook gateway, curl -d { message : this is my first webhook } -H Content-Type: application/json -X POST http://localhost:12000/example You will notice an Argo worklfow with name group-1-xxxx is created with following output, __________________________ this is my first webhook -------------------------- \\ \\ \\ ## . ## ## ## == ## ## ## ## === / ___/ === ~~~ {~~ ~~~~ ~~~ ~~~~ ~~ ~ / ===- ~~~ \\______ o __/ \\ \\ __/ \\____\\______/ Now, lets generate a Minio event so that we can run group-2-xxxx workflow. Drop a file onto test bucket. The workflow that will get created will print the name of the bucket as follow, ______ test ------ \\ \\ \\ ## . ## ## ## == ## ## ## ## === / ___/ === ~~~ {~~ ~~~~ ~~~ ~~~~ ~~ ~ / ===- ~~~ \\______ o __/ \\ \\ __/ \\____\\______/ Great!! You have now learned how to use a circuit and switch . Lets update the sensor with a trigger that waits for both groups to resolve. This is the normal sensor behavior if circuit is not defined. kubectl -n argo-events apply -f https://raw.githubusercontent.com/argoproj/argo-events/master/examples/tutorials/06-circuit-and-switches/sensor-02.yaml Send a HTTP request and perform a file drop on Minio bucket as done above. You should following output, _______________________________ this is my first webhook test ------------------------------- \\ \\ \\ ## . ## ## ## == ## ## ## ## === / ___/ === ~~~ {~~ ~~~~ ~~~ ~~~~ ~~ ~ / ===- ~~~ \\______ o __/ \\ \\ __/ \\____\\______/","title":"Circuit and Switch"},{"location":"tutorials/06-circuit-and-switch/#circuit-and-switch","text":"In previous sections, you have been dealing with just a single dependency. But in many cases, you want to wait for multiple events to occur and then trigger a resource which means you need a mechanism to determine which triggers to execute based on set of different event dependencies. This mechanism is supported through Circuit and Switch . Note : Whenever you define multiple dependencies in a sensor, the sensor applies a AND operation, meaning, it will wait for all dependencies to resolve before it executes triggers. Circuit and Switch can modify that behavior.","title":"Circuit and Switch"},{"location":"tutorials/06-circuit-and-switch/#prerequisite","text":"Minio server must be set up in the argo-events namespace with a bucket called test and it should be available at minio-service.argo-events:9000 .","title":"Prerequisite"},{"location":"tutorials/06-circuit-and-switch/#circuit","text":"A circuit is a boolean expression. To create a circuit, you just need to define event dependencies in groups and the sensor will apply the circuit logic on those groups. If the logic results in true value, the sensor will execute the triggers else it won't.","title":"Circuit"},{"location":"tutorials/06-circuit-and-switch/#switch","text":"A switch is the conditional execution gate for a trigger. Consider a scenario where you have a Webhook and Minio gateway, and you want to trigger an Argo workflow if the sensor receives an event from the Webhook gateway, but, another workflow if it receives an event from the Minio gateway. Create the webhook event source and gateway. The gateway listens to HTTP requests on port 12000 kubectl -n argo-events apply -f https://raw.githubusercontent.com/argoproj/argo-events/master/examples/tutorials/05-circuit-and-switches/webhook-event-source.yaml kubectl -n argo-events apply -f https://raw.githubusercontent.com/argoproj/argo-events/master/examples/tutorials/05-circuit-and-switches/webhook-gateway.yaml Create the minio event source and gateway. The gateway listens to events of type PUT and DELETE for objects in bucket test . kubectl -n argo-events apply -f https://raw.githubusercontent.com/argoproj/argo-events/master/examples/tutorials/05-circuit-and-switches/minio-event-source.yaml kubectl -n argo-events apply -f https://raw.githubusercontent.com/argoproj/argo-events/master/examples/tutorials/05-circuit-and-switches/minio-gateway.yaml Make sure there are no errors in any of the gateways and all event sources are active. Lets create the sensor. If you take a closer look at the trigger templates, you will notice that it contains switch key with all condition, meaning, execute this trigger when every group defined in all is resolved. In the sensor definition, there is only one group under all in both trigger templates. So, as soon as the group is resolved, the corresponding trigger will be executed. kubectl -n argo-events apply -f https://raw.githubusercontent.com/argoproj/argo-events/master/examples/tutorials/06-circuit-and-switches/sensor-01.yaml Send a HTTP request to Webhook gateway, curl -d { message : this is my first webhook } -H Content-Type: application/json -X POST http://localhost:12000/example You will notice an Argo worklfow with name group-1-xxxx is created with following output, __________________________ this is my first webhook -------------------------- \\ \\ \\ ## . ## ## ## == ## ## ## ## === / ___/ === ~~~ {~~ ~~~~ ~~~ ~~~~ ~~ ~ / ===- ~~~ \\______ o __/ \\ \\ __/ \\____\\______/ Now, lets generate a Minio event so that we can run group-2-xxxx workflow. Drop a file onto test bucket. The workflow that will get created will print the name of the bucket as follow, ______ test ------ \\ \\ \\ ## . ## ## ## == ## ## ## ## === / ___/ === ~~~ {~~ ~~~~ ~~~ ~~~~ ~~ ~ / ===- ~~~ \\______ o __/ \\ \\ __/ \\____\\______/ Great!! You have now learned how to use a circuit and switch . Lets update the sensor with a trigger that waits for both groups to resolve. This is the normal sensor behavior if circuit is not defined. kubectl -n argo-events apply -f https://raw.githubusercontent.com/argoproj/argo-events/master/examples/tutorials/06-circuit-and-switches/sensor-02.yaml Send a HTTP request and perform a file drop on Minio bucket as done above. You should following output, _______________________________ this is my first webhook test ------------------------------- \\ \\ \\ ## . ## ## ## == ## ## ## ## === / ___/ === ~~~ {~~ ~~~~ ~~~ ~~~~ ~~ ~ / ===- ~~~ \\______ o __/ \\ \\ __/ \\____\\______/","title":"Switch"},{"location":"tutorials/07-filters/","text":"Filters In previous sections, you have seen how to trigger an Argo workflow based on events. In this tutorial, you will learn how to apply filters on event data and context. Filters provide a powerful mechanism to apply constraints on the events in order to determine a validity. Argo Events offers 3 types of filters: Data Filter Context Filter Time Filter Prerequisite Webhook gateway must be set up. Data Filter Data filter as the name suggests are applied on the event data. A CloudEvent from Webhook gateway has payload structure as, { context : { type : type_of_gateway , specVersion : cloud_events_version , source : name_of_the_gateway , eventID : unique_event_id , time : event_time , dataContentType : type_of_data , subject : name_of_the_event_within_event_source }, data : { header : {}, body : {}, } } Data Filter are applied on data within the payload. We will make a simple HTTP request to webhook gateway with request data as {\"message\":\"this is my first webhook\"} and apply data filter on message . A data filter has following fields, data: - path: path_within_event_data type: types_of_the_data value: - list_of_possible_values Note : If data type is a string , then you can pass either an exact value or a regex. If data types is bool or float, then you need to pass the exact value. Lets create a webhook sensor with data filter. kubectl -n argo-events apply -f https://raw.githubusercontent.com/argoproj/argo-events/master/examples/tutorials/07-filters/sensor-data-filter.yaml Send a HTTP request to gateway curl -d { message : this is my first webhook } -H Content-Type: application/json -X POST http://localhost:12000/example You will notice that the sensor logs prints the event is invalid as the sensor expects for either hello or hey as the value of body.message . Send a valid HTTP request to gateway curl -d { message : hello } -H Content-Type: application/json -X POST http://localhost:12000/example Watch for a workflow with name data-workflow-xxxx . Context Filter Similar to the data filter, you can apply a filter on the context of the event. Change the subscriber in the webhook gateway to point it to context-filter sensor's URL. Lets create a webhook sensor with context filter. kubectl -n argo-events apply -f https://raw.githubusercontent.com/argoproj/argo-events/master/examples/tutorials/07-filters/sensor-context-filter.yaml Send a HTTP request to gateway curl -d { message : this is my first webhook } -H Content-Type: application/json -X POST http://localhost:12000/example You will notice that the sensor logs prints the event is invalid as the sensor expects for either custom-webhook as the value of the source . Time Filter Time filter is specially helpful when you need to make sure an event occurs between a certain time-frame. Time filter takes a start and stop time but you can also define just the start time, meaning, there is no end time constraint or just the stop time, meaning, there is no start time constraint. An example of time filter is available under examples/sensors .","title":"Filters"},{"location":"tutorials/07-filters/#filters","text":"In previous sections, you have seen how to trigger an Argo workflow based on events. In this tutorial, you will learn how to apply filters on event data and context. Filters provide a powerful mechanism to apply constraints on the events in order to determine a validity. Argo Events offers 3 types of filters: Data Filter Context Filter Time Filter","title":"Filters"},{"location":"tutorials/07-filters/#prerequisite","text":"Webhook gateway must be set up.","title":"Prerequisite"},{"location":"tutorials/07-filters/#data-filter","text":"Data filter as the name suggests are applied on the event data. A CloudEvent from Webhook gateway has payload structure as, { context : { type : type_of_gateway , specVersion : cloud_events_version , source : name_of_the_gateway , eventID : unique_event_id , time : event_time , dataContentType : type_of_data , subject : name_of_the_event_within_event_source }, data : { header : {}, body : {}, } } Data Filter are applied on data within the payload. We will make a simple HTTP request to webhook gateway with request data as {\"message\":\"this is my first webhook\"} and apply data filter on message . A data filter has following fields, data: - path: path_within_event_data type: types_of_the_data value: - list_of_possible_values Note : If data type is a string , then you can pass either an exact value or a regex. If data types is bool or float, then you need to pass the exact value. Lets create a webhook sensor with data filter. kubectl -n argo-events apply -f https://raw.githubusercontent.com/argoproj/argo-events/master/examples/tutorials/07-filters/sensor-data-filter.yaml Send a HTTP request to gateway curl -d { message : this is my first webhook } -H Content-Type: application/json -X POST http://localhost:12000/example You will notice that the sensor logs prints the event is invalid as the sensor expects for either hello or hey as the value of body.message . Send a valid HTTP request to gateway curl -d { message : hello } -H Content-Type: application/json -X POST http://localhost:12000/example Watch for a workflow with name data-workflow-xxxx .","title":"Data Filter"},{"location":"tutorials/07-filters/#context-filter","text":"Similar to the data filter, you can apply a filter on the context of the event. Change the subscriber in the webhook gateway to point it to context-filter sensor's URL. Lets create a webhook sensor with context filter. kubectl -n argo-events apply -f https://raw.githubusercontent.com/argoproj/argo-events/master/examples/tutorials/07-filters/sensor-context-filter.yaml Send a HTTP request to gateway curl -d { message : this is my first webhook } -H Content-Type: application/json -X POST http://localhost:12000/example You will notice that the sensor logs prints the event is invalid as the sensor expects for either custom-webhook as the value of the source .","title":"Context Filter"},{"location":"tutorials/07-filters/#time-filter","text":"Time filter is specially helpful when you need to make sure an event occurs between a certain time-frame. Time filter takes a start and stop time but you can also define just the start time, meaning, there is no end time constraint or just the stop time, meaning, there is no start time constraint. An example of time filter is available under examples/sensors .","title":"Time Filter"},{"location":"tutorials/08-policy/","text":"Policy A policy for a trigger determines whether the trigger resulted in success or failure. Currently, Argo Events supports 2 types of policies: Policy based on the K8s resource labels. Policy based on the response status for triggers like HTTP request, AWS Lambda, etc. Resource Labels Policy This type of policy determines whether trigger completed successfully based on the labels set on the trigger resource. Consider a sensor which has an Argo workflow as the trigger. When an Argo workflow completes successfully, the workflow controller sets a label on the resource as workflows.argoproj.io/completed: 'true' . So, in order for sensor to determine whether the trigger workflow completed successfully, you just need to set the policy labels as workflows.argoproj.io/completed: 'true' under trigger template. In addition to labels, you can also define a backoff and option to error out if sensor is unable to determine status of the trigger after the backoff completes. Check out the specification of resource labels policy here . Status Policy For triggers like HTTP request or AWS Lambda, you can apply the Status Policy to determine the trigger status. The Status Policy supports list of expected response statuses. If the status of the HTTP request or Lamda is within the statuses defined in the policy, then the trigger is considered successful. Complete specification is available here .","title":"Policy"},{"location":"tutorials/08-policy/#policy","text":"A policy for a trigger determines whether the trigger resulted in success or failure. Currently, Argo Events supports 2 types of policies: Policy based on the K8s resource labels. Policy based on the response status for triggers like HTTP request, AWS Lambda, etc.","title":"Policy"},{"location":"tutorials/08-policy/#resource-labels-policy","text":"This type of policy determines whether trigger completed successfully based on the labels set on the trigger resource. Consider a sensor which has an Argo workflow as the trigger. When an Argo workflow completes successfully, the workflow controller sets a label on the resource as workflows.argoproj.io/completed: 'true' . So, in order for sensor to determine whether the trigger workflow completed successfully, you just need to set the policy labels as workflows.argoproj.io/completed: 'true' under trigger template. In addition to labels, you can also define a backoff and option to error out if sensor is unable to determine status of the trigger after the backoff completes. Check out the specification of resource labels policy here .","title":"Resource Labels Policy"},{"location":"tutorials/08-policy/#status-policy","text":"For triggers like HTTP request or AWS Lambda, you can apply the Status Policy to determine the trigger status. The Status Policy supports list of expected response statuses. If the status of the HTTP request or Lamda is within the statuses defined in the policy, then the trigger is considered successful. Complete specification is available here .","title":"Status Policy"},{"location":"tutorials/09-http-trigger/","text":"HTTP Trigger Sometimes you face a situation where creating an Argo workflow on every event is not an ideal solution. This is where the HTTP trigger can help you. With this type of trigger, you can connect any old/new API server with 20+ event sources supported by Argo Events or invoke serveless fucntions without worrying about their respective event connector frameworks. Prerequisite Set up the Minio gateway and event source. The K8s manifests are available under examples/tutorials/09-http-trigger . API Server Integration We will set up a basic go http server and connect it with the minio events. Set up a simple http server that prints the request payload. kubectl -n argo-events apply -f https://raw.githubusercontent.com/argoproj/argo-events/master/examples/tutorials/09-http-trigger/http-server.yaml Create a service to expose the http server kubectl -n argo-events apply -f https://raw.githubusercontent.com/argoproj/argo-events/master/examples/tutorials/09-http-trigger/http-server-svc.yaml Either use Ingress, OpenShift Route or port-forwarding to expose the http server. We will use port-forwarding here. Create a sensor with HTTP trigger. We will discuss the trigger details in the following sections. kubectl -n argo-events apply -f https://raw.githubusercontent.com/argoproj/argo-events/master/examples/tutorials/09-http-trigger/sensor-01.yaml Now, drop a file onto test bucket in Minio server. The sensor has triggered a http request to out basic go http server. Take a look at the logs server is listening on 8090 { type : minio , bucket : test } Great!!! But how did the sensor constructed a payload for the http request? We will see that in next section. Note : HTTP trigger must have a payload, otherwise it is pretty useless to send a request without event data. Payload Construction The http trigger payload has the following structure, payload: - src: dependencyName: test-dep dataKey: s3.bucket.name dest: bucket - src: dependencyName: test-dep contextKey: type dest: type This looks very similar to the parameter structure you have seen in previous sections for trigger parameterization. The src is the source of event. It contains, dependencyName : name of the event dependency to extract the event from. dataKey : to extract a particular key-value from event's data. contextKey : to extract a particular key-value from event' context. The dest is the destination key within the result payload. So, the above trigger payload will generate a request payload as, { bucket : value_of_the_bucket_name_extracted_from_event_data , type : value_of_the_event_type_extracted_from_event_context } Note : If you define both the contextKey and dataKey within a payload item, then the dataKey takes the precedence. You can create any payload structure you want. To get more info on how to generate complex event payloads, take a look at this library . The complete specification of HTTP trigger is available here . Serverless Workload Integration HTTP trigger provides an easy integration into 20+ event sources for the existing serverless workloads. In this section, we will look at how to invoke a Kubeless function. Prerequisite Kubeless must be installed :). You can follow the installation here . Make sure to deploy the test function. Invoke Function First, lets create an http trigger for kubeless function, kubeless trigger http create hello --function-name hello Update the sensor and update the serverURL to point to your Kubeless function URL. Drop a file onto test bucket. You will see the hello function getting invoked with following output, { event-time : None, extensions : { request : LocalRequest: POST http:// URL :8080/ }, event-type : None, event-namespace : None, data : { type : minio , bucket : test } , event-id : None} Note: The output was taken from Kubeless deployed on GCP. Policy To determine whether the request was successful or not, HTTP trigger provides a Status policy. The Status holds a list of response statuses that are considered valid. Lets update the sensor with acceptable response statuses as [ 200 , 300 ] and point the http trigger to an invalid server url. kubectl -n argo-events apply -f https://raw.githubusercontent.com/argoproj/argo-events/master/examples/tutorials/09-http-trigger/sensor-02.yaml Drop a file onto test bucket. Inspect the sensor logs, you will see the trigger resulted in failure. ERRO[2020-01-13 21:27:04] failed to operate on the event notification error= policy application resulted in failure. http response status 404 is not allowed Parameterization You can either use parameters within http trigger or parameter under the template to parameterize the trigger on fly.","title":"HTTP Trigger"},{"location":"tutorials/09-http-trigger/#http-trigger","text":"Sometimes you face a situation where creating an Argo workflow on every event is not an ideal solution. This is where the HTTP trigger can help you. With this type of trigger, you can connect any old/new API server with 20+ event sources supported by Argo Events or invoke serveless fucntions without worrying about their respective event connector frameworks.","title":"HTTP Trigger"},{"location":"tutorials/09-http-trigger/#prerequisite","text":"Set up the Minio gateway and event source. The K8s manifests are available under examples/tutorials/09-http-trigger .","title":"Prerequisite"},{"location":"tutorials/09-http-trigger/#api-server-integration","text":"We will set up a basic go http server and connect it with the minio events. Set up a simple http server that prints the request payload. kubectl -n argo-events apply -f https://raw.githubusercontent.com/argoproj/argo-events/master/examples/tutorials/09-http-trigger/http-server.yaml Create a service to expose the http server kubectl -n argo-events apply -f https://raw.githubusercontent.com/argoproj/argo-events/master/examples/tutorials/09-http-trigger/http-server-svc.yaml Either use Ingress, OpenShift Route or port-forwarding to expose the http server. We will use port-forwarding here. Create a sensor with HTTP trigger. We will discuss the trigger details in the following sections. kubectl -n argo-events apply -f https://raw.githubusercontent.com/argoproj/argo-events/master/examples/tutorials/09-http-trigger/sensor-01.yaml Now, drop a file onto test bucket in Minio server. The sensor has triggered a http request to out basic go http server. Take a look at the logs server is listening on 8090 { type : minio , bucket : test } Great!!! But how did the sensor constructed a payload for the http request? We will see that in next section. Note : HTTP trigger must have a payload, otherwise it is pretty useless to send a request without event data.","title":"API Server Integration"},{"location":"tutorials/09-http-trigger/#payload-construction","text":"The http trigger payload has the following structure, payload: - src: dependencyName: test-dep dataKey: s3.bucket.name dest: bucket - src: dependencyName: test-dep contextKey: type dest: type This looks very similar to the parameter structure you have seen in previous sections for trigger parameterization. The src is the source of event. It contains, dependencyName : name of the event dependency to extract the event from. dataKey : to extract a particular key-value from event's data. contextKey : to extract a particular key-value from event' context. The dest is the destination key within the result payload. So, the above trigger payload will generate a request payload as, { bucket : value_of_the_bucket_name_extracted_from_event_data , type : value_of_the_event_type_extracted_from_event_context } Note : If you define both the contextKey and dataKey within a payload item, then the dataKey takes the precedence. You can create any payload structure you want. To get more info on how to generate complex event payloads, take a look at this library . The complete specification of HTTP trigger is available here .","title":"Payload Construction"},{"location":"tutorials/09-http-trigger/#serverless-workload-integration","text":"HTTP trigger provides an easy integration into 20+ event sources for the existing serverless workloads. In this section, we will look at how to invoke a Kubeless function.","title":"Serverless Workload Integration"},{"location":"tutorials/09-http-trigger/#prerequisite_1","text":"Kubeless must be installed :). You can follow the installation here . Make sure to deploy the test function.","title":"Prerequisite"},{"location":"tutorials/09-http-trigger/#invoke-function","text":"First, lets create an http trigger for kubeless function, kubeless trigger http create hello --function-name hello Update the sensor and update the serverURL to point to your Kubeless function URL. Drop a file onto test bucket. You will see the hello function getting invoked with following output, { event-time : None, extensions : { request : LocalRequest: POST http:// URL :8080/ }, event-type : None, event-namespace : None, data : { type : minio , bucket : test } , event-id : None} Note: The output was taken from Kubeless deployed on GCP.","title":"Invoke Function"},{"location":"tutorials/09-http-trigger/#policy","text":"To determine whether the request was successful or not, HTTP trigger provides a Status policy. The Status holds a list of response statuses that are considered valid. Lets update the sensor with acceptable response statuses as [ 200 , 300 ] and point the http trigger to an invalid server url. kubectl -n argo-events apply -f https://raw.githubusercontent.com/argoproj/argo-events/master/examples/tutorials/09-http-trigger/sensor-02.yaml Drop a file onto test bucket. Inspect the sensor logs, you will see the trigger resulted in failure. ERRO[2020-01-13 21:27:04] failed to operate on the event notification error= policy application resulted in failure. http response status 404 is not allowed","title":"Policy"},{"location":"tutorials/09-http-trigger/#parameterization","text":"You can either use parameters within http trigger or parameter under the template to parameterize the trigger on fly.","title":"Parameterization"},{"location":"tutorials/10-aws-lambda-and-openfaas-trigger/","text":"AWS Lambda Trigger AWS Lambda provides a tremendous value but the event driven lambda invocation is limited to SNS, SQS and few other event sources. Argo Events makes it easy to integrate lambda with event sources that are not native to AWS. Prerequisite Set up the webhook gateway and event source. Create a K8s secret that holds your access and secret key. Make sure to store the base64 encoded keys in the secret. Create a basic lambda function that can parse following payload, { name : foo } Lambda Trigger The trigger specification is available here Note : You must declare the payload for the lambda trigger. Check out the HTTP trigger tutorial to understand how to construct the payload. Lets create a sensor with a lambda trigger kubectl -n argo-events apply -f https://raw.githubusercontent.com/argoproj/argo-events/master/examples/tutorials/10-aws-lambda-trigger/sensor.yaml Send a http request to webhook gateway, curl -d { name : foo } -H Content-Type: application/json -X POST http://localhost:12000/example You should see the lambda execution in CloudWatch logs. Policy To determine whether the function was successful or not, Lambda trigger provides a Status policy. The Status holds a list of response statuses that are considered valid. Parameterization Similar to HTTP trigger, the Lambda trigger provides parameters at both trigger resource and trigger template level. OpenFaas Trigger Similar to AWS lambda, you can trigger a OpenFaas function. The trigger specification is available here","title":"AWS Lambda Trigger"},{"location":"tutorials/10-aws-lambda-and-openfaas-trigger/#aws-lambda-trigger","text":"AWS Lambda provides a tremendous value but the event driven lambda invocation is limited to SNS, SQS and few other event sources. Argo Events makes it easy to integrate lambda with event sources that are not native to AWS.","title":"AWS Lambda Trigger"},{"location":"tutorials/10-aws-lambda-and-openfaas-trigger/#prerequisite","text":"Set up the webhook gateway and event source. Create a K8s secret that holds your access and secret key. Make sure to store the base64 encoded keys in the secret. Create a basic lambda function that can parse following payload, { name : foo }","title":"Prerequisite"},{"location":"tutorials/10-aws-lambda-and-openfaas-trigger/#lambda-trigger","text":"The trigger specification is available here Note : You must declare the payload for the lambda trigger. Check out the HTTP trigger tutorial to understand how to construct the payload. Lets create a sensor with a lambda trigger kubectl -n argo-events apply -f https://raw.githubusercontent.com/argoproj/argo-events/master/examples/tutorials/10-aws-lambda-trigger/sensor.yaml Send a http request to webhook gateway, curl -d { name : foo } -H Content-Type: application/json -X POST http://localhost:12000/example You should see the lambda execution in CloudWatch logs.","title":"Lambda Trigger"},{"location":"tutorials/10-aws-lambda-and-openfaas-trigger/#policy","text":"To determine whether the function was successful or not, Lambda trigger provides a Status policy. The Status holds a list of response statuses that are considered valid.","title":"Policy"},{"location":"tutorials/10-aws-lambda-and-openfaas-trigger/#parameterization","text":"Similar to HTTP trigger, the Lambda trigger provides parameters at both trigger resource and trigger template level.","title":"Parameterization"},{"location":"tutorials/10-aws-lambda-and-openfaas-trigger/#openfaas-trigger","text":"Similar to AWS lambda, you can trigger a OpenFaas function. The trigger specification is available here","title":"OpenFaas Trigger"},{"location":"tutorials/11-special-argo-workflow-trigger/","text":"Special Argo Workflow Trigger Although you can trigger an Argo workflow using a standard K8s trigger, the functionality provided by argo cli can't be leveraged in a standard K8s trigger. The special argo workflow trigger supports following operations for a workflow, Submit Resubmit Resume Retry Suspend The trigger specification is available here and the example is located under examples/sensors .","title":"Special Argo Workflow Trigger"},{"location":"tutorials/11-special-argo-workflow-trigger/#special-argo-workflow-trigger","text":"Although you can trigger an Argo workflow using a standard K8s trigger, the functionality provided by argo cli can't be leveraged in a standard K8s trigger. The special argo workflow trigger supports following operations for a workflow, Submit Resubmit Resume Retry Suspend The trigger specification is available here and the example is located under examples/sensors .","title":"Special Argo Workflow Trigger"}]}