{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Argo Events - The Event-driven Workflow Automation Framework \u00b6 What is Argo Events? \u00b6 Argo Events is an event-driven workflow automation framework for Kubernetes which helps you trigger K8s objects, Argo Workflows, Serverless workloads, etc. on events from variety of sources like webhook, s3, schedules, messaging queues, gcp pubsub, sns, sqs, etc. Features \u00b6 Supports events from 20+ event sources. Ability to customize business-level constraint logic for workflow automation. Manage everything from simple, linear, real-time to complex, multi-source events. Supports Kubernetes Objects, Argo Workflow, AWS Lambda, Serverless, etc. as triggers. CloudEvents compliant. Getting Started \u00b6 Follow these instruction to set up Argo Events. Documentation \u00b6 Concepts . Argo Events in action . Deploy gateways and sensors . Deep dive into Argo Events . Triggers \u00b6 Argo Workflows Standard K8s Objects HTTP Requests / Serverless Workloads (OpenFaas, Kubeless, KNative etc.) AWS Lambda NATS Messages Kafka Messages Slack Notifications Argo Rollouts Custom Trigger / Build Your Own Trigger Apache OpenWhisk Event Sources \u00b6 Argo-Events supports 20+ event sources. The complete list of event sources is available here . Who uses Argo Events? \u00b6 Organizations below are officially using Argo Events. Please send a PR with your organization name if you are using Argo Events. BioBox Analytics BlackRock Canva Fairwinds InsideBoard Intuit Viaduct Community Blogs and Presentations \u00b6 Automating Research Workflows at BlackRock Designing A Complete CI/CD Pipeline CI/CD Pipeline Using Argo Events, Workflows, and CD TGI Kubernetes with Joe Beda: CloudEvents and Argo Events","title":"Overview"},{"location":"#argo-events-the-event-driven-workflow-automation-framework","text":"","title":"Argo Events - The Event-driven Workflow Automation Framework"},{"location":"#what-is-argo-events","text":"Argo Events is an event-driven workflow automation framework for Kubernetes which helps you trigger K8s objects, Argo Workflows, Serverless workloads, etc. on events from variety of sources like webhook, s3, schedules, messaging queues, gcp pubsub, sns, sqs, etc.","title":"What is Argo Events?"},{"location":"#features","text":"Supports events from 20+ event sources. Ability to customize business-level constraint logic for workflow automation. Manage everything from simple, linear, real-time to complex, multi-source events. Supports Kubernetes Objects, Argo Workflow, AWS Lambda, Serverless, etc. as triggers. CloudEvents compliant.","title":"Features"},{"location":"#getting-started","text":"Follow these instruction to set up Argo Events.","title":"Getting Started"},{"location":"#documentation","text":"Concepts . Argo Events in action . Deploy gateways and sensors . Deep dive into Argo Events .","title":"Documentation"},{"location":"#triggers","text":"Argo Workflows Standard K8s Objects HTTP Requests / Serverless Workloads (OpenFaas, Kubeless, KNative etc.) AWS Lambda NATS Messages Kafka Messages Slack Notifications Argo Rollouts Custom Trigger / Build Your Own Trigger Apache OpenWhisk","title":"Triggers"},{"location":"#event-sources","text":"Argo-Events supports 20+ event sources. The complete list of event sources is available here .","title":"Event Sources"},{"location":"#who-uses-argo-events","text":"Organizations below are officially using Argo Events. Please send a PR with your organization name if you are using Argo Events. BioBox Analytics BlackRock Canva Fairwinds InsideBoard Intuit Viaduct","title":"Who uses Argo Events?"},{"location":"#community-blogs-and-presentations","text":"Automating Research Workflows at BlackRock Designing A Complete CI/CD Pipeline CI/CD Pipeline Using Argo Events, Workflows, and CD TGI Kubernetes with Joe Beda: CloudEvents and Argo Events","title":"Community Blogs and Presentations"},{"location":"FAQ/","text":"FAQs \u00b6 Q. How to get started with Argo Events? A . Recommended way to get started with Argo Events is, Read the basic concepts about Gateway , Sensor and Event Source . Install the setup as outlined here . Read the tutorials available here . Q. Can I deploy gateway and sensor in a namespace different that argo-events ? A . Yes. If you want to deploy the gateway in a different namespace that argo-events , then please update the gateway definition with desired namespace and service account. Make sure to grant the service account the necessary roles. Also note that the gateway and sensor controllers are configured to process the gateway and sensor resources in argo-events namespace with instance-id argo-events . You can change the configuration by updating the appropriate controller configmap. Q. How to debug Argo-Events. A . Make sure you have installed everything as instructed here . The gateway and sensor pods must be running. If you see any issue with the pods, check the logs for sensor-controller and gateway-controller. If gateway and sensor pods are running, but you are not receiving any events: Make sure you have configured the event source correctly. Check the logs for both of the gateway pod's containers. If the gateway-client displays dispatched event but nothing happens then read following Q and A. Note: You can set environment variable DEBUG_LOG:true in any of the containers to output debug logs. Q. Gateway is receiving the events but nothing happens. A . Check the sensor resource is deployed and a pod is created for the resource. If sensor pod is running, check the subscribers list in the gateway resource. The sensor service url must be registered as a subscriber in order to receive events from gateway. The gateway-client container should also log an error related to this situation. If the gateway was able to send an event to sensor, then check the sensor logs, either the sensor event resolution circuitry has rejected the event or the sensor failed to execute the trigger due to an error. Q. Helm chart installation does not work. A. The helm chart for argo events is maintained by the community and can be out of sync with latest release version. The official installation file is available here . If you notice the helm chart is outdated, we encourage you to contribute to the argo-helm . Q. Kustomization file doesn't have a X resource. A. The kustomization.yaml file is maintained by the community. If you notice that it is out of sync with the official installation file, please raise a PR. Q. Can I use Minio gateway for AWS S3 notifications? A. No. Minio gateway is exclusive for the Minio server. If you want to trigger workloads on AWS S3 bucket notification, then set up the AWS SNS gateway. Q. If I have multiple event dependencies and triggers in a single sensor, can I execute a specific trigger upon a specific event? A. Yes, this is precisely the functionality the sensor event resolution circuitry offers. Please take a look at the Circuit and Switch . Q. The latest image tag does not point to latest release tag? A. When it comes to image tags, the golden rule is do not trust the latest tag. Always use the pinned version of the images. We will try to keep the latest in sync with the latest release version. Q. Where can I find the event structure for a particular gateway? A. Please refer this file to understand the structure of different types of events dispatched by gateways.","title":"FAQs"},{"location":"FAQ/#faqs","text":"Q. How to get started with Argo Events? A . Recommended way to get started with Argo Events is, Read the basic concepts about Gateway , Sensor and Event Source . Install the setup as outlined here . Read the tutorials available here . Q. Can I deploy gateway and sensor in a namespace different that argo-events ? A . Yes. If you want to deploy the gateway in a different namespace that argo-events , then please update the gateway definition with desired namespace and service account. Make sure to grant the service account the necessary roles. Also note that the gateway and sensor controllers are configured to process the gateway and sensor resources in argo-events namespace with instance-id argo-events . You can change the configuration by updating the appropriate controller configmap. Q. How to debug Argo-Events. A . Make sure you have installed everything as instructed here . The gateway and sensor pods must be running. If you see any issue with the pods, check the logs for sensor-controller and gateway-controller. If gateway and sensor pods are running, but you are not receiving any events: Make sure you have configured the event source correctly. Check the logs for both of the gateway pod's containers. If the gateway-client displays dispatched event but nothing happens then read following Q and A. Note: You can set environment variable DEBUG_LOG:true in any of the containers to output debug logs. Q. Gateway is receiving the events but nothing happens. A . Check the sensor resource is deployed and a pod is created for the resource. If sensor pod is running, check the subscribers list in the gateway resource. The sensor service url must be registered as a subscriber in order to receive events from gateway. The gateway-client container should also log an error related to this situation. If the gateway was able to send an event to sensor, then check the sensor logs, either the sensor event resolution circuitry has rejected the event or the sensor failed to execute the trigger due to an error. Q. Helm chart installation does not work. A. The helm chart for argo events is maintained by the community and can be out of sync with latest release version. The official installation file is available here . If you notice the helm chart is outdated, we encourage you to contribute to the argo-helm . Q. Kustomization file doesn't have a X resource. A. The kustomization.yaml file is maintained by the community. If you notice that it is out of sync with the official installation file, please raise a PR. Q. Can I use Minio gateway for AWS S3 notifications? A. No. Minio gateway is exclusive for the Minio server. If you want to trigger workloads on AWS S3 bucket notification, then set up the AWS SNS gateway. Q. If I have multiple event dependencies and triggers in a single sensor, can I execute a specific trigger upon a specific event? A. Yes, this is precisely the functionality the sensor event resolution circuitry offers. Please take a look at the Circuit and Switch . Q. The latest image tag does not point to latest release tag? A. When it comes to image tags, the golden rule is do not trust the latest tag. Always use the pinned version of the images. We will try to keep the latest in sync with the latest release version. Q. Where can I find the event structure for a particular gateway? A. Please refer this file to understand the structure of different types of events dispatched by gateways.","title":"FAQs"},{"location":"controllers/","text":"Controllers \u00b6 Sensor and Gateway controllers are the components which manage Sensor and Gateway objects respectively. Sensor and Gateway are Kubernetes Custom Resources. For more information on K8 CRDs visit here. Controller Configmap \u00b6 Defines the instance-id and the namespace for the controller. e.g. # The gateway-controller configmap includes configuration information for the gateway-controller apiVersion : v1 kind : ConfigMap metadata : name : gateway-controller-configmap data : config : | instanceID: argo-events # mandatory namespace: my-custom-namespace # optional namespace : If you don't provide namespace, controller will watch all namespaces for gateway resource. instanceID : it is used to map a gateway or sensor object to a controller. e.g. when you create a gateway with label gateways.argoproj.io/gateway-controller-instanceid: argo-events , a controller with label argo-events will process that gateway. instanceID is used to horizontally scale controllers, so you won't end up overwhelming a single controller with large number of gateways or sensors. Also keep in mind that instanceID has nothing to do with namespace where you are deploying controllers and gateways/sensors objects.","title":"Controllers"},{"location":"controllers/#controllers","text":"Sensor and Gateway controllers are the components which manage Sensor and Gateway objects respectively. Sensor and Gateway are Kubernetes Custom Resources. For more information on K8 CRDs visit here.","title":"Controllers"},{"location":"controllers/#controller-configmap","text":"Defines the instance-id and the namespace for the controller. e.g. # The gateway-controller configmap includes configuration information for the gateway-controller apiVersion : v1 kind : ConfigMap metadata : name : gateway-controller-configmap data : config : | instanceID: argo-events # mandatory namespace: my-custom-namespace # optional namespace : If you don't provide namespace, controller will watch all namespaces for gateway resource. instanceID : it is used to map a gateway or sensor object to a controller. e.g. when you create a gateway with label gateways.argoproj.io/gateway-controller-instanceid: argo-events , a controller with label argo-events will process that gateway. instanceID is used to horizontally scale controllers, so you won't end up overwhelming a single controller with large number of gateways or sensors. Also keep in mind that instanceID has nothing to do with namespace where you are deploying controllers and gateways/sensors objects.","title":"Controller Configmap"},{"location":"developer_guide/","text":"Developer Guide \u00b6 Setup your DEV environment \u00b6 Argo Events is native to Kubernetes so you'll need a running Kubernetes cluster. This guide includes steps for Minikube for local development, but if you have another cluster you can ignore the Minikube specific step 3. Requirements \u00b6 Golang 1.11 Docker dep Installation & Setup \u00b6 1. Get the project \u00b6 go get github . com / argoproj / argo - events cd $ GOPATH / src / github . com / argoproj / argo - events 2. Vendor dependencies \u00b6 dep ensure - vendor - only 3. Start Minikube and point Docker Client to Minikube's Docker Daemon \u00b6 minikube start eval $ ( minikube docker - env ) 5. Build the project \u00b6 make all Changing Types \u00b6 If you're making a change to the pkg/apis package, please ensure you re-run the K8 code-generator scripts found in the /hack folder. First, ensure you have the generate-groups.sh script at the path: vendor/k8s.io/code-generator/ . Next run the following commands in order: $ make codegen How to write a custom gateway? \u00b6 To implement a custom gateway, you need to create a gRPC server and implement the service defined below. The framework code acts as a gRPC client consuming event stream from gateway server. Proto Definition \u00b6 The proto file is located here If you choose to implement the gateway in Go , then you can find generated client stubs here To create stubs in other languages, head over to gRPC website Service, /** * Service for handling event sources. */ service Eventing { // StartEventSource starts an event source and returns stream of events . rpc StartEventSource ( EventSource ) returns ( stream Event ) ; // ValidateEventSource validates an event source . rpc ValidateEventSource ( EventSource ) returns ( ValidEventSource ) ; } Available Environment Variables to Server \u00b6 Field Description GATEWAY_NAMESPACE K8s namespace of the gateway GATEWAY_EVENT_SOURCE_CONFIG_MAP K8s configmap containing event source GATEWAY_NAME name of the gateway GATEWAY_CONTROLLER_INSTANCE_ID gateway controller instance id GATEWAY_CONTROLLER_NAME gateway controller name GATEWAY_SERVER_PORT Port on which the gateway gRPC server should run","title":"Developer Guide"},{"location":"developer_guide/#developer-guide","text":"","title":"Developer Guide"},{"location":"developer_guide/#setup-your-dev-environment","text":"Argo Events is native to Kubernetes so you'll need a running Kubernetes cluster. This guide includes steps for Minikube for local development, but if you have another cluster you can ignore the Minikube specific step 3.","title":"Setup your DEV environment"},{"location":"developer_guide/#requirements","text":"Golang 1.11 Docker dep","title":"Requirements"},{"location":"developer_guide/#installation-setup","text":"","title":"Installation &amp; Setup"},{"location":"developer_guide/#1-get-the-project","text":"go get github . com / argoproj / argo - events cd $ GOPATH / src / github . com / argoproj / argo - events","title":"1. Get the project"},{"location":"developer_guide/#2-vendor-dependencies","text":"dep ensure - vendor - only","title":"2. Vendor dependencies"},{"location":"developer_guide/#3-start-minikube-and-point-docker-client-to-minikubes-docker-daemon","text":"minikube start eval $ ( minikube docker - env )","title":"3. Start Minikube and point Docker Client to Minikube's Docker Daemon"},{"location":"developer_guide/#5-build-the-project","text":"make all","title":"5. Build the project"},{"location":"developer_guide/#changing-types","text":"If you're making a change to the pkg/apis package, please ensure you re-run the K8 code-generator scripts found in the /hack folder. First, ensure you have the generate-groups.sh script at the path: vendor/k8s.io/code-generator/ . Next run the following commands in order: $ make codegen","title":"Changing Types"},{"location":"developer_guide/#how-to-write-a-custom-gateway","text":"To implement a custom gateway, you need to create a gRPC server and implement the service defined below. The framework code acts as a gRPC client consuming event stream from gateway server.","title":"How to write a custom gateway?"},{"location":"developer_guide/#proto-definition","text":"The proto file is located here If you choose to implement the gateway in Go , then you can find generated client stubs here To create stubs in other languages, head over to gRPC website Service, /** * Service for handling event sources. */ service Eventing { // StartEventSource starts an event source and returns stream of events . rpc StartEventSource ( EventSource ) returns ( stream Event ) ; // ValidateEventSource validates an event source . rpc ValidateEventSource ( EventSource ) returns ( ValidEventSource ) ; }","title":"Proto Definition"},{"location":"developer_guide/#available-environment-variables-to-server","text":"Field Description GATEWAY_NAMESPACE K8s namespace of the gateway GATEWAY_EVENT_SOURCE_CONFIG_MAP K8s configmap containing event source GATEWAY_NAME name of the gateway GATEWAY_CONTROLLER_INSTANCE_ID gateway controller instance id GATEWAY_CONTROLLER_NAME gateway controller name GATEWAY_SERVER_PORT Port on which the gateway gRPC server should run","title":"Available Environment Variables to Server"},{"location":"installation/","text":"Installation \u00b6 Requirements \u00b6 Kubernetes cluster >v1.9 Installed the kubectl command-line tool >v1.9.0 Using kubectl \u00b6 One Command Installation \u00b6 Deploy Argo Events Namespace, SA, Roles, ConfigMap, Sensor Controller and Gateway Controller kubectl apply - f https : // raw . githubusercontent . com / argoproj / argo - events / master / hack / k8s / manifests / installation . yaml NOTE: On GKE, you may need to grant your account the ability to create new clusterroles kubectl create clusterrolebinding YOURNAME - cluster - admin - binding --clusterrole=cluster-admin --user=YOUREMAIL@gmail.com Step-by-Step Installation \u00b6 Create the namespace kubectl create namespace argo - events Create the service account kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / hack / k8s / manifests / argo - events - sa . yaml Create the role and rolebinding kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / hack / k8s / manifests / argo - events - role . yaml Install the sensor custom resource definition kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / hack / k8s / manifests / sensor - crd . yaml Install the gateway custom resource definition kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / hack / k8s / manifests / gateway - crd . yaml Install the event source custom resource definition kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / hack / k8s / manifests / event - source - crd . yaml Create the confimap for sensor controller kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / hack / k8s / manifests / sensor - controller - configmap . yaml Create the configmap for gateway controller kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / hack / k8s / manifests / gateway - controller - configmap . yaml Deploy the sensor controller kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / hack / k8s / manifests / sensor - controller - deployment . yaml Deploy the gateway controller kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / hack / k8s / manifests / gateway - controller - deployment . yaml Using Helm Chart \u00b6 Note: This method does not work with Helm 3, only Helm 2. Make sure you have helm client installed and Tiller server is running. To install helm, follow the link. Create namespace called argo-events. Add argoproj repository helm repo add argo https : // argoproj . github . io / argo - helm The helm chart for argo-events is maintained solely by the community and hence the image version for controllers can go out of sync. Update the image version in values.yaml to v0.14.0. Install argo-events chart helm install argo - events argo / argo - events Deploy at cluster level \u00b6 To deploy Argo-Events controllers at cluster level where the controllers will be able to process gateway and sensor objects created in any namespace, Make sure to apply cluster role and binding to the service account, kubectl apply - f https : // raw . githubusercontent . com / argoproj / argo - events / master / hack / k8s / manifests / argo - events - cluster - roles . yaml Update the configmap for both gateway and sensor and remove the namespace key from it. Deploy both gateway and sensor controllers and watch the magic.","title":"Installation"},{"location":"installation/#installation","text":"","title":"Installation"},{"location":"installation/#requirements","text":"Kubernetes cluster >v1.9 Installed the kubectl command-line tool >v1.9.0","title":"Requirements"},{"location":"installation/#using-kubectl","text":"","title":"Using kubectl"},{"location":"installation/#one-command-installation","text":"Deploy Argo Events Namespace, SA, Roles, ConfigMap, Sensor Controller and Gateway Controller kubectl apply - f https : // raw . githubusercontent . com / argoproj / argo - events / master / hack / k8s / manifests / installation . yaml NOTE: On GKE, you may need to grant your account the ability to create new clusterroles kubectl create clusterrolebinding YOURNAME - cluster - admin - binding --clusterrole=cluster-admin --user=YOUREMAIL@gmail.com","title":"One Command Installation"},{"location":"installation/#step-by-step-installation","text":"Create the namespace kubectl create namespace argo - events Create the service account kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / hack / k8s / manifests / argo - events - sa . yaml Create the role and rolebinding kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / hack / k8s / manifests / argo - events - role . yaml Install the sensor custom resource definition kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / hack / k8s / manifests / sensor - crd . yaml Install the gateway custom resource definition kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / hack / k8s / manifests / gateway - crd . yaml Install the event source custom resource definition kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / hack / k8s / manifests / event - source - crd . yaml Create the confimap for sensor controller kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / hack / k8s / manifests / sensor - controller - configmap . yaml Create the configmap for gateway controller kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / hack / k8s / manifests / gateway - controller - configmap . yaml Deploy the sensor controller kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / hack / k8s / manifests / sensor - controller - deployment . yaml Deploy the gateway controller kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / hack / k8s / manifests / gateway - controller - deployment . yaml","title":"Step-by-Step Installation"},{"location":"installation/#using-helm-chart","text":"Note: This method does not work with Helm 3, only Helm 2. Make sure you have helm client installed and Tiller server is running. To install helm, follow the link. Create namespace called argo-events. Add argoproj repository helm repo add argo https : // argoproj . github . io / argo - helm The helm chart for argo-events is maintained solely by the community and hence the image version for controllers can go out of sync. Update the image version in values.yaml to v0.14.0. Install argo-events chart helm install argo - events argo / argo - events","title":"Using Helm Chart"},{"location":"installation/#deploy-at-cluster-level","text":"To deploy Argo-Events controllers at cluster level where the controllers will be able to process gateway and sensor objects created in any namespace, Make sure to apply cluster role and binding to the service account, kubectl apply - f https : // raw . githubusercontent . com / argoproj / argo - events / master / hack / k8s / manifests / argo - events - cluster - roles . yaml Update the configmap for both gateway and sensor and remove the namespace key from it. Deploy both gateway and sensor controllers and watch the magic.","title":"Deploy at cluster level"},{"location":"quick_start/","text":"Getting Started \u00b6 We are going to set up a gateway, sensor and event-source for webhook. The goal is to trigger an Argo workflow upon a HTTP Post request. Note: You will need to have Argo Workflows installed to make this work. First, we need to setup event sources for gateway to listen. kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / event - sources / webhook . yaml The event-source drives the configuration required for a gateway to consume events from external sources. Create webhook gateway, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / gateways / webhook . yaml After running above command, gateway controller will create corresponding a pod and service. Create webhook sensor, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / sensors / webhook . yaml Once sensor object is created, sensor controller will create corresponding pod and service. Expose the gateway pod via Ingress, OpenShift Route or port forward to consume requests over HTTP. kubectl - n argo - events port - forward < gateway - pod - name > 12000 : 12000 Use either Curl or Postman to send a post request to the http://localhost:12000/example curl - d '{\"message\":\"this is my first webhook\"}' - H \"Content-Type: application/json\" - X POST http : // localhost : 12000 / example Verify that an Argo workflow was triggered. kubectl - n argo - events get workflows | grep \"webhook\"","title":"Getting Started"},{"location":"quick_start/#getting-started","text":"We are going to set up a gateway, sensor and event-source for webhook. The goal is to trigger an Argo workflow upon a HTTP Post request. Note: You will need to have Argo Workflows installed to make this work. First, we need to setup event sources for gateway to listen. kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / event - sources / webhook . yaml The event-source drives the configuration required for a gateway to consume events from external sources. Create webhook gateway, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / gateways / webhook . yaml After running above command, gateway controller will create corresponding a pod and service. Create webhook sensor, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / sensors / webhook . yaml Once sensor object is created, sensor controller will create corresponding pod and service. Expose the gateway pod via Ingress, OpenShift Route or port forward to consume requests over HTTP. kubectl - n argo - events port - forward < gateway - pod - name > 12000 : 12000 Use either Curl or Postman to send a post request to the http://localhost:12000/example curl - d '{\"message\":\"this is my first webhook\"}' - H \"Content-Type: application/json\" - X POST http : // localhost : 12000 / example Verify that an Argo workflow was triggered. kubectl - n argo - events get workflows | grep \"webhook\"","title":"Getting Started"},{"location":"concepts/architecture/","text":"Architecture \u00b6 Main components of Argo Events are: Gateway Sensor Event Source Trigger","title":"Architecture"},{"location":"concepts/architecture/#architecture","text":"Main components of Argo Events are: Gateway Sensor Event Source Trigger","title":"Architecture"},{"location":"concepts/event_source/","text":"Event Source \u00b6 Event Sources are the configuration store for gateways. The configuration stored in an Event Source is used by a gateway to consume events from external entities like AWS SNS, SQS, GCP PubSub, Webhooks etc. Currently supported event sources - AMQP AWS SNS AWS SQS Cron Schedules GCP PubSub GitHub GitLab HDFS File Based Events Kafka Minio NATS MQTT K8s Resources Slack NetApp StorageGrid Webhooks Stripe NSQ Emitter Redis Azure Events Hub Specification \u00b6 Complete specification is available here . Examples \u00b6 Examples are located under examples/event-sources .","title":"Event Source"},{"location":"concepts/event_source/#event-source","text":"Event Sources are the configuration store for gateways. The configuration stored in an Event Source is used by a gateway to consume events from external entities like AWS SNS, SQS, GCP PubSub, Webhooks etc. Currently supported event sources - AMQP AWS SNS AWS SQS Cron Schedules GCP PubSub GitHub GitLab HDFS File Based Events Kafka Minio NATS MQTT K8s Resources Slack NetApp StorageGrid Webhooks Stripe NSQ Emitter Redis Azure Events Hub","title":"Event Source"},{"location":"concepts/event_source/#specification","text":"Complete specification is available here .","title":"Specification"},{"location":"concepts/event_source/#examples","text":"Examples are located under examples/event-sources .","title":"Examples"},{"location":"concepts/gateway/","text":"Gateway \u00b6 What is a gateway? \u00b6 A gateway consumes events from outside entities, transforms them into the cloudevents specification compliant events and dispatches them to sensors. There are two components for a gateway, Gateway Client \u00b6 Gateway client manages the event source for the gateway. Its responsibilities are, Monitor and manage the event sources. Monitor and manage the subscribers. Convert the events received from the gateway server into CloudEvents. Dispatch the cloudevents to subscribers. Gateway Server \u00b6 Gateway server listens to events from event sources. Its responsibilities are, Validate an event source. Implement the logic for consuming events from an event source. Dispatch events to gateway client. Gateway & Event Source \u00b6 Event Source are event configuration store for a gateway. The configuration stored in an Event Source is used by a gateway to consume events from external entities like AWS SNS, SQS, GCP PubSub, Webhooks etc. Specification \u00b6 Complete specification is available here . Examples \u00b6 Examples are located under examples/gateways .","title":"Gateway"},{"location":"concepts/gateway/#gateway","text":"","title":"Gateway"},{"location":"concepts/gateway/#what-is-a-gateway","text":"A gateway consumes events from outside entities, transforms them into the cloudevents specification compliant events and dispatches them to sensors. There are two components for a gateway,","title":"What is a gateway?"},{"location":"concepts/gateway/#gateway-client","text":"Gateway client manages the event source for the gateway. Its responsibilities are, Monitor and manage the event sources. Monitor and manage the subscribers. Convert the events received from the gateway server into CloudEvents. Dispatch the cloudevents to subscribers.","title":"Gateway Client"},{"location":"concepts/gateway/#gateway-server","text":"Gateway server listens to events from event sources. Its responsibilities are, Validate an event source. Implement the logic for consuming events from an event source. Dispatch events to gateway client.","title":"Gateway Server"},{"location":"concepts/gateway/#gateway-event-source","text":"Event Source are event configuration store for a gateway. The configuration stored in an Event Source is used by a gateway to consume events from external entities like AWS SNS, SQS, GCP PubSub, Webhooks etc.","title":"Gateway &amp; Event Source"},{"location":"concepts/gateway/#specification","text":"Complete specification is available here .","title":"Specification"},{"location":"concepts/gateway/#examples","text":"Examples are located under examples/gateways .","title":"Examples"},{"location":"concepts/sensor/","text":"Sensor \u00b6 Sensor defines a set of event dependencies (inputs) and triggers (outputs). It listens to events from one or more gateways and act as an event dependency manager. Event dependency \u00b6 A dependency is an event the sensor is waiting to happen. Specification \u00b6 Complete specification is available here . Examples \u00b6 Examples are located under examples/sensors .","title":"Sensor"},{"location":"concepts/sensor/#sensor","text":"Sensor defines a set of event dependencies (inputs) and triggers (outputs). It listens to events from one or more gateways and act as an event dependency manager.","title":"Sensor"},{"location":"concepts/sensor/#event-dependency","text":"A dependency is an event the sensor is waiting to happen.","title":"Event dependency"},{"location":"concepts/sensor/#specification","text":"Complete specification is available here .","title":"Specification"},{"location":"concepts/sensor/#examples","text":"Examples are located under examples/sensors .","title":"Examples"},{"location":"concepts/trigger/","text":"Trigger \u00b6 A Trigger is the resource/workload executed by the sensor once the event dependencies are resolved. Trigger Types \u00b6 Argo Workflows Standard K8s Objects HTTP Requests AWS Lambda NATS Messages Kafka Messages Slack Notifications Argo Rollouts CR Custom / Build Your Own Triggers Apache OpenWhisk","title":"Trigger"},{"location":"concepts/trigger/#trigger","text":"A Trigger is the resource/workload executed by the sensor once the event dependencies are resolved.","title":"Trigger"},{"location":"concepts/trigger/#trigger-types","text":"Argo Workflows Standard K8s Objects HTTP Requests AWS Lambda NATS Messages Kafka Messages Slack Notifications Argo Rollouts CR Custom / Build Your Own Triggers Apache OpenWhisk","title":"Trigger Types"},{"location":"demo/notebooks/","text":"Event-Driven Parameterized Jupyter Notebooks \u00b6 Jupyter notebooks are prevalent in data science community to develop models, run analysis and generate reports, etc. But in many situations, a data scientist must feed varying parameters to tune the notebook to generate an optimal model. Tools like papermill makes it easy to parameterize the notebook and Argo Events makes it super easy to set up event-driven parameterized notebooks. Prerequisites \u00b6 Install Argo Workflows . Install Argo Events . Install NATS, kubectl - n argo - events apply - f https : // raw . githubusercontent . com / VaibhavPage / argo - events - demo / master / nats - deploy . yaml Port forward to NATS pod, kubectl - n argo - events port - forward < nats - pod - name > 4222 : 4222 Install Minio, kubectl - n argo - events apply - f https : // raw . githubusercontent . com / VaibhavPage / argo - events - demo / master / artifact - minio . yaml kubectl - n argo - events apply - f https : // raw . githubusercontent . com / VaibhavPage / argo - events - demo / master / minio - deploy . yaml Port forward to Minio pod, kubectl - n argo - events port - forward < minio - pod - name > 9000 : 9000 Setup \u00b6 In this demo, we are going to set up an image processing pipeline using 2 notebooks. Lets consider the ArgoProj icon image The first notebook will take the clean ArgoProj logo and add noise to it. The second notebook is going to determine the similarity between clean image and the image with noise. If the match is > 80%, then model is optimal, else we need to tune the noise parameters. We will set up two gateways, Webhook and Minio. The webhook gateway will listen to HTTP requests to tune the notebook to add noise to image. The notebook will store the noisy image to Minio. The minio gateway will listen to file drop events for a specific bucket. Once the noisy image is dropped into that bucket, we will run the second notebook that determines the similarity of images. Create webhook event source. It consist configuration for gateway to listen for HTTP POST requests on port 12000. kubectl - n argo - events apply - f https : // raw . githubusercontent . com / VaibhavPage / argo - events - demo / master / webhook - event - source . yaml Create webhook gateway, kubectl - n argo - events apply - f https : // raw . githubusercontent . com / VaibhavPage / argo - events - demo / master / webhook - gateway . yaml Port forward to webhook gateway pod, kubectl - n argo - events port - forward < webhook - gateway - pod - name > 12000 : 12000 Create webhook sensor, kubectl - n argo - events apply - f https : // raw . githubusercontent . com / VaibhavPage / argo - events - demo / master / webhook - sensor . yaml Lets inspect webhook sensor, apiVersion : argoproj . io / v1alpha1 kind : Sensor metadata : name : webhook - sensor labels : sensors . argoproj . io / sensor - controller - instanceid : argo - events spec : template : spec : containers : - name : sensor image : argoproj / sensor : v0 .13.0 - rc imagePullPolicy : Always serviceAccountName : argo - events - sa dependencies : - name : test - dep gatewayName : webhook - gateway eventName : example subscription : http : port : 9300 triggers : - template : name : webhook - workflow - trigger k8s : group : argoproj . io version : v1alpha1 resource : workflows operation : create source : resource : apiVersion : argoproj . io / v1alpha1 kind : Workflow metadata : generateName : noisy - processor - spec : entrypoint : noisy arguments : parameters : - name : filterA value : \"5\" - name : filterB value : \"5\" - name : sVSp value : \"0.5\" - name : amount value : \"0.004\" templates : - name : noisy serviceAccountName : argo - events - sa inputs : parameters : - name : filterA - name : filterB - name : sVSp - name : amount container : image : metalgearsolid / demo - blur - argo - logo : latest command : [ papermill ] imagePullPolicy : Always env : - name : AWS_ACCESS_KEY_ID value : minio - name : AWS_SECRET_ACCESS_KEY value : minio123 - name : AWS_DEFAULT_REGION value : us - east - 1 - name : BOTO3_ENDPOINT_URL value : http : // minio - service . argo - events . svc : 9000 args : - \"noise.ipynb\" - \"s3://output/noisy-out.ipynb\" - \"-p\" - \"filterA\" - \"{{inputs.parameters.filterA}}\" - \"-p\" - \"filterB\" - \"{{inputs.parameters.filterB}}\" - \"-p\" - \"sVSp\" - \"{{inputs.parameters.sVSp}}\" - \"-p\" - \"amount\" - \"{{inputs.parameters.amount}}\" parameters : - src : dependencyName : test - dep dataKey : body . filterA dest : spec . arguments . parameters .0 . value - src : dependencyName : test - dep dataKey : body . filterB dest : spec . arguments . parameters .1 . value - src : dependencyName : test - dep dataKey : body . sVSp dest : spec . arguments . parameters .2 . value - src : dependencyName : test - dep dataKey : body . amount dest : spec . arguments . parameters .3 . value The sensor trigger is an Argo workflow that runs a jupyter notebook with papermill. It takes arguments for Guassian filter and Slat+Pepper noise in addition to S3 configuration. The event data received from HTTP POST request is made to override the arguments to workflow on the fly. Lets configurre Minio client mc, mc config host add minio http : // localhost : 9000 minio minio123 Create a bucket on Minio called output . mc mb minio / output Create the Minio event source that makes the gateway listen to file events for output bucket, kubectl - n argo - events apply - f https : // raw . githubusercontent . com / VaibhavPage / argo - events - demo / master / minio - event - source . yaml Create Minio gateway kubectl - n argo - events apply - f https : // raw . githubusercontent . com / VaibhavPage / argo - events - demo / master / minio - gateway . yaml Create Minio sensor, kubectl - n argo - events apply - f https : // raw . githubusercontent . com / VaibhavPage / argo - events - demo / master / minio - sensor . yaml The Minio sensor triggers an Argo workflow that determines the similarity index of the image that was put onto noisy-bucket on Minio and the original Argo logo. If the match is less than 80%, it publishes failure message on NATS subject called image-match Run a NATS subject subscriber in a separate terminal, go get github . com / nats - io / nats . go / cd examples / nats - sub go run main . go - s localhost : 4222 image - match Now, its time to send a HTTP request to parameterize the notebook that add noise to original Argo logo and execute the image processing pipeline. curl - d '{\"filterA\":\"15\", \"filterB\": \"15\", \"sVSp\": \"0.003\", \"amount\": \"0.010\"}' - H \"Content-Type: application/json\" - X POST http : // localhost : 12000 / example List argo workflows to list the noise-processor- and matcher-workflow- argo list Check the noisy image in bucket output . As soon as the matcher-workflow- completes, you will see a message on NATS subject ` [ # 1 ] Received on [ image - match ]: 'FAILURE: 0.4500723255991941' ` Lets change the parameters for the curl request to get >80% match, curl - d '{\"filterA\":\"5\", \"filterB\": \"5\", \"sVSp\": \"0.008\", \"amount\": \"0.0008\"}' - H \"Content-Type: application/json\" - X POST http : // localhost : 12000 / example Still only 51% match, [ # 2 ] Received on [ image - match ]: 'FAILURE: 0.519408012194793' Lets reduce the amount of noise to 0.0008, curl - d '{\"filterA\":\"5\", \"filterB\": \"5\", \"sVSp\": \"0.008\", \"amount\": \"0.0008\"}' - H \"Content-Type: application/json\" - X POST http : // localhost : 12000 / example You will see a success message, [ # 4 ] Received on [ image - match ]: 'SUCCESS: 0.9157008012270712' This was a simple image processing pipeline using Argo Events. You can easily set up CI pipelines, Machine Learning pipelines etc, using Argo Events and Argo Workflows.","title":"Event-Driven Parameterized Jupyter Notebooks"},{"location":"demo/notebooks/#event-driven-parameterized-jupyter-notebooks","text":"Jupyter notebooks are prevalent in data science community to develop models, run analysis and generate reports, etc. But in many situations, a data scientist must feed varying parameters to tune the notebook to generate an optimal model. Tools like papermill makes it easy to parameterize the notebook and Argo Events makes it super easy to set up event-driven parameterized notebooks.","title":"Event-Driven Parameterized Jupyter Notebooks"},{"location":"demo/notebooks/#prerequisites","text":"Install Argo Workflows . Install Argo Events . Install NATS, kubectl - n argo - events apply - f https : // raw . githubusercontent . com / VaibhavPage / argo - events - demo / master / nats - deploy . yaml Port forward to NATS pod, kubectl - n argo - events port - forward < nats - pod - name > 4222 : 4222 Install Minio, kubectl - n argo - events apply - f https : // raw . githubusercontent . com / VaibhavPage / argo - events - demo / master / artifact - minio . yaml kubectl - n argo - events apply - f https : // raw . githubusercontent . com / VaibhavPage / argo - events - demo / master / minio - deploy . yaml Port forward to Minio pod, kubectl - n argo - events port - forward < minio - pod - name > 9000 : 9000","title":"Prerequisites"},{"location":"demo/notebooks/#setup","text":"In this demo, we are going to set up an image processing pipeline using 2 notebooks. Lets consider the ArgoProj icon image The first notebook will take the clean ArgoProj logo and add noise to it. The second notebook is going to determine the similarity between clean image and the image with noise. If the match is > 80%, then model is optimal, else we need to tune the noise parameters. We will set up two gateways, Webhook and Minio. The webhook gateway will listen to HTTP requests to tune the notebook to add noise to image. The notebook will store the noisy image to Minio. The minio gateway will listen to file drop events for a specific bucket. Once the noisy image is dropped into that bucket, we will run the second notebook that determines the similarity of images. Create webhook event source. It consist configuration for gateway to listen for HTTP POST requests on port 12000. kubectl - n argo - events apply - f https : // raw . githubusercontent . com / VaibhavPage / argo - events - demo / master / webhook - event - source . yaml Create webhook gateway, kubectl - n argo - events apply - f https : // raw . githubusercontent . com / VaibhavPage / argo - events - demo / master / webhook - gateway . yaml Port forward to webhook gateway pod, kubectl - n argo - events port - forward < webhook - gateway - pod - name > 12000 : 12000 Create webhook sensor, kubectl - n argo - events apply - f https : // raw . githubusercontent . com / VaibhavPage / argo - events - demo / master / webhook - sensor . yaml Lets inspect webhook sensor, apiVersion : argoproj . io / v1alpha1 kind : Sensor metadata : name : webhook - sensor labels : sensors . argoproj . io / sensor - controller - instanceid : argo - events spec : template : spec : containers : - name : sensor image : argoproj / sensor : v0 .13.0 - rc imagePullPolicy : Always serviceAccountName : argo - events - sa dependencies : - name : test - dep gatewayName : webhook - gateway eventName : example subscription : http : port : 9300 triggers : - template : name : webhook - workflow - trigger k8s : group : argoproj . io version : v1alpha1 resource : workflows operation : create source : resource : apiVersion : argoproj . io / v1alpha1 kind : Workflow metadata : generateName : noisy - processor - spec : entrypoint : noisy arguments : parameters : - name : filterA value : \"5\" - name : filterB value : \"5\" - name : sVSp value : \"0.5\" - name : amount value : \"0.004\" templates : - name : noisy serviceAccountName : argo - events - sa inputs : parameters : - name : filterA - name : filterB - name : sVSp - name : amount container : image : metalgearsolid / demo - blur - argo - logo : latest command : [ papermill ] imagePullPolicy : Always env : - name : AWS_ACCESS_KEY_ID value : minio - name : AWS_SECRET_ACCESS_KEY value : minio123 - name : AWS_DEFAULT_REGION value : us - east - 1 - name : BOTO3_ENDPOINT_URL value : http : // minio - service . argo - events . svc : 9000 args : - \"noise.ipynb\" - \"s3://output/noisy-out.ipynb\" - \"-p\" - \"filterA\" - \"{{inputs.parameters.filterA}}\" - \"-p\" - \"filterB\" - \"{{inputs.parameters.filterB}}\" - \"-p\" - \"sVSp\" - \"{{inputs.parameters.sVSp}}\" - \"-p\" - \"amount\" - \"{{inputs.parameters.amount}}\" parameters : - src : dependencyName : test - dep dataKey : body . filterA dest : spec . arguments . parameters .0 . value - src : dependencyName : test - dep dataKey : body . filterB dest : spec . arguments . parameters .1 . value - src : dependencyName : test - dep dataKey : body . sVSp dest : spec . arguments . parameters .2 . value - src : dependencyName : test - dep dataKey : body . amount dest : spec . arguments . parameters .3 . value The sensor trigger is an Argo workflow that runs a jupyter notebook with papermill. It takes arguments for Guassian filter and Slat+Pepper noise in addition to S3 configuration. The event data received from HTTP POST request is made to override the arguments to workflow on the fly. Lets configurre Minio client mc, mc config host add minio http : // localhost : 9000 minio minio123 Create a bucket on Minio called output . mc mb minio / output Create the Minio event source that makes the gateway listen to file events for output bucket, kubectl - n argo - events apply - f https : // raw . githubusercontent . com / VaibhavPage / argo - events - demo / master / minio - event - source . yaml Create Minio gateway kubectl - n argo - events apply - f https : // raw . githubusercontent . com / VaibhavPage / argo - events - demo / master / minio - gateway . yaml Create Minio sensor, kubectl - n argo - events apply - f https : // raw . githubusercontent . com / VaibhavPage / argo - events - demo / master / minio - sensor . yaml The Minio sensor triggers an Argo workflow that determines the similarity index of the image that was put onto noisy-bucket on Minio and the original Argo logo. If the match is less than 80%, it publishes failure message on NATS subject called image-match Run a NATS subject subscriber in a separate terminal, go get github . com / nats - io / nats . go / cd examples / nats - sub go run main . go - s localhost : 4222 image - match Now, its time to send a HTTP request to parameterize the notebook that add noise to original Argo logo and execute the image processing pipeline. curl - d '{\"filterA\":\"15\", \"filterB\": \"15\", \"sVSp\": \"0.003\", \"amount\": \"0.010\"}' - H \"Content-Type: application/json\" - X POST http : // localhost : 12000 / example List argo workflows to list the noise-processor- and matcher-workflow- argo list Check the noisy image in bucket output . As soon as the matcher-workflow- completes, you will see a message on NATS subject ` [ # 1 ] Received on [ image - match ]: 'FAILURE: 0.4500723255991941' ` Lets change the parameters for the curl request to get >80% match, curl - d '{\"filterA\":\"5\", \"filterB\": \"5\", \"sVSp\": \"0.008\", \"amount\": \"0.0008\"}' - H \"Content-Type: application/json\" - X POST http : // localhost : 12000 / example Still only 51% match, [ # 2 ] Received on [ image - match ]: 'FAILURE: 0.519408012194793' Lets reduce the amount of noise to 0.0008, curl - d '{\"filterA\":\"5\", \"filterB\": \"5\", \"sVSp\": \"0.008\", \"amount\": \"0.0008\"}' - H \"Content-Type: application/json\" - X POST http : // localhost : 12000 / example You will see a success message, [ # 4 ] Received on [ image - match ]: 'SUCCESS: 0.9157008012270712' This was a simple image processing pipeline using Argo Events. You can easily set up CI pipelines, Machine Learning pipelines etc, using Argo Events and Argo Workflows.","title":"Setup"},{"location":"setup/amqp/","text":"AMQP \u00b6 AMQP gateway listens to messages on the MQ and helps sensor trigger the workloads. Event Structure \u00b6 The structure of an event dispatched by the gateway to the sensor looks like following, { \"context\" : { \"type\" : \"type_of_gateway\" , \"specVersion\" : \"cloud_events_version\" , \"source\" : \"name_of_the_gateway\" , \"eventID\" : \"unique_event_id\" , \"time\" : \"event_time\" , \"dataContentType\" : \"type_of_data\" , \"subject\" : \"name_of_the_event_within_event_source\" } , \"data\" : { \"contentType\" : \"ContentType is the MIME content type\" , \"contentEncoding\" : \"ContentEncoding is the MIME content encoding\" , \"deliveryMode\" : \"Delivery mode can be either - non-persistent (1) or persistent (2)\" , \"priority\" : \"Priority refers to the use - 0 to 9\" , \"correlationId\" : \"CorrelationId is the correlation identifier\" , \"replyTo\" : \"ReplyTo is the address to reply to (ex: RPC)\" , \"expiration\" : \"Expiration refers to message expiration spec\" , \"messageId\" : \"MessageId is message identifier\" , \"timestamp\" : \"Timestamp refers to the message timestamp\" , \"type\" : \"Type refers to the message type name\" , \"appId\" : \"AppId refers to the application id\" , \"exchange\" : \"Exchange is basic.publish exchange\" , \"routingKey\" : \"RoutingKey is basic.publish routing key\" , \"body\" : \"Body represents the messsage body\" , } } Setup \u00b6 Lets set up RabbitMQ locally, apiVersion : v1 kind : Service metadata : labels : component : rabbitmq name : rabbitmq - service spec : ports : - port : 5672 selector : app : taskQueue component : rabbitmq --- apiVersion : v1 kind : ReplicationController metadata : labels : component : rabbitmq name : rabbitmq - controller spec : replicas : 1 template : metadata : labels : app : taskQueue component : rabbitmq spec : containers : - image : rabbitmq name : rabbitmq ports : - containerPort : 5672 resources : limits : cpu : 100 m Make sure the RabbitMQ controller pod is up and running before proceeding further. Expose the RabbitMQ server to local publisher using port-forward , kubectl - n argo - events port - forward < rabbitmq - pod - name > 5672 : 5672 Create the event source by running the following command. kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / event - sources / amqp . yaml Create the gateway by running the following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / gateways / amqp . yaml Inspect the gateway pod logs to make sure the gateway was able to subscribe to the exchange specified in the event source to consume messages. Create the sensor by running the following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / sensors / amqp . yaml Lets set up a rabbitmq publisher. If you don't have pika installed, run, python - m pip install pika --upgrade Open a python REPL and run following code to publish a message on exhange called test . import pika connection = pika . BlockingConnection ( pika . ConnectionParameters ( 'localhost' )) channel = connection . channel () channel . basic_publish ( exchange = 'test' , routing_key = 'hello' , body = '{\"message\": \"hello\"}' ) As soon as you publish a message, sensor will trigger an Argo workflow. Run argo list to find the workflow. Troubleshoot \u00b6 Please read the FAQ .","title":"AMQP"},{"location":"setup/amqp/#amqp","text":"AMQP gateway listens to messages on the MQ and helps sensor trigger the workloads.","title":"AMQP"},{"location":"setup/amqp/#event-structure","text":"The structure of an event dispatched by the gateway to the sensor looks like following, { \"context\" : { \"type\" : \"type_of_gateway\" , \"specVersion\" : \"cloud_events_version\" , \"source\" : \"name_of_the_gateway\" , \"eventID\" : \"unique_event_id\" , \"time\" : \"event_time\" , \"dataContentType\" : \"type_of_data\" , \"subject\" : \"name_of_the_event_within_event_source\" } , \"data\" : { \"contentType\" : \"ContentType is the MIME content type\" , \"contentEncoding\" : \"ContentEncoding is the MIME content encoding\" , \"deliveryMode\" : \"Delivery mode can be either - non-persistent (1) or persistent (2)\" , \"priority\" : \"Priority refers to the use - 0 to 9\" , \"correlationId\" : \"CorrelationId is the correlation identifier\" , \"replyTo\" : \"ReplyTo is the address to reply to (ex: RPC)\" , \"expiration\" : \"Expiration refers to message expiration spec\" , \"messageId\" : \"MessageId is message identifier\" , \"timestamp\" : \"Timestamp refers to the message timestamp\" , \"type\" : \"Type refers to the message type name\" , \"appId\" : \"AppId refers to the application id\" , \"exchange\" : \"Exchange is basic.publish exchange\" , \"routingKey\" : \"RoutingKey is basic.publish routing key\" , \"body\" : \"Body represents the messsage body\" , } }","title":"Event Structure"},{"location":"setup/amqp/#setup","text":"Lets set up RabbitMQ locally, apiVersion : v1 kind : Service metadata : labels : component : rabbitmq name : rabbitmq - service spec : ports : - port : 5672 selector : app : taskQueue component : rabbitmq --- apiVersion : v1 kind : ReplicationController metadata : labels : component : rabbitmq name : rabbitmq - controller spec : replicas : 1 template : metadata : labels : app : taskQueue component : rabbitmq spec : containers : - image : rabbitmq name : rabbitmq ports : - containerPort : 5672 resources : limits : cpu : 100 m Make sure the RabbitMQ controller pod is up and running before proceeding further. Expose the RabbitMQ server to local publisher using port-forward , kubectl - n argo - events port - forward < rabbitmq - pod - name > 5672 : 5672 Create the event source by running the following command. kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / event - sources / amqp . yaml Create the gateway by running the following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / gateways / amqp . yaml Inspect the gateway pod logs to make sure the gateway was able to subscribe to the exchange specified in the event source to consume messages. Create the sensor by running the following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / sensors / amqp . yaml Lets set up a rabbitmq publisher. If you don't have pika installed, run, python - m pip install pika --upgrade Open a python REPL and run following code to publish a message on exhange called test . import pika connection = pika . BlockingConnection ( pika . ConnectionParameters ( 'localhost' )) channel = connection . channel () channel . basic_publish ( exchange = 'test' , routing_key = 'hello' , body = '{\"message\": \"hello\"}' ) As soon as you publish a message, sensor will trigger an Argo workflow. Run argo list to find the workflow.","title":"Setup"},{"location":"setup/amqp/#troubleshoot","text":"Please read the FAQ .","title":"Troubleshoot"},{"location":"setup/aws-sns/","text":"AWS SNS \u00b6 SNS gateway subscribes to AWS SNS topics, listens events and helps sensor trigger workloads. Event Structure \u00b6 The structure of an event dispatched by the gateway to the sensor looks like following, { \"context\" : { \"type\" : \"type_of_gateway\" , \"specVersion\" : \"cloud_events_version\" , \"source\" : \"name_of_the_gateway\" , \"eventID\" : \"unique_event_id\" , \"time\" : \"event_time\" , \"dataContentType\" : \"type_of_data\" , \"subject\" : \"name_of_the_event_within_event_source\" } , \"data\" : { \"header\" : \"sns headers\" , \"body\" : \"body refers to the sns notification data\" , } } Setup \u00b6 Create a topic called test using aws cli or AWS SNS console. Fetch your access and secret key for AWS account and base64 encode them. Create a secret called aws-secret as follows, apiVersion : v1 kind : Secret metadata : name : aws - secret type : Opaque data : accesskey : < base64 - access - key > secretkey : < base64 - secret - key > Deploy the secret kubectl - n argo - events apply - f aws - secret . yaml Create the gateway by running the following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / gateways / aws - sns . yaml Wait for gateway pod to get into the running state. Create an Ingress or Openshift Route for the gateway service to that it can be reached from AWS. You can find more information on Ingress or Route online. Get the event source stored at https://raw.githubusercontent.com/argoproj/argo-events/master/examples/event-sources/aws-sns.yaml Change the topicArn and url under webhook to your gateway service url created in a previous step. Make sure this url is reachable from AWS. Create the event source by running the following command. kubectl apply - n argo - events - f < event - source - file - updated - in - previous - step > Go to SNS settings on AWS and verify the webhook is registered. You can also do the same by looking at the gateway pod logs. Create the sensor by running the following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / sensors / aws - sns . yaml Publish a message to the SNS topic and it will trigger an argo workflow. Run argo list to find the workflow. Troubleshoot \u00b6 Please read the FAQ .","title":"AWS SNS"},{"location":"setup/aws-sns/#aws-sns","text":"SNS gateway subscribes to AWS SNS topics, listens events and helps sensor trigger workloads.","title":"AWS SNS"},{"location":"setup/aws-sns/#event-structure","text":"The structure of an event dispatched by the gateway to the sensor looks like following, { \"context\" : { \"type\" : \"type_of_gateway\" , \"specVersion\" : \"cloud_events_version\" , \"source\" : \"name_of_the_gateway\" , \"eventID\" : \"unique_event_id\" , \"time\" : \"event_time\" , \"dataContentType\" : \"type_of_data\" , \"subject\" : \"name_of_the_event_within_event_source\" } , \"data\" : { \"header\" : \"sns headers\" , \"body\" : \"body refers to the sns notification data\" , } }","title":"Event Structure"},{"location":"setup/aws-sns/#setup","text":"Create a topic called test using aws cli or AWS SNS console. Fetch your access and secret key for AWS account and base64 encode them. Create a secret called aws-secret as follows, apiVersion : v1 kind : Secret metadata : name : aws - secret type : Opaque data : accesskey : < base64 - access - key > secretkey : < base64 - secret - key > Deploy the secret kubectl - n argo - events apply - f aws - secret . yaml Create the gateway by running the following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / gateways / aws - sns . yaml Wait for gateway pod to get into the running state. Create an Ingress or Openshift Route for the gateway service to that it can be reached from AWS. You can find more information on Ingress or Route online. Get the event source stored at https://raw.githubusercontent.com/argoproj/argo-events/master/examples/event-sources/aws-sns.yaml Change the topicArn and url under webhook to your gateway service url created in a previous step. Make sure this url is reachable from AWS. Create the event source by running the following command. kubectl apply - n argo - events - f < event - source - file - updated - in - previous - step > Go to SNS settings on AWS and verify the webhook is registered. You can also do the same by looking at the gateway pod logs. Create the sensor by running the following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / sensors / aws - sns . yaml Publish a message to the SNS topic and it will trigger an argo workflow. Run argo list to find the workflow.","title":"Setup"},{"location":"setup/aws-sns/#troubleshoot","text":"Please read the FAQ .","title":"Troubleshoot"},{"location":"setup/aws-sqs/","text":"AWS SQS \u00b6 SQS gateway listens to messages on AWS SQS queue and helps sensor trigger workloads. Event Structure \u00b6 The structure of an event dispatched by the gateway to the sensor looks like following, { \" context \" : { \" type \" : \" type_of_gateway \" , \" specVersion \" : \" cloud_events_version \" , \" source \" : \" name_of_the_gateway \" , \" eventID \" : \" unique_event_id \" , \" time \" : \" event_time \" , \" dataContentType \" : \" type_of_data \" , \" subject \" : \" name_of_the_event_within_event_source \" }, \" data \" : { \" messageId \" : \" message id \" , // Each message attribute consists of a Name , Type , and Value . For more information , // see Amazon SQS Message Attributes // ( https : // docs . aws . amazon . com / AWSSimpleQueueService / latest / SQSDeveloperGuide / sqs - message - attributes . html ) // in the Amazon Simple Queue Service Developer Guide . \" messageAttributes \" : \" message attributes \" , \" body \" : \" Body is the message data \" , } } Setup \u00b6 Create a queue called test either using aws cli or AWS SQS management console. Fetch your access and secret key for AWS account and base64 encode them. Create a secret called aws-secret as follows, apiVersion : v1 kind : Secret metadata : name : aws - secret type : Opaque data : accesskey : < base64 - access - key > secretkey : < base64 - secret - key > Deploy the secret kubectl - n argo - events apply - f aws - secret . yaml Create the event source by running the following command. kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / event - sources / aws - sqs . yaml Create the gateway by running the following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / gateways / aws - sqs . yaml Inspect the gateway pod logs to make sure the gateway was able to subscribe to the queue specified in the event source to consume messages. Create the sensor by running the following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / sensors / aws - sqs . yaml Dispatch a message on sqs queue, aws sqs send - message -- queue - url https : // sqs . us - east - 1 . amazonaws . com / XXXXX / test -- message - body ' {\"message\": \"hello\"} ' Once a message is published, an argo workflow will be triggered. Run argo list to find the workflow. Troubleshoot \u00b6 Please read the FAQ .","title":"AWS SQS"},{"location":"setup/aws-sqs/#aws-sqs","text":"SQS gateway listens to messages on AWS SQS queue and helps sensor trigger workloads.","title":"AWS SQS"},{"location":"setup/aws-sqs/#event-structure","text":"The structure of an event dispatched by the gateway to the sensor looks like following, { \" context \" : { \" type \" : \" type_of_gateway \" , \" specVersion \" : \" cloud_events_version \" , \" source \" : \" name_of_the_gateway \" , \" eventID \" : \" unique_event_id \" , \" time \" : \" event_time \" , \" dataContentType \" : \" type_of_data \" , \" subject \" : \" name_of_the_event_within_event_source \" }, \" data \" : { \" messageId \" : \" message id \" , // Each message attribute consists of a Name , Type , and Value . For more information , // see Amazon SQS Message Attributes // ( https : // docs . aws . amazon . com / AWSSimpleQueueService / latest / SQSDeveloperGuide / sqs - message - attributes . html ) // in the Amazon Simple Queue Service Developer Guide . \" messageAttributes \" : \" message attributes \" , \" body \" : \" Body is the message data \" , } }","title":"Event Structure"},{"location":"setup/aws-sqs/#setup","text":"Create a queue called test either using aws cli or AWS SQS management console. Fetch your access and secret key for AWS account and base64 encode them. Create a secret called aws-secret as follows, apiVersion : v1 kind : Secret metadata : name : aws - secret type : Opaque data : accesskey : < base64 - access - key > secretkey : < base64 - secret - key > Deploy the secret kubectl - n argo - events apply - f aws - secret . yaml Create the event source by running the following command. kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / event - sources / aws - sqs . yaml Create the gateway by running the following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / gateways / aws - sqs . yaml Inspect the gateway pod logs to make sure the gateway was able to subscribe to the queue specified in the event source to consume messages. Create the sensor by running the following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / sensors / aws - sqs . yaml Dispatch a message on sqs queue, aws sqs send - message -- queue - url https : // sqs . us - east - 1 . amazonaws . com / XXXXX / test -- message - body ' {\"message\": \"hello\"} ' Once a message is published, an argo workflow will be triggered. Run argo list to find the workflow.","title":"Setup"},{"location":"setup/aws-sqs/#troubleshoot","text":"Please read the FAQ .","title":"Troubleshoot"},{"location":"setup/calendar/","text":"Calendar \u00b6 Calendar gateway generates events on either a cron schedule or an interval and help trigger workloads. Event Structure \u00b6 The structure of an event dispatched by the gateway to the sensor looks like following, { \"context\" : { \"type\" : \"type_of_gateway\" , \"specVersion\" : \"cloud_events_version\" , \"source\" : \"name_of_the_gateway\" , \"eventID\" : \"unique_event_id\" , \"time\" : \"event_time\" , \"dataContentType\" : \"type_of_data\" , \"subject\" : \"name_of_the_event_within_event_source\" } , \"data\" : { \"eventTime\" : { /* UTC time of the event */ } , \"userPayload\" : { /* static payload available in the event source */ } , } } Setup \u00b6 Install gateway in the argo-events namespace using following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / gateways / calendar . yaml Once the gateway resource is created, the gateway controller will process it and create a pod. If you don't see the pod in argo-events namespace, check the gateway controller logs for errors. If you inspect the gateway resource definition, you will notice it points to the event source called calendar-event-source . Lets install event source in the argo-events namespace, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / event - sources / calendar . yaml Check the gateway logs to make sure the gateway has processed the event source. The gateway will generate events at every 10 seconds. Lets create the sensor, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / sensors / calendar . yaml Once the sensor pod is in running state, wait for next interval to occur. Troubleshoot \u00b6 Please read the FAQ .","title":"Calendar"},{"location":"setup/calendar/#calendar","text":"Calendar gateway generates events on either a cron schedule or an interval and help trigger workloads.","title":"Calendar"},{"location":"setup/calendar/#event-structure","text":"The structure of an event dispatched by the gateway to the sensor looks like following, { \"context\" : { \"type\" : \"type_of_gateway\" , \"specVersion\" : \"cloud_events_version\" , \"source\" : \"name_of_the_gateway\" , \"eventID\" : \"unique_event_id\" , \"time\" : \"event_time\" , \"dataContentType\" : \"type_of_data\" , \"subject\" : \"name_of_the_event_within_event_source\" } , \"data\" : { \"eventTime\" : { /* UTC time of the event */ } , \"userPayload\" : { /* static payload available in the event source */ } , } }","title":"Event Structure"},{"location":"setup/calendar/#setup","text":"Install gateway in the argo-events namespace using following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / gateways / calendar . yaml Once the gateway resource is created, the gateway controller will process it and create a pod. If you don't see the pod in argo-events namespace, check the gateway controller logs for errors. If you inspect the gateway resource definition, you will notice it points to the event source called calendar-event-source . Lets install event source in the argo-events namespace, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / event - sources / calendar . yaml Check the gateway logs to make sure the gateway has processed the event source. The gateway will generate events at every 10 seconds. Lets create the sensor, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / sensors / calendar . yaml Once the sensor pod is in running state, wait for next interval to occur.","title":"Setup"},{"location":"setup/calendar/#troubleshoot","text":"Please read the FAQ .","title":"Troubleshoot"},{"location":"setup/emitter/","text":"Emitter \u00b6 Emitter gateway subscribes to a channel and helps sensor trigger the workloads. Event Structure \u00b6 The structure of an event dispatched by the gateway to the sensor looks like following, { \"context\" : { \"type\" : \"type_of_gateway\" , \"specVersion\" : \"cloud_events_version\" , \"source\" : \"name_of_the_gateway\" , \"eventID\" : \"unique_event_id\" , \"time\" : \"event_time\" , \"dataContentType\" : \"type_of_data\" , \"subject\" : \"name_of_the_event_within_event_source\" } , \"data\" : { \"topic\" : \"name_of_the_topic\" , \"body\" : \"message_payload\" } } Setup \u00b6 Deploy emitter in your local K8s cluster, --- apiVersion : v1 kind : Service metadata : name : broker labels : app : broker spec : clusterIP : None ports : - port : 4000 targetPort : 4000 selector : app : broker --- apiVersion : apps / v1 kind : Deployment metadata : name : broker spec : replicas : 1 selector : matchLabels : app : broker template : metadata : labels : app : broker spec : containers : - env : - name : EMITTER_LICENSE value : \" zT83oDV0DWY5_JysbSTPTDr8KB0AAAAAAAAAAAAAAAI \" # This is a test license , DO NOT USE IN PRODUCTION ! - name : EMITTER_CLUSTER_SEED value : \" broker \" - name : EMITTER_CLUSTER_ADVERTISE value : \" private:4000 \" name : broker image : emitter / server : latest ports : - containerPort : 8080 - containerPort : 443 - containerPort : 4000 volumeMounts : - name : broker - volume mountPath : / data volumes : - name : broker - volume hostPath : path : / emitter # directory on host Create the event source by running the following command. kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / event - sources / emitter . yaml Create the gateway by running the following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / gateways / emitter . yaml Inspect the gateway pod logs to make sure the gateway was able to subscribe to the topic specified in the event source to consume messages. Create the sensor by running the following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / sensors / emitter . yaml Send message on emitter channel using one of the clients https://emitter.io/develop/golang/ Once a message is published, an argo workflow will be triggered. Run argo list to find the workflow. Troubleshoot \u00b6 Please read the FAQ .","title":"Emitter"},{"location":"setup/emitter/#emitter","text":"Emitter gateway subscribes to a channel and helps sensor trigger the workloads.","title":"Emitter"},{"location":"setup/emitter/#event-structure","text":"The structure of an event dispatched by the gateway to the sensor looks like following, { \"context\" : { \"type\" : \"type_of_gateway\" , \"specVersion\" : \"cloud_events_version\" , \"source\" : \"name_of_the_gateway\" , \"eventID\" : \"unique_event_id\" , \"time\" : \"event_time\" , \"dataContentType\" : \"type_of_data\" , \"subject\" : \"name_of_the_event_within_event_source\" } , \"data\" : { \"topic\" : \"name_of_the_topic\" , \"body\" : \"message_payload\" } }","title":"Event Structure"},{"location":"setup/emitter/#setup","text":"Deploy emitter in your local K8s cluster, --- apiVersion : v1 kind : Service metadata : name : broker labels : app : broker spec : clusterIP : None ports : - port : 4000 targetPort : 4000 selector : app : broker --- apiVersion : apps / v1 kind : Deployment metadata : name : broker spec : replicas : 1 selector : matchLabels : app : broker template : metadata : labels : app : broker spec : containers : - env : - name : EMITTER_LICENSE value : \" zT83oDV0DWY5_JysbSTPTDr8KB0AAAAAAAAAAAAAAAI \" # This is a test license , DO NOT USE IN PRODUCTION ! - name : EMITTER_CLUSTER_SEED value : \" broker \" - name : EMITTER_CLUSTER_ADVERTISE value : \" private:4000 \" name : broker image : emitter / server : latest ports : - containerPort : 8080 - containerPort : 443 - containerPort : 4000 volumeMounts : - name : broker - volume mountPath : / data volumes : - name : broker - volume hostPath : path : / emitter # directory on host Create the event source by running the following command. kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / event - sources / emitter . yaml Create the gateway by running the following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / gateways / emitter . yaml Inspect the gateway pod logs to make sure the gateway was able to subscribe to the topic specified in the event source to consume messages. Create the sensor by running the following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / sensors / emitter . yaml Send message on emitter channel using one of the clients https://emitter.io/develop/golang/ Once a message is published, an argo workflow will be triggered. Run argo list to find the workflow.","title":"Setup"},{"location":"setup/emitter/#troubleshoot","text":"Please read the FAQ .","title":"Troubleshoot"},{"location":"setup/file/","text":"File \u00b6 File gateway listens to file system events and helps sensor trigger workloads. Event Structure \u00b6 The structure of an event dispatched by the gateway to the sensor looks like following, { \"context\" : { \"type\" : \"type_of_gateway\" , \"specVersion\" : \"cloud_events_version\" , \"source\" : \"name_of_the_gateway\" , \"eventID\" : \"unique_event_id\" , \"time\" : \"event_time\" , \"dataContentType\" : \"type_of_data\" , \"subject\" : \"name_of_the_event_within_event_source\" } , \"data\" : { \"name\" : \"Relative path to the file or directory\" , \"op\" : \"File operation that triggered the event\" // Create , Write , Remove , Rename , Chmod } } Setup \u00b6 Create the event source by running the following command. kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / event - sources / file . yaml The event source has configuration to listen to file system events for test-data directory and file called x.txt . Create the gateway by running the following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / gateways / file . yaml Create the sensor by running the following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / sensors / file . yaml Make sure there are no errors in either gateway or sensor pod. Log into the gateway pod by running following command, kubectl - n argo - events exec - it < file - gateway - pod - name > - c file - events -- / bin / bash Lets create a file called x.txt under test-data directory in gateway pod. cd test - data cat << EOF > x . txt hello EOF Once you create file x.txt , the sensor will trigger argo workflow. Run argo list to find the workflow. For real-world use cases, you should use PersistentVolume and PersistentVolumeClaim. Troubleshoot \u00b6 Please read the FAQ .","title":"File"},{"location":"setup/file/#file","text":"File gateway listens to file system events and helps sensor trigger workloads.","title":"File"},{"location":"setup/file/#event-structure","text":"The structure of an event dispatched by the gateway to the sensor looks like following, { \"context\" : { \"type\" : \"type_of_gateway\" , \"specVersion\" : \"cloud_events_version\" , \"source\" : \"name_of_the_gateway\" , \"eventID\" : \"unique_event_id\" , \"time\" : \"event_time\" , \"dataContentType\" : \"type_of_data\" , \"subject\" : \"name_of_the_event_within_event_source\" } , \"data\" : { \"name\" : \"Relative path to the file or directory\" , \"op\" : \"File operation that triggered the event\" // Create , Write , Remove , Rename , Chmod } }","title":"Event Structure"},{"location":"setup/file/#setup","text":"Create the event source by running the following command. kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / event - sources / file . yaml The event source has configuration to listen to file system events for test-data directory and file called x.txt . Create the gateway by running the following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / gateways / file . yaml Create the sensor by running the following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / sensors / file . yaml Make sure there are no errors in either gateway or sensor pod. Log into the gateway pod by running following command, kubectl - n argo - events exec - it < file - gateway - pod - name > - c file - events -- / bin / bash Lets create a file called x.txt under test-data directory in gateway pod. cd test - data cat << EOF > x . txt hello EOF Once you create file x.txt , the sensor will trigger argo workflow. Run argo list to find the workflow. For real-world use cases, you should use PersistentVolume and PersistentVolumeClaim.","title":"Setup"},{"location":"setup/file/#troubleshoot","text":"Please read the FAQ .","title":"Troubleshoot"},{"location":"setup/gcp-pub-sub/","text":"GCP PubSub \u00b6 GCP PubSub gateway subscribes to messages published by GCP publisher and helps sensor trigger workloads. Event Structure \u00b6 The structure of an event dispatched by the gateway to the sensor looks like following, { \"context\" : { \"type\" : \"type_of_gateway\" , \"specVersion\" : \"cloud_events_version\" , \"source\" : \"name_of_the_gateway\" , \"eventID\" : \"unique_event_id\" , \"time\" : \"event_time\" , \"dataContentType\" : \"type_of_data\" , \"subject\" : \"name_of_the_event_within_event_source\" } , \"data\" : { \"id\" : \"message id\" , // Attributes represents the key - value pairs the current message // is labelled with . \"attributes\" : \"key-values\" , \"publishTime\" : \"// The time at which the message was published\" , \"body\" : \"body refers to the message data\" , } } Setup \u00b6 Fetch the project credentials JSON file from GCP console. Create a K8s secret called gcp-credentials to store the credentials file apiVersion : v1 data : key . json : < YOUR_CREDENTIALS_STRING_FROM_JSON_FILE > kind : Secret metadata : name : gcp - credentials namespace : argo - events type : Opaque Create the gateway by running the following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / gateways / gcp - pubsub . yaml Create the event source by running the following command. kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / event - sources / gcp - pubsub . yaml Inspect the gateway pod logs to make sure the gateway was able to subscribe to the topic specified in the event source to consume messages. Create the sensor by running the following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / sensors / gcp - pubsub . yaml Publish a message from GCP PubSub console. Once a message is published, an argo workflow will be triggered. Run argo list to find the workflow. Troubleshoot \u00b6 Please read the FAQ .","title":"GCP PubSub"},{"location":"setup/gcp-pub-sub/#gcp-pubsub","text":"GCP PubSub gateway subscribes to messages published by GCP publisher and helps sensor trigger workloads.","title":"GCP PubSub"},{"location":"setup/gcp-pub-sub/#event-structure","text":"The structure of an event dispatched by the gateway to the sensor looks like following, { \"context\" : { \"type\" : \"type_of_gateway\" , \"specVersion\" : \"cloud_events_version\" , \"source\" : \"name_of_the_gateway\" , \"eventID\" : \"unique_event_id\" , \"time\" : \"event_time\" , \"dataContentType\" : \"type_of_data\" , \"subject\" : \"name_of_the_event_within_event_source\" } , \"data\" : { \"id\" : \"message id\" , // Attributes represents the key - value pairs the current message // is labelled with . \"attributes\" : \"key-values\" , \"publishTime\" : \"// The time at which the message was published\" , \"body\" : \"body refers to the message data\" , } }","title":"Event Structure"},{"location":"setup/gcp-pub-sub/#setup","text":"Fetch the project credentials JSON file from GCP console. Create a K8s secret called gcp-credentials to store the credentials file apiVersion : v1 data : key . json : < YOUR_CREDENTIALS_STRING_FROM_JSON_FILE > kind : Secret metadata : name : gcp - credentials namespace : argo - events type : Opaque Create the gateway by running the following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / gateways / gcp - pubsub . yaml Create the event source by running the following command. kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / event - sources / gcp - pubsub . yaml Inspect the gateway pod logs to make sure the gateway was able to subscribe to the topic specified in the event source to consume messages. Create the sensor by running the following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / sensors / gcp - pubsub . yaml Publish a message from GCP PubSub console. Once a message is published, an argo workflow will be triggered. Run argo list to find the workflow.","title":"Setup"},{"location":"setup/gcp-pub-sub/#troubleshoot","text":"Please read the FAQ .","title":"Troubleshoot"},{"location":"setup/github/","text":"GitHub \u00b6 GitHub gateway programatically configures webhooks for projects on GitHub and helps sensor trigger the workloads upon events. Event Structure \u00b6 The structure of an event dispatched by the gateway to the sensor looks like following, { \"context\" : { \"type\" : \"type_of_gateway\" , \"specVersion\" : \"cloud_events_version\" , \"source\" : \"name_of_the_gateway\" , \"eventID\" : \"unique_event_id\" , \"time\" : \"event_time\" , \"dataContentType\" : \"type_of_data\" , \"subject\" : \"name_of_the_event_within_event_source\" } , \"data\" : { \"body\" : \"Body is the github event data\" , \"headers\" : \"Headers from the Gitlab event\" , } } Setup \u00b6 Create an API token if you don't have one. Follow instructions to create a new GitHub API Token. Grant it the repo_hook permissions. Base64 encode your api token key, echo - n < api - token - key > | base64 Create a secret called github-access . apiVersion : v1 kind : Secret metadata : name : github - access type : Opaque data : access : < base64 - encoded - api - token - from - previous - step > Deploy the secret into K8s cluster kubectl - n argo - events apply - f github - access . yaml Create the gateway by running the following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / gateways / github . yaml Wait for gateway pod to get into the running state. Create an Ingress or Openshift Route for the gateway service to that it can be reached from GitHub. You can find more information on Ingress or Route online. Get the event source stored at https://raw.githubusercontent.com/argoproj/argo-events/master/examples/event-sources/github.yaml Change the url under webhook to your gateway service url created in a previous step. Make sure this url is reachable from GitHub. Create the event source by running the following command. kubectl apply - n argo - events - f < event - source - file - updated - in - previous - step > Go to Webhooks under your project settings on GitHub and verify the webhook is registered. You can also do the same by looking at the gateway pod logs. Create the sensor by running the following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / sensors / github . yaml Make a change to one of your project files and commit. It will trigger an argo workflow. Run argo list to find the workflow. Troubleshoot \u00b6 Please read the FAQ .","title":"GitHub"},{"location":"setup/github/#github","text":"GitHub gateway programatically configures webhooks for projects on GitHub and helps sensor trigger the workloads upon events.","title":"GitHub"},{"location":"setup/github/#event-structure","text":"The structure of an event dispatched by the gateway to the sensor looks like following, { \"context\" : { \"type\" : \"type_of_gateway\" , \"specVersion\" : \"cloud_events_version\" , \"source\" : \"name_of_the_gateway\" , \"eventID\" : \"unique_event_id\" , \"time\" : \"event_time\" , \"dataContentType\" : \"type_of_data\" , \"subject\" : \"name_of_the_event_within_event_source\" } , \"data\" : { \"body\" : \"Body is the github event data\" , \"headers\" : \"Headers from the Gitlab event\" , } }","title":"Event Structure"},{"location":"setup/github/#setup","text":"Create an API token if you don't have one. Follow instructions to create a new GitHub API Token. Grant it the repo_hook permissions. Base64 encode your api token key, echo - n < api - token - key > | base64 Create a secret called github-access . apiVersion : v1 kind : Secret metadata : name : github - access type : Opaque data : access : < base64 - encoded - api - token - from - previous - step > Deploy the secret into K8s cluster kubectl - n argo - events apply - f github - access . yaml Create the gateway by running the following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / gateways / github . yaml Wait for gateway pod to get into the running state. Create an Ingress or Openshift Route for the gateway service to that it can be reached from GitHub. You can find more information on Ingress or Route online. Get the event source stored at https://raw.githubusercontent.com/argoproj/argo-events/master/examples/event-sources/github.yaml Change the url under webhook to your gateway service url created in a previous step. Make sure this url is reachable from GitHub. Create the event source by running the following command. kubectl apply - n argo - events - f < event - source - file - updated - in - previous - step > Go to Webhooks under your project settings on GitHub and verify the webhook is registered. You can also do the same by looking at the gateway pod logs. Create the sensor by running the following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / sensors / github . yaml Make a change to one of your project files and commit. It will trigger an argo workflow. Run argo list to find the workflow.","title":"Setup"},{"location":"setup/github/#troubleshoot","text":"Please read the FAQ .","title":"Troubleshoot"},{"location":"setup/gitlab/","text":"GitLab \u00b6 GitLab gateway programatically configures webhooks for projects on GitLab and helps sensor trigger the workloads upon events. Event Structure \u00b6 The structure of an event dispatched by the gateway to the sensor looks like following, { \"context\" : { \"type\" : \"type_of_gateway\" , \"specVersion\" : \"cloud_events_version\" , \"source\" : \"name_of_the_gateway\" , \"eventID\" : \"unique_event_id\" , \"time\" : \"event_time\" , \"dataContentType\" : \"type_of_data\" , \"subject\" : \"name_of_the_event_within_event_source\" } , \"data\" : { \"body\" : \"Body is the gitlab event data\" , \"headers\" : \"Headers from the Gitlab event\" , } } Setup \u00b6 Create an API token if you don't have one. Follow instructions to create a new GitLab API Token. Grant it the api permissions. Base64 encode your api token key, echo - n < api - token - key > | base64 Create a secret called gitlab-access . apiVersion : v1 kind : Secret metadata : name : gitlab - access type : Opaque data : access : < base64 - encoded - api - token - from - previous - step > Deploy the secret into K8s cluster kubectl - n argo - events apply - f gitlab - access . yaml Create the gateway by running the following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / gateways / gitlab . yaml Wait for gateway pod to get into the running state. Create an Ingress or Openshift Route for the gateway service to that it can be reached from GitLab. You can find more information on Ingress or Route online. Get the event source stored at https://raw.githubusercontent.com/argoproj/argo-events/master/examples/event-sources/gitlab.yaml Change the url under webhook to your gateway service url created in a previous step. Make sure this url is reachable from GitLab. Create the event source by running the following command. kubectl apply - n argo - events - f < event - source - file - updated - in - previous - step > Go to Webhooks under your project settings on GitLab and verify the webhook is registered. You can also do the same by looking at the gateway pod logs. Create the sensor by running the following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / sensors / gitlab . yaml Make a change to one of your project files and commit. It will trigger an argo workflow. Run argo list to find the workflow. Troubleshoot \u00b6 Please read the FAQ .","title":"GitLab"},{"location":"setup/gitlab/#gitlab","text":"GitLab gateway programatically configures webhooks for projects on GitLab and helps sensor trigger the workloads upon events.","title":"GitLab"},{"location":"setup/gitlab/#event-structure","text":"The structure of an event dispatched by the gateway to the sensor looks like following, { \"context\" : { \"type\" : \"type_of_gateway\" , \"specVersion\" : \"cloud_events_version\" , \"source\" : \"name_of_the_gateway\" , \"eventID\" : \"unique_event_id\" , \"time\" : \"event_time\" , \"dataContentType\" : \"type_of_data\" , \"subject\" : \"name_of_the_event_within_event_source\" } , \"data\" : { \"body\" : \"Body is the gitlab event data\" , \"headers\" : \"Headers from the Gitlab event\" , } }","title":"Event Structure"},{"location":"setup/gitlab/#setup","text":"Create an API token if you don't have one. Follow instructions to create a new GitLab API Token. Grant it the api permissions. Base64 encode your api token key, echo - n < api - token - key > | base64 Create a secret called gitlab-access . apiVersion : v1 kind : Secret metadata : name : gitlab - access type : Opaque data : access : < base64 - encoded - api - token - from - previous - step > Deploy the secret into K8s cluster kubectl - n argo - events apply - f gitlab - access . yaml Create the gateway by running the following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / gateways / gitlab . yaml Wait for gateway pod to get into the running state. Create an Ingress or Openshift Route for the gateway service to that it can be reached from GitLab. You can find more information on Ingress or Route online. Get the event source stored at https://raw.githubusercontent.com/argoproj/argo-events/master/examples/event-sources/gitlab.yaml Change the url under webhook to your gateway service url created in a previous step. Make sure this url is reachable from GitLab. Create the event source by running the following command. kubectl apply - n argo - events - f < event - source - file - updated - in - previous - step > Go to Webhooks under your project settings on GitLab and verify the webhook is registered. You can also do the same by looking at the gateway pod logs. Create the sensor by running the following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / sensors / gitlab . yaml Make a change to one of your project files and commit. It will trigger an argo workflow. Run argo list to find the workflow.","title":"Setup"},{"location":"setup/gitlab/#troubleshoot","text":"Please read the FAQ .","title":"Troubleshoot"},{"location":"setup/kafka/","text":"Kafka \u00b6 Event Structure \u00b6 The structure of an event dispatched by the gateway to the sensor looks like following, { \"context\" : { \"type\" : \"type_of_gateway\" , \"specVersion\" : \"cloud_events_version\" , \"source\" : \"name_of_the_gateway\" , \"eventID\" : \"unique_event_id\" , \"time\" : \"event_time\" , \"dataContentType\" : \"type_of_data\" , \"subject\" : \"name_of_the_event_within_event_source\" } , \"data\" : { \"topic\" : \"kafka_topic\" , \"partition\" : \"partition_number\" , \"body\" : \"message_body\" , \"timestamp\" : \"timestamp_of_the_message\" } } Setup \u00b6 Make sure to setup the Kafka cluster in Kubernetes if you don't already have one. You can refer to https://github.com/Yolean/kubernetes-kafka for installation instructions. Create the event source by running the following command. Make sure to update the appropriate fields. kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / event - sources / kafka . yaml Create the gateway by running the following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / gateways / kafka . yaml Inspect the gateway pod logs to make sure the gateway was able to subscribe to the topic specified in the event source to consume messages. Create the sensor by running the following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / sensors / kafka . yaml Send message by using Kafka client. More info on how to send message at https://kafka.apache.org/quickstart Once a message is published, an argo workflow will be triggered. Run argo list to find the workflow. Troubleshoot \u00b6 Please read the FAQ .","title":"Kafka"},{"location":"setup/kafka/#kafka","text":"","title":"Kafka"},{"location":"setup/kafka/#event-structure","text":"The structure of an event dispatched by the gateway to the sensor looks like following, { \"context\" : { \"type\" : \"type_of_gateway\" , \"specVersion\" : \"cloud_events_version\" , \"source\" : \"name_of_the_gateway\" , \"eventID\" : \"unique_event_id\" , \"time\" : \"event_time\" , \"dataContentType\" : \"type_of_data\" , \"subject\" : \"name_of_the_event_within_event_source\" } , \"data\" : { \"topic\" : \"kafka_topic\" , \"partition\" : \"partition_number\" , \"body\" : \"message_body\" , \"timestamp\" : \"timestamp_of_the_message\" } }","title":"Event Structure"},{"location":"setup/kafka/#setup","text":"Make sure to setup the Kafka cluster in Kubernetes if you don't already have one. You can refer to https://github.com/Yolean/kubernetes-kafka for installation instructions. Create the event source by running the following command. Make sure to update the appropriate fields. kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / event - sources / kafka . yaml Create the gateway by running the following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / gateways / kafka . yaml Inspect the gateway pod logs to make sure the gateway was able to subscribe to the topic specified in the event source to consume messages. Create the sensor by running the following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / sensors / kafka . yaml Send message by using Kafka client. More info on how to send message at https://kafka.apache.org/quickstart Once a message is published, an argo workflow will be triggered. Run argo list to find the workflow.","title":"Setup"},{"location":"setup/kafka/#troubleshoot","text":"Please read the FAQ .","title":"Troubleshoot"},{"location":"setup/minio/","text":"Minio \u00b6 Minio gateway listens to minio bucket notifications and helps sensor trigger the workloads. Note : Minio gateway is exclusive for the Minio server. If you want to trigger workloads on AWS S3 bucket notification, please set up the AWS SNS gateway. Event Structure \u00b6 The structure of an event dispatched by the gateway to the sensor looks like following, { \"context\" : { \"type\" : \"type_of_gateway\" , \"specVersion\" : \"cloud_events_version\" , \"source\" : \"name_of_the_gateway\" , \"eventID\" : \"unique_event_id\" , \"time\" : \"event_time\" , \"dataContentType\" : \"type_of_data\" , \"subject\" : \"name_of_the_event_within_event_source\" } , \"data\" : { notification : [ { /* Minio notification. More info is available at https://docs.min.io/docs/minio-bucket-notification-guide.html } ] } } Setup \u00b6 Make sure to have minio server deployed and reachable from the gateway. More info on minio server setup is available at https://github.com/minio/minio/blob/master/docs/orchestration/kubernetes/k8s-yaml.md. If you are running Minio locally, make sure to port-forward to minio pod in order to make the service available outside local K8s cluster. kubectl - n argo - events port - forward < minio - pod - name > 9000 : 9000 Configure the minio client mc . mc config host add minio http : // localhost : 9000 minio minio123 Create a K8s secret that holds the access and secret key. This secret will be referred in the minio event source definition that we are going to install in a later step. apiVersion : v1 data : # base64 of minio accesskey : bWluaW8 = # base64 of minio123 secretkey : bWluaW8xMjM = kind : Secret metadata : name : artifacts - minio namespace : argo - events The event source we are going to use configures notifications for a bucket called input . mc mb minio / input If you inspect the gateway resource definition, you will notice that it refers to the event source minio-event-source . Lets install event source in the argo-events namespace, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / event - sources / minio . yaml Always make sure to first create a bucket on Minio and then refer it in event source. Install gateway in the argo-events namespace using following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / gateways / minio . yaml Once the gateway resource is created, the gateway controller will process it and create a pod. If you don't see the pod in argo-events namespace, check the gateway controller logs for errors. Check the gateway logs to make sure the gateway has processed the event source. Lets create the sensor, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / sensors / minio . yaml Create a file named and hello-world.txt and upload it onto to the input bucket. This will trigger the argo workflow. Run argo list to find the workflow. Troubleshoot \u00b6 Please read the FAQ .","title":"Minio"},{"location":"setup/minio/#minio","text":"Minio gateway listens to minio bucket notifications and helps sensor trigger the workloads. Note : Minio gateway is exclusive for the Minio server. If you want to trigger workloads on AWS S3 bucket notification, please set up the AWS SNS gateway.","title":"Minio"},{"location":"setup/minio/#event-structure","text":"The structure of an event dispatched by the gateway to the sensor looks like following, { \"context\" : { \"type\" : \"type_of_gateway\" , \"specVersion\" : \"cloud_events_version\" , \"source\" : \"name_of_the_gateway\" , \"eventID\" : \"unique_event_id\" , \"time\" : \"event_time\" , \"dataContentType\" : \"type_of_data\" , \"subject\" : \"name_of_the_event_within_event_source\" } , \"data\" : { notification : [ { /* Minio notification. More info is available at https://docs.min.io/docs/minio-bucket-notification-guide.html } ] } }","title":"Event Structure"},{"location":"setup/minio/#setup","text":"Make sure to have minio server deployed and reachable from the gateway. More info on minio server setup is available at https://github.com/minio/minio/blob/master/docs/orchestration/kubernetes/k8s-yaml.md. If you are running Minio locally, make sure to port-forward to minio pod in order to make the service available outside local K8s cluster. kubectl - n argo - events port - forward < minio - pod - name > 9000 : 9000 Configure the minio client mc . mc config host add minio http : // localhost : 9000 minio minio123 Create a K8s secret that holds the access and secret key. This secret will be referred in the minio event source definition that we are going to install in a later step. apiVersion : v1 data : # base64 of minio accesskey : bWluaW8 = # base64 of minio123 secretkey : bWluaW8xMjM = kind : Secret metadata : name : artifacts - minio namespace : argo - events The event source we are going to use configures notifications for a bucket called input . mc mb minio / input If you inspect the gateway resource definition, you will notice that it refers to the event source minio-event-source . Lets install event source in the argo-events namespace, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / event - sources / minio . yaml Always make sure to first create a bucket on Minio and then refer it in event source. Install gateway in the argo-events namespace using following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / gateways / minio . yaml Once the gateway resource is created, the gateway controller will process it and create a pod. If you don't see the pod in argo-events namespace, check the gateway controller logs for errors. Check the gateway logs to make sure the gateway has processed the event source. Lets create the sensor, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / sensors / minio . yaml Create a file named and hello-world.txt and upload it onto to the input bucket. This will trigger the argo workflow. Run argo list to find the workflow.","title":"Setup"},{"location":"setup/minio/#troubleshoot","text":"Please read the FAQ .","title":"Troubleshoot"},{"location":"setup/mqtt/","text":"MQTT \u00b6 MQTT gateway listens to messages on from IoT devices and helps sensor trigger the workloads. Event Structure \u00b6 The structure of an event dispatched by the gateway to the sensor looks like following, { \" context \" : { \" type \" : \" type_of_gateway \" , \" specVersion \" : \" cloud_events_version \" , \" source \" : \" name_of_the_gateway \" , \" eventID \" : \" unique_event_id \" , \" time \" : \" event_time \" , \" dataContentType \" : \" type_of_data \" , \" subject \" : \" name_of_the_event_within_event_source \" }, \" data \" : { \" topic \" : \" Topic refers to the MQTT topic name \" , \" messageId \" : \" MessageId is the unique ID for the message \" , \" body \" : \" Body is the message payload \" } } Setup \u00b6 Make sure to setup the MQTT Broker and Bridge in Kubernetes if you don't already have one. Create the event source by running the following command. Make sure to update the appropriate fields. kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / event - sources / mqtt . yaml Create the gateway by running the following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / gateways / mqtt . yaml Inspect the gateway pod logs to make sure the gateway was able to subscribe to the topic specified in the event source to consume messages. Create the sensor by running the following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / sensors / mqtt . yaml Send message by using MQTT client. Once a message is published, an argo workflow will be triggered. Run argo list to find the workflow. Troubleshoot \u00b6 Please read the FAQ .","title":"MQTT"},{"location":"setup/mqtt/#mqtt","text":"MQTT gateway listens to messages on from IoT devices and helps sensor trigger the workloads.","title":"MQTT"},{"location":"setup/mqtt/#event-structure","text":"The structure of an event dispatched by the gateway to the sensor looks like following, { \" context \" : { \" type \" : \" type_of_gateway \" , \" specVersion \" : \" cloud_events_version \" , \" source \" : \" name_of_the_gateway \" , \" eventID \" : \" unique_event_id \" , \" time \" : \" event_time \" , \" dataContentType \" : \" type_of_data \" , \" subject \" : \" name_of_the_event_within_event_source \" }, \" data \" : { \" topic \" : \" Topic refers to the MQTT topic name \" , \" messageId \" : \" MessageId is the unique ID for the message \" , \" body \" : \" Body is the message payload \" } }","title":"Event Structure"},{"location":"setup/mqtt/#setup","text":"Make sure to setup the MQTT Broker and Bridge in Kubernetes if you don't already have one. Create the event source by running the following command. Make sure to update the appropriate fields. kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / event - sources / mqtt . yaml Create the gateway by running the following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / gateways / mqtt . yaml Inspect the gateway pod logs to make sure the gateway was able to subscribe to the topic specified in the event source to consume messages. Create the sensor by running the following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / sensors / mqtt . yaml Send message by using MQTT client. Once a message is published, an argo workflow will be triggered. Run argo list to find the workflow.","title":"Setup"},{"location":"setup/mqtt/#troubleshoot","text":"Please read the FAQ .","title":"Troubleshoot"},{"location":"setup/nats/","text":"NATS \u00b6 NATS gateway listens to NATS subject notifications and helps sensor trigger the workloads. Event Structure \u00b6 The structure of an event dispatched by the gateway to the sensor looks like following, { \"context\" : { \"type\" : \"type_of_gateway\" , \"specVersion\" : \"cloud_events_version\" , \"source\" : \"name_of_the_gateway\" , \"eventID\" : \"unique_event_id\" , \"time\" : \"event_time\" , \"dataContentType\" : \"type_of_data\" , \"subject\" : \"name_of_the_event_within_event_source\" } , \"data\" : { \"subject\" : \"name_of_the_nats_subject\" , \"body\" : \"message_payload\" } } Setup \u00b6 Make sure to have NATS cluster deployed in the Kubernetes. If you don't have one already installed, please refer https://github.com/nats-io/nats-operator for details. NATS cluster setup for test purposes, apiVersion : v1 kind : Service metadata : name : nats namespace : argo - events labels : component : nats spec : selector : component : nats type : ClusterIP ports : - name : client port : 4222 - name : cluster port : 6222 - name : monitor port : 8222 --- apiVersion : apps / v1beta1 kind : StatefulSet metadata : name : nats namespace : argo - events labels : component : nats spec : serviceName : nats replicas : 1 template : metadata : labels : component : nats spec : serviceAccountName : argo - events - sa containers : - name : nats image : nats : latest ports : - containerPort : 4222 name : client - containerPort : 6222 name : cluster - containerPort : 8222 name : monitor livenessProbe : httpGet : path : / port : 8222 initialDelaySeconds : 10 timeoutSeconds : 5 Create the event source by running the following command. kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / event - sources / nats . yaml Create the gateway by running the following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / gateways / nats . yaml Inspect the gateway pod logs to make sure the gateway was able to subscribe to the subject specified in the event source to consume messages. Create the sensor by running the following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / sensors / nats . yaml If you are running NATS on local K8s cluster, make sure to port-forward to pod, kubectl - n argo - events port - forward < nats - pod - name > 4222 : 4222 Publish a message for the subject specified in the event source. Refer the nats example to publish a message to the subject https://github.com/nats-io/go-nats-examples/tree/master/patterns/publish-subscribe. go run main . go - s localhost foo '{\"message\": \"hello\"}' Once a message is published, an argo workflow will be triggered. Run argo list to find the workflow. Troubleshoot \u00b6 Please read the FAQ .","title":"NATS"},{"location":"setup/nats/#nats","text":"NATS gateway listens to NATS subject notifications and helps sensor trigger the workloads.","title":"NATS"},{"location":"setup/nats/#event-structure","text":"The structure of an event dispatched by the gateway to the sensor looks like following, { \"context\" : { \"type\" : \"type_of_gateway\" , \"specVersion\" : \"cloud_events_version\" , \"source\" : \"name_of_the_gateway\" , \"eventID\" : \"unique_event_id\" , \"time\" : \"event_time\" , \"dataContentType\" : \"type_of_data\" , \"subject\" : \"name_of_the_event_within_event_source\" } , \"data\" : { \"subject\" : \"name_of_the_nats_subject\" , \"body\" : \"message_payload\" } }","title":"Event Structure"},{"location":"setup/nats/#setup","text":"Make sure to have NATS cluster deployed in the Kubernetes. If you don't have one already installed, please refer https://github.com/nats-io/nats-operator for details. NATS cluster setup for test purposes, apiVersion : v1 kind : Service metadata : name : nats namespace : argo - events labels : component : nats spec : selector : component : nats type : ClusterIP ports : - name : client port : 4222 - name : cluster port : 6222 - name : monitor port : 8222 --- apiVersion : apps / v1beta1 kind : StatefulSet metadata : name : nats namespace : argo - events labels : component : nats spec : serviceName : nats replicas : 1 template : metadata : labels : component : nats spec : serviceAccountName : argo - events - sa containers : - name : nats image : nats : latest ports : - containerPort : 4222 name : client - containerPort : 6222 name : cluster - containerPort : 8222 name : monitor livenessProbe : httpGet : path : / port : 8222 initialDelaySeconds : 10 timeoutSeconds : 5 Create the event source by running the following command. kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / event - sources / nats . yaml Create the gateway by running the following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / gateways / nats . yaml Inspect the gateway pod logs to make sure the gateway was able to subscribe to the subject specified in the event source to consume messages. Create the sensor by running the following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / sensors / nats . yaml If you are running NATS on local K8s cluster, make sure to port-forward to pod, kubectl - n argo - events port - forward < nats - pod - name > 4222 : 4222 Publish a message for the subject specified in the event source. Refer the nats example to publish a message to the subject https://github.com/nats-io/go-nats-examples/tree/master/patterns/publish-subscribe. go run main . go - s localhost foo '{\"message\": \"hello\"}' Once a message is published, an argo workflow will be triggered. Run argo list to find the workflow.","title":"Setup"},{"location":"setup/nats/#troubleshoot","text":"Please read the FAQ .","title":"Troubleshoot"},{"location":"setup/nsq/","text":"NSQ \u00b6 NSQ gateway subscribes to nsq pub/sub notifications and helps sensor trigger the workloads. Event Structure \u00b6 The structure of an event dispatched by the gateway to the sensor looks like following, { \"context\" : { \"type\" : \"type_of_gateway\" , \"specVersion\" : \"cloud_events_version\" , \"source\" : \"name_of_the_gateway\" , \"eventID\" : \"unique_event_id\" , \"time\" : \"event_time\" , \"dataContentType\" : \"type_of_data\" , \"subject\" : \"name_of_the_event_within_event_source\" } , \"data\" : { \"body\" : \"Body is the message data\" , \"timestamp\" : \"timestamp of the message\" , \"nsqdAddress\" : \"NSQDAddress is the address of the nsq host\" } } Setup \u00b6 Deploy NSQ on local K8s cluster apiVersion : v1 kind : Service metadata : name : nsqlookupd labels : app : nsq spec : ports : - port : 4160 targetPort : 4160 name : tcp - port : 4161 targetPort : 4161 name : http clusterIP : None selector : app : nsq component : nsqlookupd --- apiVersion : v1 kind : Service metadata : name : nsqd labels : app : nsq spec : ports : - port : 4150 targetPort : 4150 name : tcp - port : 4151 targetPort : 4151 name : http clusterIP : None selector : app : nsq component : nsqd --- apiVersion : v1 kind : Service metadata : name : nsqadmin labels : app : nsq spec : ports : - port : 4170 targetPort : 4170 name : tcp - port : 4171 targetPort : 4171 name : http selector : app : nsq component : nsqadmin --- apiVersion : apps / v1beta1 kind : StatefulSet metadata : name : nsqlookupd spec : serviceName : \"nsqlookupd\" replicas : 1 updateStrategy : type : RollingUpdate template : metadata : labels : app : nsq component : nsqlookupd spec : containers : - name : nsqlookupd image : nsqio / nsq : v1 . 1.0 imagePullPolicy : Always resources : requests : cpu : 30 m memory : 64 Mi ports : - containerPort : 4160 name : tcp - containerPort : 4161 name : http livenessProbe : httpGet : path : / ping port : http initialDelaySeconds : 5 readinessProbe : httpGet : path : / ping port : http initialDelaySeconds : 2 command : - / nsqlookupd terminationGracePeriodSeconds : 5 --- apiVersion : apps / v1beta1 kind : Deployment metadata : name : nsqd spec : replicas : 1 selector : matchLabels : app : nsq component : nsqd template : metadata : labels : app : nsq component : nsqd spec : containers : - name : nsqd image : nsqio / nsq : v1 . 1.0 imagePullPolicy : Always resources : requests : cpu : 30 m memory : 64 Mi ports : - containerPort : 4150 name : tcp - containerPort : 4151 name : http livenessProbe : httpGet : path : / ping port : http initialDelaySeconds : 5 readinessProbe : httpGet : path : / ping port : http initialDelaySeconds : 2 volumeMounts : - name : datadir mountPath : / data command : - / nsqd - - data - path - / data - - lookupd - tcp - address - nsqlookupd . argo - events . svc : 4160 - - broadcast - address - nsqd . argo - events . svc env : - name : HOSTNAME valueFrom : fieldRef : fieldPath : metadata . name terminationGracePeriodSeconds : 5 volumes : - name : datadir emptyDir : {} --- apiVersion : extensions / v1beta1 kind : Deployment metadata : name : nsqadmin spec : replicas : 1 template : metadata : labels : app : nsq component : nsqadmin spec : containers : - name : nsqadmin image : nsqio / nsq : v1 . 1.0 imagePullPolicy : Always resources : requests : cpu : 30 m memory : 64 Mi ports : - containerPort : 4170 name : tcp - containerPort : 4171 name : http livenessProbe : httpGet : path : / ping port : http initialDelaySeconds : 10 readinessProbe : httpGet : path : / ping port : http initialDelaySeconds : 5 command : - / nsqadmin - - lookupd - http - address - nsqlookupd . argo - events . svc : 4161 terminationGracePeriodSeconds : 5 Expose NSQD by kubectl port-forward , kubectl - n argo - events port - forward service / nsqd 4151 : 4151 Create topic hello and channel my-channel curl - X POST 'http://localhost:4151/topic/create?topic=hello' curl - X POST 'http://localhost:4151/channel/create?topic=hello&channel=my-channel' Create the event source by running the following command. kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / event - sources / nsq . yaml Create the gateway by running the following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / gateways / nsq . yaml Inspect the gateway pod logs to make sure the gateway was able to subscribe to the channel specified in the event source to consume messages. Create the sensor by running the following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / sensors / nsq . yaml Publish a message on topic hello and channel my-channel , curl - d '{\"message\": \"hello\"}' 'http://localhost:4151/pub?topic=hello&channel=my-channel' Once a message is published, an argo workflow will be triggered. Run argo list to find the workflow. Troubleshoot \u00b6 Please read the FAQ .","title":"NSQ"},{"location":"setup/nsq/#nsq","text":"NSQ gateway subscribes to nsq pub/sub notifications and helps sensor trigger the workloads.","title":"NSQ"},{"location":"setup/nsq/#event-structure","text":"The structure of an event dispatched by the gateway to the sensor looks like following, { \"context\" : { \"type\" : \"type_of_gateway\" , \"specVersion\" : \"cloud_events_version\" , \"source\" : \"name_of_the_gateway\" , \"eventID\" : \"unique_event_id\" , \"time\" : \"event_time\" , \"dataContentType\" : \"type_of_data\" , \"subject\" : \"name_of_the_event_within_event_source\" } , \"data\" : { \"body\" : \"Body is the message data\" , \"timestamp\" : \"timestamp of the message\" , \"nsqdAddress\" : \"NSQDAddress is the address of the nsq host\" } }","title":"Event Structure"},{"location":"setup/nsq/#setup","text":"Deploy NSQ on local K8s cluster apiVersion : v1 kind : Service metadata : name : nsqlookupd labels : app : nsq spec : ports : - port : 4160 targetPort : 4160 name : tcp - port : 4161 targetPort : 4161 name : http clusterIP : None selector : app : nsq component : nsqlookupd --- apiVersion : v1 kind : Service metadata : name : nsqd labels : app : nsq spec : ports : - port : 4150 targetPort : 4150 name : tcp - port : 4151 targetPort : 4151 name : http clusterIP : None selector : app : nsq component : nsqd --- apiVersion : v1 kind : Service metadata : name : nsqadmin labels : app : nsq spec : ports : - port : 4170 targetPort : 4170 name : tcp - port : 4171 targetPort : 4171 name : http selector : app : nsq component : nsqadmin --- apiVersion : apps / v1beta1 kind : StatefulSet metadata : name : nsqlookupd spec : serviceName : \"nsqlookupd\" replicas : 1 updateStrategy : type : RollingUpdate template : metadata : labels : app : nsq component : nsqlookupd spec : containers : - name : nsqlookupd image : nsqio / nsq : v1 . 1.0 imagePullPolicy : Always resources : requests : cpu : 30 m memory : 64 Mi ports : - containerPort : 4160 name : tcp - containerPort : 4161 name : http livenessProbe : httpGet : path : / ping port : http initialDelaySeconds : 5 readinessProbe : httpGet : path : / ping port : http initialDelaySeconds : 2 command : - / nsqlookupd terminationGracePeriodSeconds : 5 --- apiVersion : apps / v1beta1 kind : Deployment metadata : name : nsqd spec : replicas : 1 selector : matchLabels : app : nsq component : nsqd template : metadata : labels : app : nsq component : nsqd spec : containers : - name : nsqd image : nsqio / nsq : v1 . 1.0 imagePullPolicy : Always resources : requests : cpu : 30 m memory : 64 Mi ports : - containerPort : 4150 name : tcp - containerPort : 4151 name : http livenessProbe : httpGet : path : / ping port : http initialDelaySeconds : 5 readinessProbe : httpGet : path : / ping port : http initialDelaySeconds : 2 volumeMounts : - name : datadir mountPath : / data command : - / nsqd - - data - path - / data - - lookupd - tcp - address - nsqlookupd . argo - events . svc : 4160 - - broadcast - address - nsqd . argo - events . svc env : - name : HOSTNAME valueFrom : fieldRef : fieldPath : metadata . name terminationGracePeriodSeconds : 5 volumes : - name : datadir emptyDir : {} --- apiVersion : extensions / v1beta1 kind : Deployment metadata : name : nsqadmin spec : replicas : 1 template : metadata : labels : app : nsq component : nsqadmin spec : containers : - name : nsqadmin image : nsqio / nsq : v1 . 1.0 imagePullPolicy : Always resources : requests : cpu : 30 m memory : 64 Mi ports : - containerPort : 4170 name : tcp - containerPort : 4171 name : http livenessProbe : httpGet : path : / ping port : http initialDelaySeconds : 10 readinessProbe : httpGet : path : / ping port : http initialDelaySeconds : 5 command : - / nsqadmin - - lookupd - http - address - nsqlookupd . argo - events . svc : 4161 terminationGracePeriodSeconds : 5 Expose NSQD by kubectl port-forward , kubectl - n argo - events port - forward service / nsqd 4151 : 4151 Create topic hello and channel my-channel curl - X POST 'http://localhost:4151/topic/create?topic=hello' curl - X POST 'http://localhost:4151/channel/create?topic=hello&channel=my-channel' Create the event source by running the following command. kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / event - sources / nsq . yaml Create the gateway by running the following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / gateways / nsq . yaml Inspect the gateway pod logs to make sure the gateway was able to subscribe to the channel specified in the event source to consume messages. Create the sensor by running the following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / sensors / nsq . yaml Publish a message on topic hello and channel my-channel , curl - d '{\"message\": \"hello\"}' 'http://localhost:4151/pub?topic=hello&channel=my-channel' Once a message is published, an argo workflow will be triggered. Run argo list to find the workflow.","title":"Setup"},{"location":"setup/nsq/#troubleshoot","text":"Please read the FAQ .","title":"Troubleshoot"},{"location":"setup/redis/","text":"Redis \u00b6 Redis gateway subscribes to Redis publisher and helps sensor trigger workloads. Event Structure \u00b6 The structure of an event dispatched by the gateway to the sensor looks like following, { \"context\" : { \"type\" : \"type_of_gateway\" , \"specVersion\" : \"cloud_events_version\" , \"source\" : \"name_of_the_gateway\" , \"eventID\" : \"unique_event_id\" , \"time\" : \"event_time\" , \"dataContentType\" : \"type_of_data\" , \"subject\" : \"name_of_the_event_within_event_source\" } , \"data\" : { \"channel\" : \"Subscription channel\" , \"pattern\" : \"Message pattern\" , \"body\" : \"message body\" // string } } Setup \u00b6 Follow the documentation to set up Redis database. Create the event source by running the following command. kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / event - sources / redis . yaml Create the gateway by running the following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / gateways / redis . yaml Inspect the gateway pod logs to make sure the gateway was able to subscribe to the channel specified in the event source to consume messages. Create the sensor by running the following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / sensors / redis . yaml Log into redis pod using kubectl , kubectl - n argo - events exec - it < redis - pod - name > - c < redis - container - name > -- / bin / bash Run redis-cli and publish a message on FOO channel. PUBLISH FOO hello Once a message is published, an argo workflow will be triggered. Run argo list to find the workflow. Troubleshoot \u00b6 Please read the FAQ .","title":"Redis"},{"location":"setup/redis/#redis","text":"Redis gateway subscribes to Redis publisher and helps sensor trigger workloads.","title":"Redis"},{"location":"setup/redis/#event-structure","text":"The structure of an event dispatched by the gateway to the sensor looks like following, { \"context\" : { \"type\" : \"type_of_gateway\" , \"specVersion\" : \"cloud_events_version\" , \"source\" : \"name_of_the_gateway\" , \"eventID\" : \"unique_event_id\" , \"time\" : \"event_time\" , \"dataContentType\" : \"type_of_data\" , \"subject\" : \"name_of_the_event_within_event_source\" } , \"data\" : { \"channel\" : \"Subscription channel\" , \"pattern\" : \"Message pattern\" , \"body\" : \"message body\" // string } }","title":"Event Structure"},{"location":"setup/redis/#setup","text":"Follow the documentation to set up Redis database. Create the event source by running the following command. kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / event - sources / redis . yaml Create the gateway by running the following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / gateways / redis . yaml Inspect the gateway pod logs to make sure the gateway was able to subscribe to the channel specified in the event source to consume messages. Create the sensor by running the following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / sensors / redis . yaml Log into redis pod using kubectl , kubectl - n argo - events exec - it < redis - pod - name > - c < redis - container - name > -- / bin / bash Run redis-cli and publish a message on FOO channel. PUBLISH FOO hello Once a message is published, an argo workflow will be triggered. Run argo list to find the workflow.","title":"Setup"},{"location":"setup/redis/#troubleshoot","text":"Please read the FAQ .","title":"Troubleshoot"},{"location":"setup/resource/","text":"Resource \u00b6 Resource gateway watches change notifications for K8s object and helps sensor trigger the workloads. Event Structure \u00b6 The structure of an event dispatched by the gateway to the sensor looks like following, { \"context\" : { \"type\" : \"type_of_gateway\" , \"specVersion\" : \"cloud_events_version\" , \"source\" : \"name_of_the_gateway\" , \"eventID\" : \"unique_event_id\" , \"time\" : \"event_time\" , \"dataContentType\" : \"type_of_data\" , \"subject\" : \"name_of_the_event_within_event_source\" } , \"data\" : { \"type\" : \"type_of_the_event\" , // ADD , UPDATE or DELETE \"body\" : \"resource_body\" , // JSON format \"group\" : \"resource_group_name\" , \"version\" : \"resource_version_name\" , \"resource\" : \"resource_name\" } } Setup \u00b6 Create the event source by running the following command. kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / event - sources / resource . yaml Create the gateway by running the following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / gateways / resource . yaml Create the sensor by running the following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / sensors / resource . yaml The event source we created in step 1 contains configuration which makes the gateway listen to Argo workflows marked with label app: my-workflow . Lets create a workflow called my-workflow with label app: my-workflow apiVersion : argoproj . io / v1alpha1 kind : Workflow metadata : name : my - workflow labels : name : my - workflow spec : entrypoint : whalesay templates : - name : whalesay container : image : docker / whalesay : latest command : [ cowsay ] args : [ \"hello world\" ] Once the my-workflow is created, the sensor will trigger the workflow. Run argo list to list the triggered workflow. List Options \u00b6 The Resource Event-Source allows to configure the list options through labels and field selectors for setting up a watch on objects. In the example above, we had set up the list option as follows, filter : # labels and filters are meant to provide K8s API options to filter the object list that are being watched . # Please read https : // kubernetes . io / docs / concepts / overview / working - with - objects / labels / # api for more details . # labels provide listing options to K8s API to watch objects labels : - key : app # Supported operations like == , != , etc . # Defaults to == . # Refer https : // kubernetes . io / docs / concepts / overview / working - with - objects / labels / # label - selectors for more info . # optional . operation : \" == \" value : my - workflow The key-operation-value items under the filter -> labels are used by the gateway to filter the objects that are eligible for the watch. So, in the present case, the gateway will set up a watch for those objects who have label \"app: my-workflow\". You can add more key-operation-value items to the list as per your use-case. Similarly, you can pass field selectors to the watch list options, e.g., filter : # labels and filters are meant to provide K8s API options to filter the object list that are being watched . # Please read https : // kubernetes . io / docs / concepts / overview / working - with - objects / labels / # api for more details . # fields provide listing options to K8s API to watch objects fields : - key : metadata . name # Supported operations like == , != , <= , >= etc . # Defaults to == . # Refer https : // kubernetes . io / docs / concepts / overview / working - with - objects / field - selectors / for more info . # optional . operation : == value : my - workflow Note: The label and fields under filter are used at the time of setting up the watch by the gateway. If you want to filter the objects based on the annotations or some other fields, use the Data Filters available in sensor. Troubleshoot \u00b6 Please read the FAQ .","title":"Resource"},{"location":"setup/resource/#resource","text":"Resource gateway watches change notifications for K8s object and helps sensor trigger the workloads.","title":"Resource"},{"location":"setup/resource/#event-structure","text":"The structure of an event dispatched by the gateway to the sensor looks like following, { \"context\" : { \"type\" : \"type_of_gateway\" , \"specVersion\" : \"cloud_events_version\" , \"source\" : \"name_of_the_gateway\" , \"eventID\" : \"unique_event_id\" , \"time\" : \"event_time\" , \"dataContentType\" : \"type_of_data\" , \"subject\" : \"name_of_the_event_within_event_source\" } , \"data\" : { \"type\" : \"type_of_the_event\" , // ADD , UPDATE or DELETE \"body\" : \"resource_body\" , // JSON format \"group\" : \"resource_group_name\" , \"version\" : \"resource_version_name\" , \"resource\" : \"resource_name\" } }","title":"Event Structure"},{"location":"setup/resource/#setup","text":"Create the event source by running the following command. kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / event - sources / resource . yaml Create the gateway by running the following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / gateways / resource . yaml Create the sensor by running the following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / sensors / resource . yaml The event source we created in step 1 contains configuration which makes the gateway listen to Argo workflows marked with label app: my-workflow . Lets create a workflow called my-workflow with label app: my-workflow apiVersion : argoproj . io / v1alpha1 kind : Workflow metadata : name : my - workflow labels : name : my - workflow spec : entrypoint : whalesay templates : - name : whalesay container : image : docker / whalesay : latest command : [ cowsay ] args : [ \"hello world\" ] Once the my-workflow is created, the sensor will trigger the workflow. Run argo list to list the triggered workflow.","title":"Setup"},{"location":"setup/resource/#list-options","text":"The Resource Event-Source allows to configure the list options through labels and field selectors for setting up a watch on objects. In the example above, we had set up the list option as follows, filter : # labels and filters are meant to provide K8s API options to filter the object list that are being watched . # Please read https : // kubernetes . io / docs / concepts / overview / working - with - objects / labels / # api for more details . # labels provide listing options to K8s API to watch objects labels : - key : app # Supported operations like == , != , etc . # Defaults to == . # Refer https : // kubernetes . io / docs / concepts / overview / working - with - objects / labels / # label - selectors for more info . # optional . operation : \" == \" value : my - workflow The key-operation-value items under the filter -> labels are used by the gateway to filter the objects that are eligible for the watch. So, in the present case, the gateway will set up a watch for those objects who have label \"app: my-workflow\". You can add more key-operation-value items to the list as per your use-case. Similarly, you can pass field selectors to the watch list options, e.g., filter : # labels and filters are meant to provide K8s API options to filter the object list that are being watched . # Please read https : // kubernetes . io / docs / concepts / overview / working - with - objects / labels / # api for more details . # fields provide listing options to K8s API to watch objects fields : - key : metadata . name # Supported operations like == , != , <= , >= etc . # Defaults to == . # Refer https : // kubernetes . io / docs / concepts / overview / working - with - objects / field - selectors / for more info . # optional . operation : == value : my - workflow Note: The label and fields under filter are used at the time of setting up the watch by the gateway. If you want to filter the objects based on the annotations or some other fields, use the Data Filters available in sensor.","title":"List Options"},{"location":"setup/resource/#troubleshoot","text":"Please read the FAQ .","title":"Troubleshoot"},{"location":"setup/webhook/","text":"Webhook \u00b6 Webhook gateway exposes a http server and allows external entities to trigger workloads via http requests. Event Structure \u00b6 The structure of an event dispatched by the gateway to the sensor looks like following, { \"context\" : { \"type\" : \"type_of_gateway\" , \"specVersion\" : \"cloud_events_version\" , \"source\" : \"name_of_the_gateway\" , \"eventID\" : \"unique_event_id\" , \"time\" : \"event_time\" , \"dataContentType\" : \"type_of_data\" , \"subject\" : \"name_of_the_event_within_event_source\" } , \"data\" : { \"header\" : { /* the headers from the request received by the gateway from the external entity */ } , \"body\" : { /* the payload of the request received by the gateway from the external entity */ } , } } Setup \u00b6 Install gateway in the argo-events namespace using following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / gateways / webhook . yaml Once the gateway resource is created, the gateway controller will process it and create a pod and a service. If you don't see the pod and service in argo-events namespace, check the gateway controller logs for errors. If you inspect the gateway resource definition, you will notice it points to the event source called webhook-event-source . Lets install event source in the argo-events namespace, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / event - sources / webhook . yaml Check the gateway logs to make sure the gateway has processed the event source. The gateway is now listening for HTTP requests on port 12000 and endpoint /example . Its time to create the sensor, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / sensors / webhook . yaml Once the sensor pod is in running state, test the setup by sending a POST request to gateway service. Troubleshoot \u00b6 Please read the FAQ .","title":"Webhook"},{"location":"setup/webhook/#webhook","text":"Webhook gateway exposes a http server and allows external entities to trigger workloads via http requests.","title":"Webhook"},{"location":"setup/webhook/#event-structure","text":"The structure of an event dispatched by the gateway to the sensor looks like following, { \"context\" : { \"type\" : \"type_of_gateway\" , \"specVersion\" : \"cloud_events_version\" , \"source\" : \"name_of_the_gateway\" , \"eventID\" : \"unique_event_id\" , \"time\" : \"event_time\" , \"dataContentType\" : \"type_of_data\" , \"subject\" : \"name_of_the_event_within_event_source\" } , \"data\" : { \"header\" : { /* the headers from the request received by the gateway from the external entity */ } , \"body\" : { /* the payload of the request received by the gateway from the external entity */ } , } }","title":"Event Structure"},{"location":"setup/webhook/#setup","text":"Install gateway in the argo-events namespace using following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / gateways / webhook . yaml Once the gateway resource is created, the gateway controller will process it and create a pod and a service. If you don't see the pod and service in argo-events namespace, check the gateway controller logs for errors. If you inspect the gateway resource definition, you will notice it points to the event source called webhook-event-source . Lets install event source in the argo-events namespace, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / event - sources / webhook . yaml Check the gateway logs to make sure the gateway has processed the event source. The gateway is now listening for HTTP requests on port 12000 and endpoint /example . Its time to create the sensor, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / sensors / webhook . yaml Once the sensor pod is in running state, test the setup by sending a POST request to gateway service.","title":"Setup"},{"location":"setup/webhook/#troubleshoot","text":"Please read the FAQ .","title":"Troubleshoot"},{"location":"triggers/argo-workflow/","text":"Argo Workflow Trigger \u00b6 Argo workflow is K8s custom resource which help orchestrating parallel jobs on Kubernetes. Trigger a workflow \u00b6 We will use webhook gateway and sensor to trigger an Argo workflow. Lets set up a webhook gateway and event source to process incoming requests. kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / gateways / webhook . yaml kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / event - sources / webhook . yaml To trigger a workflow, we need to create a sensor as defined below, apiVersion : argoproj . io / v1alpha1 kind : Sensor metadata : name : webhook - sensor labels : sensors . argoproj . io / sensor - controller - instanceid : argo - events spec : template : spec : containers : - name : sensor image : argoproj / sensor : v0 .13.0 imagePullPolicy : Always serviceAccountName : argo - events - sa dependencies : - name : test - dep gatewayName : webhook - gateway eventName : example subscription : http : port : 9300 triggers : - template : name : webhook - workflow - trigger k8s : group : argoproj . io version : v1alpha1 resource : workflows operation : create source : resource : apiVersion : argoproj . io / v1alpha1 kind : Workflow metadata : generateName : webhook - spec : entrypoint : whalesay arguments : parameters : - name : message # the value will get overridden by event payload from test - dep value : hello world templates : - name : whalesay serviceAccountName : argo - events - sa inputs : parameters : - name : message container : image : docker / whalesay : latest command : [ cowsay ] args : [ \"{{inputs.parameters.message}}\" ] parameters : - src : dependencyName : test - dep dest : spec . arguments . parameters .0 . value Create the sensor, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / sensors / webhook . yaml Lets expose the webhook gateway using port-forward so that we can make a request to it. kubectl - n argo - events port - forward < name - of - gateway - pod > 12000 : 12000 Use either Curl or Postman to send a post request to the http://localhost:12000/example curl - d '{\"message\":\"ok\"}' - H \"Content-Type: application/json\" - X POST http : // localhost : 12000 / example List the workflow using argo list . Parameterization \u00b6 Similar to other type of triggers, sensor offers parameterization for the Argo workflow trigger. Parameterization is specially useful when you want to define a generic trigger template in the sensor and populate the workflow object values on the fly. You can learn more about trigger parameterization here . Policy \u00b6 Trigger policy helps you determine the status of the triggered Argo workflow object and decide whether to stop or continue sensor. Take a look at K8s Trigger Policy . Argo CLI \u00b6 Although the sensor defined above lets you trigger an Argo workflow, it doesn't have the ability to leverage the functionality provided by the Argo CLI such as, Submit Resubmit Resume Retry Suspend To make use of Argo CLI operations, The sensor provides the argoWorkflow trigger template, argoWorkflow : group : argoproj . io version : v1alpha1 resource : workflows operation : submit # submit , resubmit , resume , retry or suspend Complete example is available here .","title":"Argo Workflow Trigger"},{"location":"triggers/argo-workflow/#argo-workflow-trigger","text":"Argo workflow is K8s custom resource which help orchestrating parallel jobs on Kubernetes.","title":"Argo Workflow Trigger"},{"location":"triggers/argo-workflow/#trigger-a-workflow","text":"We will use webhook gateway and sensor to trigger an Argo workflow. Lets set up a webhook gateway and event source to process incoming requests. kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / gateways / webhook . yaml kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / event - sources / webhook . yaml To trigger a workflow, we need to create a sensor as defined below, apiVersion : argoproj . io / v1alpha1 kind : Sensor metadata : name : webhook - sensor labels : sensors . argoproj . io / sensor - controller - instanceid : argo - events spec : template : spec : containers : - name : sensor image : argoproj / sensor : v0 .13.0 imagePullPolicy : Always serviceAccountName : argo - events - sa dependencies : - name : test - dep gatewayName : webhook - gateway eventName : example subscription : http : port : 9300 triggers : - template : name : webhook - workflow - trigger k8s : group : argoproj . io version : v1alpha1 resource : workflows operation : create source : resource : apiVersion : argoproj . io / v1alpha1 kind : Workflow metadata : generateName : webhook - spec : entrypoint : whalesay arguments : parameters : - name : message # the value will get overridden by event payload from test - dep value : hello world templates : - name : whalesay serviceAccountName : argo - events - sa inputs : parameters : - name : message container : image : docker / whalesay : latest command : [ cowsay ] args : [ \"{{inputs.parameters.message}}\" ] parameters : - src : dependencyName : test - dep dest : spec . arguments . parameters .0 . value Create the sensor, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / sensors / webhook . yaml Lets expose the webhook gateway using port-forward so that we can make a request to it. kubectl - n argo - events port - forward < name - of - gateway - pod > 12000 : 12000 Use either Curl or Postman to send a post request to the http://localhost:12000/example curl - d '{\"message\":\"ok\"}' - H \"Content-Type: application/json\" - X POST http : // localhost : 12000 / example List the workflow using argo list .","title":"Trigger a workflow"},{"location":"triggers/argo-workflow/#parameterization","text":"Similar to other type of triggers, sensor offers parameterization for the Argo workflow trigger. Parameterization is specially useful when you want to define a generic trigger template in the sensor and populate the workflow object values on the fly. You can learn more about trigger parameterization here .","title":"Parameterization"},{"location":"triggers/argo-workflow/#policy","text":"Trigger policy helps you determine the status of the triggered Argo workflow object and decide whether to stop or continue sensor. Take a look at K8s Trigger Policy .","title":"Policy"},{"location":"triggers/argo-workflow/#argo-cli","text":"Although the sensor defined above lets you trigger an Argo workflow, it doesn't have the ability to leverage the functionality provided by the Argo CLI such as, Submit Resubmit Resume Retry Suspend To make use of Argo CLI operations, The sensor provides the argoWorkflow trigger template, argoWorkflow : group : argoproj . io version : v1alpha1 resource : workflows operation : submit # submit , resubmit , resume , retry or suspend Complete example is available here .","title":"Argo CLI"},{"location":"triggers/aws-lambda/","text":"AWS Lambda \u00b6 AWS Lambda provides a tremendous value but the event driven lambda invocation is limited to SNS, SQS and few other event sources. Argo Events makes it easy to integrate lambda with event sources that are not native to AWS. Trigger A Simple Lambda \u00b6 Make sure your AWS account has permissions to execute Lambda. More info on AWS permissions is available here . Fetch your access and secret key for AWS account and base64 encode them. Create a secret called aws-secret as follows, apiVersion : v1 kind : Secret metadata : name : aws - secret type : Opaque data : accesskey : < base64 - access - key > secretkey : < base64 - secret - key > Create a basic lambda function called hello either using AWS cli or console. exports . handler = async ( event , context ) => { console . log ( ' name = ' , event . name ) ; return event . name ; } ; Lets set up webhook gateway to invoke the lambda over http requests. kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / gateways / webhook . yaml kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / event - sources / webhook . yaml Lets expose the webhook gateway using port-forward so that we can make a request to it. kubectl - n argo - events port - forward < name - of - gateway - pod > 12000 : 12000 Deploy the webhook sensor with AWS Lambda trigger kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / sensors / aws - lambda - trigger . yaml Once the sensor pod is in running state, make a curl request to webhook gateway, curl - d '{\"name\":\"foo\"}' - H \"Content-Type: application/json\" - X POST http : // localhost : 12000 / example It will trigger the AWS Lambda function hello . Look at the CloudWatch logs to verify. Specification \u00b6 The AWS Lambda trigger specification is available here . Request Payload \u00b6 Invoking the AWS Lambda without a request payload would not be very useful. The lambda trigger within a sensor is invoked when sensor receives an event from a gateway. In order to construct a request payload based on the event data, sensor offers payload field as a part of the lambda trigger. Let's examine a lambda trigger, awsLambda : functionName : hello accessKey : name : aws - secret key : accesskey secretKey : name : aws - secret key : secretkey namespace : argo - events region : us - east - 1 payload : - src : dependencyName : test - dep dataKey : body . name dest : name The payload contains the list of src which refers to the source event and dest which refers to destination key within result request payload. The payload declared above will generate a request payload like below, { \"name\" : \"foo\" // name field from event data } The above payload will be passed in the request to invoke the AWS lambda. You can add however many number of src and dest under payload . Note : Take a look at Parameterization in order to understand how to extract particular key-value from event data. Parameterization \u00b6 Similar to other type of triggers, sensor offers parameterization for the AWS Lambda trigger. Parameterization is specially useful when you want to define a generic trigger template in the sensor and populate values like function name, payload values on the fly. Consider a scenario where you don't want to hard-code the function name and let the event data populate it. awsLambda : functionName : hello // this will be replaced . accessKey : name : aws - secret key : accesskey secretKey : name : aws - secret key : secretkey namespace : argo - events region : us - east - 1 payload : - src : dependencyName : test - dep dataKey : body . message dest : message parameters : - src : dependencyName : test - dep dataKey : body . function_name dest : functionName With parameters the sensor will replace the function name hello with the value of field function_name from event data. You can learn more about trigger parameterization here . Policy \u00b6 Trigger policy helps you determine the status of the lambda invocation and decide whether to stop or continue sensor. To determine whether the lamda was successful or not, Lambda trigger provides a Status policy. The Status holds a list of response statuses that are considered valid. awsLambda : functionName : hello accessKey : name : aws - secret key : accesskey secretKey : name : aws - secret key : secretkey namespace : argo - events region : us - east - 1 payload : - src : dependencyName : test - dep dataKey : body . message dest : message policy : status : allow : - 200 - 201 The above lambda trigger will be treated successful only if its invocation returns with either 200 or 201 status.","title":"AWS Lambda"},{"location":"triggers/aws-lambda/#aws-lambda","text":"AWS Lambda provides a tremendous value but the event driven lambda invocation is limited to SNS, SQS and few other event sources. Argo Events makes it easy to integrate lambda with event sources that are not native to AWS.","title":"AWS Lambda"},{"location":"triggers/aws-lambda/#trigger-a-simple-lambda","text":"Make sure your AWS account has permissions to execute Lambda. More info on AWS permissions is available here . Fetch your access and secret key for AWS account and base64 encode them. Create a secret called aws-secret as follows, apiVersion : v1 kind : Secret metadata : name : aws - secret type : Opaque data : accesskey : < base64 - access - key > secretkey : < base64 - secret - key > Create a basic lambda function called hello either using AWS cli or console. exports . handler = async ( event , context ) => { console . log ( ' name = ' , event . name ) ; return event . name ; } ; Lets set up webhook gateway to invoke the lambda over http requests. kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / gateways / webhook . yaml kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / event - sources / webhook . yaml Lets expose the webhook gateway using port-forward so that we can make a request to it. kubectl - n argo - events port - forward < name - of - gateway - pod > 12000 : 12000 Deploy the webhook sensor with AWS Lambda trigger kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / sensors / aws - lambda - trigger . yaml Once the sensor pod is in running state, make a curl request to webhook gateway, curl - d '{\"name\":\"foo\"}' - H \"Content-Type: application/json\" - X POST http : // localhost : 12000 / example It will trigger the AWS Lambda function hello . Look at the CloudWatch logs to verify.","title":"Trigger A Simple Lambda"},{"location":"triggers/aws-lambda/#specification","text":"The AWS Lambda trigger specification is available here .","title":"Specification"},{"location":"triggers/aws-lambda/#request-payload","text":"Invoking the AWS Lambda without a request payload would not be very useful. The lambda trigger within a sensor is invoked when sensor receives an event from a gateway. In order to construct a request payload based on the event data, sensor offers payload field as a part of the lambda trigger. Let's examine a lambda trigger, awsLambda : functionName : hello accessKey : name : aws - secret key : accesskey secretKey : name : aws - secret key : secretkey namespace : argo - events region : us - east - 1 payload : - src : dependencyName : test - dep dataKey : body . name dest : name The payload contains the list of src which refers to the source event and dest which refers to destination key within result request payload. The payload declared above will generate a request payload like below, { \"name\" : \"foo\" // name field from event data } The above payload will be passed in the request to invoke the AWS lambda. You can add however many number of src and dest under payload . Note : Take a look at Parameterization in order to understand how to extract particular key-value from event data.","title":"Request Payload"},{"location":"triggers/aws-lambda/#parameterization","text":"Similar to other type of triggers, sensor offers parameterization for the AWS Lambda trigger. Parameterization is specially useful when you want to define a generic trigger template in the sensor and populate values like function name, payload values on the fly. Consider a scenario where you don't want to hard-code the function name and let the event data populate it. awsLambda : functionName : hello // this will be replaced . accessKey : name : aws - secret key : accesskey secretKey : name : aws - secret key : secretkey namespace : argo - events region : us - east - 1 payload : - src : dependencyName : test - dep dataKey : body . message dest : message parameters : - src : dependencyName : test - dep dataKey : body . function_name dest : functionName With parameters the sensor will replace the function name hello with the value of field function_name from event data. You can learn more about trigger parameterization here .","title":"Parameterization"},{"location":"triggers/aws-lambda/#policy","text":"Trigger policy helps you determine the status of the lambda invocation and decide whether to stop or continue sensor. To determine whether the lamda was successful or not, Lambda trigger provides a Status policy. The Status holds a list of response statuses that are considered valid. awsLambda : functionName : hello accessKey : name : aws - secret key : accesskey secretKey : name : aws - secret key : secretkey namespace : argo - events region : us - east - 1 payload : - src : dependencyName : test - dep dataKey : body . message dest : message policy : status : allow : - 200 - 201 The above lambda trigger will be treated successful only if its invocation returns with either 200 or 201 status.","title":"Policy"},{"location":"triggers/http-trigger/","text":"HTTP Trigger \u00b6 Argo Events offers HTTP trigger which can easily invoke serverless functions like OpenFaas, Kubeless, Knative, Nuclio and make REST API calls. Specification \u00b6 The HTTP trigger specification is available here . REST API Calls \u00b6 Consider a scenario where your REST API server needs to consume events from event-sources S3, GitHub, SQS etc. Usually, you'd end up writing the integration yourself in the server code, although server logic has nothing to do any of the event-sources. This is where Argo Events HTTP trigger can hel. The HTTP trigger takes the task of consuming events from event-sources away from API server and seamlessly integrates these events via REST API calls. We will set up a basic go http server and connect it with the minio events. The HTTP server simply prints the request body as follows, package main import ( \"fmt\" \"io/ioutil\" \"net/http\" ) func hello ( w http . ResponseWriter , req * http . Request ) { body , err : = ioutil . ReadAll ( req . Body ) if err != nil { fmt . Printf ( \"%+v \\n \" , err ) return } fmt . Println ( string ( body )) fmt . Fprintf ( w , \"hello \\n \" ) } func main () { http . HandleFunc ( \"/hello\" , hello ) fmt . Println ( \"server is listening on 8090\" ) http . ListenAndServe ( \":8090\" , nil ) } Deploy the HTTP server, kubectl - n argo - events apply - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / tutorials / 09 - http - trigger / http - server . yaml Create a service to expose the http server kubectl - n argo - events apply - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / tutorials / 09 - http - trigger / http - server - svc . yaml Either use Ingress, OpenShift Route or port-forwarding to expose the http server.. kubectl - n argo - events port - forward < http - server - pod - name > 8090 : 8090 Our goals is to seamlessly integrate Minio S3 bucket notifications with REST API server created in previous step. So, lets set up the Minio Gateway and EventSource available here . Don't create the sensor as we will be deploying it in next step. Create a sensor as follows, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / sensors / http - trigger . yaml Now, drop a file onto input bucket in Minio server. The sensor has triggered a http request to the http server. Take a look at the logs server is listening on 8090 { \"type\" : \"minio\" , \"bucket\" : \"input\" } Great!!! Request Payload \u00b6 In order to construct a request payload based on the event data, sensor offers payload field as a part of the HTP trigger. Let's examine a HTTP trigger, http : url : http : // http - server . argo - events . svc : 8090 / hello payload : - src : dependencyName : test - dep dataKey : notification . 0 . s3 . bucket . name dest : bucket - src : dependencyName : test - dep contextKey : type dest : type method : POST // GET , DELETE , POST , PUT , HEAD , etc . The payload contains the list of src which refers to the source event and dest which refers to destination key within result request payload. The payload declared above will generate a request payload like below, { \"type\" : \"type of event from event's context\" \"bucket\" : \"bucket name from event data\" } The above payload will be passed in the HTTP request. You can add however many number of src and dest under payload . Note : Take a look at Parameterization in order to understand how to extract particular key-value from event data. Parameterization \u00b6 Similar to other type of triggers, sensor offers parameterization for the HTTP trigger. Parameterization is specially useful when you want to define a generic trigger template in the sensor and populate values like URL, payload values on the fly. You can learn more about trigger parameterization here . Policy \u00b6 Trigger policy helps you determine the status of the HTTP request and decide whether to stop or continue sensor. To determine whether the HTTP request was successful or not, the HTTP trigger provides a Status policy. The Status holds a list of response statuses that are considered valid. http : url : http : // http - server . argo - events . svc : 8090 / hello payload : - src : dependencyName : test - dep dataKey : notification . 0 s3 . bucket . name dest : bucket - src : dependencyName : test - dep contextKey : type dest : type method : POST // GET , DELETE , POST , PUT , HEAD , etc . policy : status : allow : - 200 - 201 The above HTTP trigger will be treated successful only if the HTTP request returns with either 200 or 201 status. OpenFaas \u00b6 OpenFaas offers a simple way to spin up serverless functions. Lets see how we can leverage Argo Events HTTP trigger to invoke OpenFaas function. If you don't have OpenFaas installed, follow the instructions . Lets create a basic function. You can follow the steps to set up the function. package function import ( \"fmt\" ) // Handle a serverless request func Handle ( req [] byte ) string { return fmt . Sprintf ( \"Hello, Go. You said: %s \" , string ( req )) } Make sure the function pod is up and running. We are going to invoke OpenFaas function on a message on Redis Subscriber. Lets set up the Redis Database, Redis PubSub Gateway and EventSource as specified here . Do not create the Redis sensor, we are going to create it in next step. Lets create the sensor with OpenFaas trigger apiVersion : argoproj . io / v1alpha1 kind : Sensor metadata : name : redis - sensor labels : sensors . argoproj . io / sensor - controller - instanceid : argo - events spec : template : spec : containers : - name : sensor image : argoproj / sensor : v0 . 13.0 imagePullPolicy : Always serviceAccountName : argo - events - sa dependencies : - name : test - dep gatewayName : redis - gateway eventName : example subscription : http : port : 9300 triggers : - template : name : openfaas - trigger http : url : http :// gateway . openfaas . svc . cluster . local : 8080 /function/g ohash payload : - src : dependencyName : test - dep dest : bucket method : POST Publish a message on FOO channel using redis-cli . PUBLISH FOO hello As soon as you publish the message, the sensor will invoke the OpenFaas function gohash . Kubeless \u00b6 Similar to REST API calls, you can easily invoke Kubeless functions using HTTP trigger. If you don't have Kubeless installed, follow the installation . Lets create a basic function, def hello ( event , context ) : print event return event [ ' data ' ] Make sure the function pod and service is created. Now, we are going to invoke the Kubeless function when a message is placed on a NATS queue. Lets set up the NATS Gateway and EventSource. Follow instructions for details. Do not create the NATS sensor, we are going to create it in next step. Lets create NATS sensor with HTTP trigger, apiVersion : argoproj . io / v1alpha1 kind : Sensor metadata : name : nats - sensor labels : sensors . argoproj . io / sensor - controller - instanceid : argo - events spec : template : spec : containers : - name : sensor image : argoproj / sensor : v0 . 13.0 imagePullPolicy : Always serviceAccountName : argo - events - sa dependencies : - name : test - dep gatewayName : nats - gateway eventName : example subscription : http : port : 9300 triggers : - template : name : kubeless - trigger http : serverURL : http :// hello . kubeless . svc . cluster . local : 8080 payload : - src : dependencyName : test - dep dataKey : body . first_name dest : first_name - src : dependencyName : test - dep dataKey : body . last_name dest : last_name method : POST Once gateway and sensor pod are up and running, dispatch a message on foo subject using nats client, go run main . go - s localhost foo '{\"first_name\": \"foo\", \"last_name\": \"bar\"}' It will invoke Kubeless function hello , { 'event-time' : None , 'extensions' : { 'request' : < LocalRequest : POST http : // hello . kubeless . svc . cluster . local : 8080 /> } , 'event-type' : None , 'event-namespace' : None , 'data' : '{\"first_name\":\"foo\",\"last_name\":\"bar\"}' , 'event-id' : None } Other serverless frameworks \u00b6 Similar to OpenFaas and Kubeless invocation demonstrated above, you can easily trigger KNative, Nuclio, Fission functions using HTTP trigger.","title":"HTTP Trigger"},{"location":"triggers/http-trigger/#http-trigger","text":"Argo Events offers HTTP trigger which can easily invoke serverless functions like OpenFaas, Kubeless, Knative, Nuclio and make REST API calls.","title":"HTTP Trigger"},{"location":"triggers/http-trigger/#specification","text":"The HTTP trigger specification is available here .","title":"Specification"},{"location":"triggers/http-trigger/#rest-api-calls","text":"Consider a scenario where your REST API server needs to consume events from event-sources S3, GitHub, SQS etc. Usually, you'd end up writing the integration yourself in the server code, although server logic has nothing to do any of the event-sources. This is where Argo Events HTTP trigger can hel. The HTTP trigger takes the task of consuming events from event-sources away from API server and seamlessly integrates these events via REST API calls. We will set up a basic go http server and connect it with the minio events. The HTTP server simply prints the request body as follows, package main import ( \"fmt\" \"io/ioutil\" \"net/http\" ) func hello ( w http . ResponseWriter , req * http . Request ) { body , err : = ioutil . ReadAll ( req . Body ) if err != nil { fmt . Printf ( \"%+v \\n \" , err ) return } fmt . Println ( string ( body )) fmt . Fprintf ( w , \"hello \\n \" ) } func main () { http . HandleFunc ( \"/hello\" , hello ) fmt . Println ( \"server is listening on 8090\" ) http . ListenAndServe ( \":8090\" , nil ) } Deploy the HTTP server, kubectl - n argo - events apply - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / tutorials / 09 - http - trigger / http - server . yaml Create a service to expose the http server kubectl - n argo - events apply - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / tutorials / 09 - http - trigger / http - server - svc . yaml Either use Ingress, OpenShift Route or port-forwarding to expose the http server.. kubectl - n argo - events port - forward < http - server - pod - name > 8090 : 8090 Our goals is to seamlessly integrate Minio S3 bucket notifications with REST API server created in previous step. So, lets set up the Minio Gateway and EventSource available here . Don't create the sensor as we will be deploying it in next step. Create a sensor as follows, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / sensors / http - trigger . yaml Now, drop a file onto input bucket in Minio server. The sensor has triggered a http request to the http server. Take a look at the logs server is listening on 8090 { \"type\" : \"minio\" , \"bucket\" : \"input\" } Great!!!","title":"REST API Calls"},{"location":"triggers/http-trigger/#request-payload","text":"In order to construct a request payload based on the event data, sensor offers payload field as a part of the HTP trigger. Let's examine a HTTP trigger, http : url : http : // http - server . argo - events . svc : 8090 / hello payload : - src : dependencyName : test - dep dataKey : notification . 0 . s3 . bucket . name dest : bucket - src : dependencyName : test - dep contextKey : type dest : type method : POST // GET , DELETE , POST , PUT , HEAD , etc . The payload contains the list of src which refers to the source event and dest which refers to destination key within result request payload. The payload declared above will generate a request payload like below, { \"type\" : \"type of event from event's context\" \"bucket\" : \"bucket name from event data\" } The above payload will be passed in the HTTP request. You can add however many number of src and dest under payload . Note : Take a look at Parameterization in order to understand how to extract particular key-value from event data.","title":"Request Payload"},{"location":"triggers/http-trigger/#parameterization","text":"Similar to other type of triggers, sensor offers parameterization for the HTTP trigger. Parameterization is specially useful when you want to define a generic trigger template in the sensor and populate values like URL, payload values on the fly. You can learn more about trigger parameterization here .","title":"Parameterization"},{"location":"triggers/http-trigger/#policy","text":"Trigger policy helps you determine the status of the HTTP request and decide whether to stop or continue sensor. To determine whether the HTTP request was successful or not, the HTTP trigger provides a Status policy. The Status holds a list of response statuses that are considered valid. http : url : http : // http - server . argo - events . svc : 8090 / hello payload : - src : dependencyName : test - dep dataKey : notification . 0 s3 . bucket . name dest : bucket - src : dependencyName : test - dep contextKey : type dest : type method : POST // GET , DELETE , POST , PUT , HEAD , etc . policy : status : allow : - 200 - 201 The above HTTP trigger will be treated successful only if the HTTP request returns with either 200 or 201 status.","title":"Policy"},{"location":"triggers/http-trigger/#openfaas","text":"OpenFaas offers a simple way to spin up serverless functions. Lets see how we can leverage Argo Events HTTP trigger to invoke OpenFaas function. If you don't have OpenFaas installed, follow the instructions . Lets create a basic function. You can follow the steps to set up the function. package function import ( \"fmt\" ) // Handle a serverless request func Handle ( req [] byte ) string { return fmt . Sprintf ( \"Hello, Go. You said: %s \" , string ( req )) } Make sure the function pod is up and running. We are going to invoke OpenFaas function on a message on Redis Subscriber. Lets set up the Redis Database, Redis PubSub Gateway and EventSource as specified here . Do not create the Redis sensor, we are going to create it in next step. Lets create the sensor with OpenFaas trigger apiVersion : argoproj . io / v1alpha1 kind : Sensor metadata : name : redis - sensor labels : sensors . argoproj . io / sensor - controller - instanceid : argo - events spec : template : spec : containers : - name : sensor image : argoproj / sensor : v0 . 13.0 imagePullPolicy : Always serviceAccountName : argo - events - sa dependencies : - name : test - dep gatewayName : redis - gateway eventName : example subscription : http : port : 9300 triggers : - template : name : openfaas - trigger http : url : http :// gateway . openfaas . svc . cluster . local : 8080 /function/g ohash payload : - src : dependencyName : test - dep dest : bucket method : POST Publish a message on FOO channel using redis-cli . PUBLISH FOO hello As soon as you publish the message, the sensor will invoke the OpenFaas function gohash .","title":"OpenFaas"},{"location":"triggers/http-trigger/#kubeless","text":"Similar to REST API calls, you can easily invoke Kubeless functions using HTTP trigger. If you don't have Kubeless installed, follow the installation . Lets create a basic function, def hello ( event , context ) : print event return event [ ' data ' ] Make sure the function pod and service is created. Now, we are going to invoke the Kubeless function when a message is placed on a NATS queue. Lets set up the NATS Gateway and EventSource. Follow instructions for details. Do not create the NATS sensor, we are going to create it in next step. Lets create NATS sensor with HTTP trigger, apiVersion : argoproj . io / v1alpha1 kind : Sensor metadata : name : nats - sensor labels : sensors . argoproj . io / sensor - controller - instanceid : argo - events spec : template : spec : containers : - name : sensor image : argoproj / sensor : v0 . 13.0 imagePullPolicy : Always serviceAccountName : argo - events - sa dependencies : - name : test - dep gatewayName : nats - gateway eventName : example subscription : http : port : 9300 triggers : - template : name : kubeless - trigger http : serverURL : http :// hello . kubeless . svc . cluster . local : 8080 payload : - src : dependencyName : test - dep dataKey : body . first_name dest : first_name - src : dependencyName : test - dep dataKey : body . last_name dest : last_name method : POST Once gateway and sensor pod are up and running, dispatch a message on foo subject using nats client, go run main . go - s localhost foo '{\"first_name\": \"foo\", \"last_name\": \"bar\"}' It will invoke Kubeless function hello , { 'event-time' : None , 'extensions' : { 'request' : < LocalRequest : POST http : // hello . kubeless . svc . cluster . local : 8080 /> } , 'event-type' : None , 'event-namespace' : None , 'data' : '{\"first_name\":\"foo\",\"last_name\":\"bar\"}' , 'event-id' : None }","title":"Kubeless"},{"location":"triggers/http-trigger/#other-serverless-frameworks","text":"Similar to OpenFaas and Kubeless invocation demonstrated above, you can easily trigger KNative, Nuclio, Fission functions using HTTP trigger.","title":"Other serverless frameworks"},{"location":"triggers/k8s-object-trigger/","text":"Kubernetes Object Trigger \u00b6 Apart from Argo workflow objects, the sensor lets you trigger standard Kubernetes objects such as Pod, Deployment, Job, CronJob, etc. Having the ability to trigger standard Kubernetes objects is quite powerful as provides an avenue to set up event-driven pipelines for existing workloads. Trigger a K8s Pod \u00b6 We will use webhook gateway and sensor to trigger a K8s pod. Lets set up a webhook gateway and event source to process incoming requests. kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / gateways / webhook . yaml kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / event - sources / webhook . yaml To trigger a pod, we need to create a sensor as defined below, apiVersion : argoproj . io / v1alpha1 kind : Sensor metadata : name : webhook - sensor labels : sensors . argoproj . io / sensor - controller - instanceid : argo - events spec : template : spec : containers : - name : sensor image : argoproj / sensor : v0 . 13.0 imagePullPolicy : Always serviceAccountName : argo - events - sa dependencies : - name : test - dep gatewayName : webhook - gateway eventName : example subscription : http : port : 9300 triggers : - template : name : webhook - pod - trigger k8s : group : \"\" version : v1 resource : pods operation : create source : resource : apiVersion : v1 kind : Pod metadata : generateName : hello - world - spec : containers : - name : hello - container args : - \"hello-world\" command : - cowsay image : \"docker/whalesay:latest\" parameters : - src : dependencyName : test - dep dest : spec . containers . 0 . args . 0 The group , version and resource under k8s in the trigger template determines the type of K8s object. Change it accordingly if you want to trigger something else than a pod. Create the sensor, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / sensors / trigger - standard - k8s - resource . yaml Lets expose the webhook gateway using port-forward so that we can make a request to it. kubectl - n argo - events port - forward < name - of - gateway - pod > 12000 : 12000 Use either Curl or Postman to send a post request to the http://localhost:12000/example curl - d '{\"message\":\"ok\"}' - H \"Content-Type: application/json\" - X POST http : // localhost : 12000 / example Inspect the logs of the pod, you will something similar as below, _________________________________________ / { \"context\" : { \"type\" : \"webhook\" , \"specVersi \\ | on\" : \"0.3\" , \"source\" : \"webhook-gateway\" , \"e | | ventID\" : \"30306463666539362d346666642d34 | | 3336332d383861312d336538363333613564313 | | 932\" , \"time\" : \"2020-01-11T21:23:07.682961 | | Z\" , \"dataContentType\" : \"application/json\" | | , \"subject\" : \"example\" } , \"data\" : \"eyJoZWFkZ | | XIiOnsiQWNjZXB0IjpbIiovKiJdLCJDb250ZW50 | | LUxlbmd0aCI6WyIxOSJdLCJDb250ZW50LVR5cGU | | iOlsiYXBwbGljYXRpb24vanNvbiJdLCJVc2VyLU | | FnZW50IjpbImN1cmwvNy41NC4wIl19LCJib2R5I | \\ jp7Im1lc3NhZ2UiOiJoZXkhISJ9fQ==\" } / ----------------------------------------- \\ \\ \\ ## . ## ## ## == ## ## ## ## === / \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" ___ / === ~~~ { ~~ ~~~~ ~~~ ~~~~ ~~ ~ / ===- ~~~ \\ ______ o __ / \\ \\ __ / \\ ____ \\ ______ / Parameterization \u00b6 Similar to other type of triggers, sensor offers parameterization for the K8s trigger. Parameterization is specially useful when you want to define a generic trigger template in the sensor and populate the K8s object values on the fly. You can learn more about trigger parameterization here . Policy \u00b6 Trigger policy helps you determine the status of the triggered K8s object and decide whether to stop or continue sensor. To determine whether the K8s object was successful or not, the K8s trigger provides a Resource Labels policy. The Resource Labels holds a list of labels which are checked against the triggered K8s object to determine the status of the object. # Policy to configure backoff and execution criteria for the trigger # Because the sensor is able to trigger any K8s resource , it determines the resource state by looking at the resource ' s labels. policy : k8s : # Backoff before checking the resource labels backoff : # Duration is the duration in nanoseconds duration : 1000000000 # 1 second # Duration is multiplied by factor each iteration factor : 2 # The amount of jitter applied each iteration jitter : 0 . 1 # Exit with error after these many steps steps : 5 # labels set on the resource decide if the resource has transitioned into the success state . labels : workflows . argoproj . io / phase : Succeeded # Determines whether trigger should be marked as failed if the backoff times out and sensor is still unable to decide the state of the trigger . # defaults to false errorOnBackoffTimeout : true Complete example is available here .","title":"Kubernetes Object Trigger"},{"location":"triggers/k8s-object-trigger/#kubernetes-object-trigger","text":"Apart from Argo workflow objects, the sensor lets you trigger standard Kubernetes objects such as Pod, Deployment, Job, CronJob, etc. Having the ability to trigger standard Kubernetes objects is quite powerful as provides an avenue to set up event-driven pipelines for existing workloads.","title":"Kubernetes Object Trigger"},{"location":"triggers/k8s-object-trigger/#trigger-a-k8s-pod","text":"We will use webhook gateway and sensor to trigger a K8s pod. Lets set up a webhook gateway and event source to process incoming requests. kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / gateways / webhook . yaml kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / event - sources / webhook . yaml To trigger a pod, we need to create a sensor as defined below, apiVersion : argoproj . io / v1alpha1 kind : Sensor metadata : name : webhook - sensor labels : sensors . argoproj . io / sensor - controller - instanceid : argo - events spec : template : spec : containers : - name : sensor image : argoproj / sensor : v0 . 13.0 imagePullPolicy : Always serviceAccountName : argo - events - sa dependencies : - name : test - dep gatewayName : webhook - gateway eventName : example subscription : http : port : 9300 triggers : - template : name : webhook - pod - trigger k8s : group : \"\" version : v1 resource : pods operation : create source : resource : apiVersion : v1 kind : Pod metadata : generateName : hello - world - spec : containers : - name : hello - container args : - \"hello-world\" command : - cowsay image : \"docker/whalesay:latest\" parameters : - src : dependencyName : test - dep dest : spec . containers . 0 . args . 0 The group , version and resource under k8s in the trigger template determines the type of K8s object. Change it accordingly if you want to trigger something else than a pod. Create the sensor, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / sensors / trigger - standard - k8s - resource . yaml Lets expose the webhook gateway using port-forward so that we can make a request to it. kubectl - n argo - events port - forward < name - of - gateway - pod > 12000 : 12000 Use either Curl or Postman to send a post request to the http://localhost:12000/example curl - d '{\"message\":\"ok\"}' - H \"Content-Type: application/json\" - X POST http : // localhost : 12000 / example Inspect the logs of the pod, you will something similar as below, _________________________________________ / { \"context\" : { \"type\" : \"webhook\" , \"specVersi \\ | on\" : \"0.3\" , \"source\" : \"webhook-gateway\" , \"e | | ventID\" : \"30306463666539362d346666642d34 | | 3336332d383861312d336538363333613564313 | | 932\" , \"time\" : \"2020-01-11T21:23:07.682961 | | Z\" , \"dataContentType\" : \"application/json\" | | , \"subject\" : \"example\" } , \"data\" : \"eyJoZWFkZ | | XIiOnsiQWNjZXB0IjpbIiovKiJdLCJDb250ZW50 | | LUxlbmd0aCI6WyIxOSJdLCJDb250ZW50LVR5cGU | | iOlsiYXBwbGljYXRpb24vanNvbiJdLCJVc2VyLU | | FnZW50IjpbImN1cmwvNy41NC4wIl19LCJib2R5I | \\ jp7Im1lc3NhZ2UiOiJoZXkhISJ9fQ==\" } / ----------------------------------------- \\ \\ \\ ## . ## ## ## == ## ## ## ## === / \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" ___ / === ~~~ { ~~ ~~~~ ~~~ ~~~~ ~~ ~ / ===- ~~~ \\ ______ o __ / \\ \\ __ / \\ ____ \\ ______ /","title":"Trigger a K8s Pod"},{"location":"triggers/k8s-object-trigger/#parameterization","text":"Similar to other type of triggers, sensor offers parameterization for the K8s trigger. Parameterization is specially useful when you want to define a generic trigger template in the sensor and populate the K8s object values on the fly. You can learn more about trigger parameterization here .","title":"Parameterization"},{"location":"triggers/k8s-object-trigger/#policy","text":"Trigger policy helps you determine the status of the triggered K8s object and decide whether to stop or continue sensor. To determine whether the K8s object was successful or not, the K8s trigger provides a Resource Labels policy. The Resource Labels holds a list of labels which are checked against the triggered K8s object to determine the status of the object. # Policy to configure backoff and execution criteria for the trigger # Because the sensor is able to trigger any K8s resource , it determines the resource state by looking at the resource ' s labels. policy : k8s : # Backoff before checking the resource labels backoff : # Duration is the duration in nanoseconds duration : 1000000000 # 1 second # Duration is multiplied by factor each iteration factor : 2 # The amount of jitter applied each iteration jitter : 0 . 1 # Exit with error after these many steps steps : 5 # labels set on the resource decide if the resource has transitioned into the success state . labels : workflows . argoproj . io / phase : Succeeded # Determines whether trigger should be marked as failed if the backoff times out and sensor is still unable to decide the state of the trigger . # defaults to false errorOnBackoffTimeout : true Complete example is available here .","title":"Policy"},{"location":"triggers/kafka-trigger/","text":"Kafka Trigger \u00b6 Kafka trigger allows sensor to publish events on Kafka topic. This trigger helps source the events from outside world into your messaging queues. Specification \u00b6 The Kafka trigger specification is available here . Walkthrough \u00b6 Consider a scenario where you are expecting a file drop onto a Minio bucket and want to place that event on a Kafka topic. Set up the Minio Event Source and Gateway here . Do not create the Minio sensor, we are going to create it in next step. Lets create the sensor, apiVersion : argoproj . io / v1alpha1 kind : Sensor metadata : name : minio - sensor labels : sensors . argoproj . io / sensor - controller - instanceid : argo - events spec : template : spec : containers : - name : sensor image : argoproj / sensor : v0 . 13.0 imagePullPolicy : Always serviceAccountName : argo - events - sa dependencies : - name : test - dep gatewayName : minio - gateway eventName : example subscription : http : port : 9300 triggers : - template : name : kafka - trigger kafka : # Kafka URL url : kafka . argo - events . svc : 9092 # Name of the topic topic : minio - events # partition id partition : 0 payload : - src : dependencyName : test - dep dataKey : notification . 0 . s3 . object . key dest : fileName - src : dependencyName : test - dep dataKey : notification . 0 . s3 . bucket . name dest : bucket The Kafka message needs a body. In order to construct message based on the event data, sensor offers payload field as a part of the Kafka trigger. The payload contains the list of src which refers to the source event and dest which refers to destination key within result request payload. The payload declared above will generate a message body like below, { \"fileName\" : \"hello.txt\" // name / key of the object \"bucket\" : \"input\" // name of the bucket } Drop a file called hello.txt onto the bucket input and you will receive the message on Kafka topic","title":"Kafka Trigger"},{"location":"triggers/kafka-trigger/#kafka-trigger","text":"Kafka trigger allows sensor to publish events on Kafka topic. This trigger helps source the events from outside world into your messaging queues.","title":"Kafka Trigger"},{"location":"triggers/kafka-trigger/#specification","text":"The Kafka trigger specification is available here .","title":"Specification"},{"location":"triggers/kafka-trigger/#walkthrough","text":"Consider a scenario where you are expecting a file drop onto a Minio bucket and want to place that event on a Kafka topic. Set up the Minio Event Source and Gateway here . Do not create the Minio sensor, we are going to create it in next step. Lets create the sensor, apiVersion : argoproj . io / v1alpha1 kind : Sensor metadata : name : minio - sensor labels : sensors . argoproj . io / sensor - controller - instanceid : argo - events spec : template : spec : containers : - name : sensor image : argoproj / sensor : v0 . 13.0 imagePullPolicy : Always serviceAccountName : argo - events - sa dependencies : - name : test - dep gatewayName : minio - gateway eventName : example subscription : http : port : 9300 triggers : - template : name : kafka - trigger kafka : # Kafka URL url : kafka . argo - events . svc : 9092 # Name of the topic topic : minio - events # partition id partition : 0 payload : - src : dependencyName : test - dep dataKey : notification . 0 . s3 . object . key dest : fileName - src : dependencyName : test - dep dataKey : notification . 0 . s3 . bucket . name dest : bucket The Kafka message needs a body. In order to construct message based on the event data, sensor offers payload field as a part of the Kafka trigger. The payload contains the list of src which refers to the source event and dest which refers to destination key within result request payload. The payload declared above will generate a message body like below, { \"fileName\" : \"hello.txt\" // name / key of the object \"bucket\" : \"input\" // name of the bucket } Drop a file called hello.txt onto the bucket input and you will receive the message on Kafka topic","title":"Walkthrough"},{"location":"triggers/nats-trigger/","text":"NATS Trigger \u00b6 NATS trigger allows sensor to publish events on NATS subjects. This trigger helps source the events from outside world into your messaging queues. Specification \u00b6 The NATS trigger specification is available here . Walkthrough \u00b6 Consider a scenario where you are expecting a file drop onto a Minio bucket and want to place that event on a NATS subject. Set up the Minio Event Source and Gateway here . Do not create the Minio sensor, we are going to create it in next step. Lets create the sensor, apiVersion : argoproj . io / v1alpha1 kind : Sensor metadata : name : minio - sensor labels : sensors . argoproj . io / sensor - controller - instanceid : argo - events spec : template : spec : containers : - name : sensor image : argoproj / sensor : v0 . 13.0 imagePullPolicy : Always serviceAccountName : argo - events - sa dependencies : - name : test - dep gatewayName : minio - gateway eventName : example subscription : http : port : 9300 triggers : - template : name : nats - trigger nats : # NATS Server URL url : nats . argo - events . svc : 4222 # Name of the subject subject : minio - events payload : - src : dependencyName : test - dep dataKey : notification . 0 . s3 . object . key dest : fileName - src : dependencyName : test - dep dataKey : notification . 0 . s3 . bucket . name dest : bucket The NATS message needs a body. In order to construct message based on the event data, sensor offers payload field as a part of the NATS trigger. The payload contains the list of src which refers to the source event and dest which refers to destination key within result request payload. The payload declared above will generate a message body like below, { \"fileName\" : \"hello.txt\" // name / key of the object \"bucket\" : \"input\" // name of the bucket } If you are running NATS on local K8s cluster, make sure to port-forward to pod, kubectl - n argo - events port - forward < nats - pod - name > 4222 : 4222 Subscribe to the subject called minio-events . Refer the nats example to publish a message to the subject https://github.com/nats-io/go-nats-examples/tree/master/patterns/publish-subscribe. go run main . go - s localhost minio - events ' Drop a file called hello.txt onto the bucket input and you will receive the message on NATS subscriber as follows, [ # 1 ] Received on [ minio - events ]: '{\"bucket\":\"input\",\"fileName\":\"hello.txt\"}'","title":"NATS Trigger"},{"location":"triggers/nats-trigger/#nats-trigger","text":"NATS trigger allows sensor to publish events on NATS subjects. This trigger helps source the events from outside world into your messaging queues.","title":"NATS Trigger"},{"location":"triggers/nats-trigger/#specification","text":"The NATS trigger specification is available here .","title":"Specification"},{"location":"triggers/nats-trigger/#walkthrough","text":"Consider a scenario where you are expecting a file drop onto a Minio bucket and want to place that event on a NATS subject. Set up the Minio Event Source and Gateway here . Do not create the Minio sensor, we are going to create it in next step. Lets create the sensor, apiVersion : argoproj . io / v1alpha1 kind : Sensor metadata : name : minio - sensor labels : sensors . argoproj . io / sensor - controller - instanceid : argo - events spec : template : spec : containers : - name : sensor image : argoproj / sensor : v0 . 13.0 imagePullPolicy : Always serviceAccountName : argo - events - sa dependencies : - name : test - dep gatewayName : minio - gateway eventName : example subscription : http : port : 9300 triggers : - template : name : nats - trigger nats : # NATS Server URL url : nats . argo - events . svc : 4222 # Name of the subject subject : minio - events payload : - src : dependencyName : test - dep dataKey : notification . 0 . s3 . object . key dest : fileName - src : dependencyName : test - dep dataKey : notification . 0 . s3 . bucket . name dest : bucket The NATS message needs a body. In order to construct message based on the event data, sensor offers payload field as a part of the NATS trigger. The payload contains the list of src which refers to the source event and dest which refers to destination key within result request payload. The payload declared above will generate a message body like below, { \"fileName\" : \"hello.txt\" // name / key of the object \"bucket\" : \"input\" // name of the bucket } If you are running NATS on local K8s cluster, make sure to port-forward to pod, kubectl - n argo - events port - forward < nats - pod - name > 4222 : 4222 Subscribe to the subject called minio-events . Refer the nats example to publish a message to the subject https://github.com/nats-io/go-nats-examples/tree/master/patterns/publish-subscribe. go run main . go - s localhost minio - events ' Drop a file called hello.txt onto the bucket input and you will receive the message on NATS subscriber as follows, [ # 1 ] Received on [ minio - events ]: '{\"bucket\":\"input\",\"fileName\":\"hello.txt\"}'","title":"Walkthrough"},{"location":"triggers/openwhisk-trigger/","text":"OpenWhisk Trigger \u00b6 OpenWhisk is a framework to run serverless workloads. It ships with its own event sources but their numbers are limited and it doesn't have support for circuits, parameterization, filtering, on-demand payload construction, etc that a sensor provides. Prerequisite \u00b6 OpenWhisk must be up and running. Setup \u00b6 Coming Soon...","title":"OpenWhisk Trigger"},{"location":"triggers/openwhisk-trigger/#openwhisk-trigger","text":"OpenWhisk is a framework to run serverless workloads. It ships with its own event sources but their numbers are limited and it doesn't have support for circuits, parameterization, filtering, on-demand payload construction, etc that a sensor provides.","title":"OpenWhisk Trigger"},{"location":"triggers/openwhisk-trigger/#prerequisite","text":"OpenWhisk must be up and running.","title":"Prerequisite"},{"location":"triggers/openwhisk-trigger/#setup","text":"Coming Soon...","title":"Setup"},{"location":"triggers/slack-trigger/","text":"Slack Trigger \u00b6 The Slack trigger is used to send a custom message to a desired Slack channel in a Slack workspace. The intended use is for notifications for a build pipeline, but can be used for any notification scenario. Prerequisite \u00b6 Have a Slack workspace setup you wish to send a message to. Set up the webhook gateway and event source. The K8s manifests are available under examples . Set up a simple webhook gateway. kubectl - n argo - events apply - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / gateways / webhook . yaml Create a webhook event source. kubectl - n argo - events apply - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / event - sources / webhook . yaml Set up port-forwarding to expose the http server. We will use port-forwarding here. kubectl port - forward - n argo - events webhook - gateway - YOUR - GATEWAY - POD 12000 : 12000 Create a Slack App \u00b6 We need to create a Slack App which will send messages to your Slack Workspace. We will add OAuth Permissions and add the OAuth token to the k8s cluster via a secret. Create a Slack app by clicking Create New App at the Slack API Page . Name your app and choose your intended Slack Workspace Navigate to your app, then to Features > OAuth & Permissions Scroll down to Scopes and add the scopes channels:join , and chat:write Scroll to the top of the OAuth & Permissions page and click Install App to Workspace and follow the install Wizard You should land back on the OAuth & Permissions page. Copy your app's OAuth Access Token. This will allow the trigger to act on behalf of your newly created Slack app. Encode your OAuth token in base64. This can done easily with the command line echo - n \"YOUR-OAUTH-TOKEN\" | base64 Create a kubernetes secret file slack-secret.yaml with your OAuth token in the following format apiVersion : v1 kind : Secret metadata : name : slack - secret data : token : YOUR - BASE64 - ENCODED - OAUTH - TOKEN Apply the kubernetes secret kubectl - n argo - events apply - f slack - secret . yaml Slack Trigger \u00b6 We will set up a basic slack trigger and send a default message, and then a dynamic custom message. Create a sensor with Slack trigger. We will discuss the trigger details in the following sections. kubectl - n argo - events apply - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / sensors / slack - trigger . yaml Send a http request to the newly setup gateway to fire the Slack trigger. curl - d '{\"text\":\"Hello, World!\"}' - H \"Content-Type: application/json\" - X POST http : // localhost : 12000 / example Note : The default slack-trigger will send the message \"hello world\" to the #general channel. You may change the default message and channel in slack-trigger.yaml under triggers.slack.channel and triggers.slack.message. 3. Alternatively, you can dynamically determine the channel and message based on parameterization of your event. curl - d ' {\"channel\":\"random\",\"message\":\"test message\"} ' - H \" Content-Type: application/json \" - X POST http : // localhost : 12000 / example Great! But how did the sensor use the event to customize the message and channel from the http request? We will see that in next section. Parameterization \u00b6 The slack trigger parameters have the following structure, parameters : - src : dependencyName : test - dep dataKey : body . channel dest : slack . channel - src : dependencyName : test - dep contextKey : body . message dest : slack . message The src is the source of event. It contains, dependencyName : name of the event dependency to extract the event from. dataKey : to extract a particular key-value from event's data. contextKey : to extract a particular key-value from event' context. The dest is the destination key within the result payload. So, the above trigger paramters will generate a request payload as, { \"channel\" : \"channel_to_send_message\" , \"message\" : \"message_to_send_to_channel\" } Note : If you define both the contextKey and dataKey within a paramter item, then the dataKey takes the precedence. You can create any paramater structure you want. To get more info on how to generate complex event payloads, take a look at this library . The complete specification of Slack trigger is available here .","title":"Slack Trigger"},{"location":"triggers/slack-trigger/#slack-trigger","text":"The Slack trigger is used to send a custom message to a desired Slack channel in a Slack workspace. The intended use is for notifications for a build pipeline, but can be used for any notification scenario.","title":"Slack Trigger"},{"location":"triggers/slack-trigger/#prerequisite","text":"Have a Slack workspace setup you wish to send a message to. Set up the webhook gateway and event source. The K8s manifests are available under examples . Set up a simple webhook gateway. kubectl - n argo - events apply - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / gateways / webhook . yaml Create a webhook event source. kubectl - n argo - events apply - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / event - sources / webhook . yaml Set up port-forwarding to expose the http server. We will use port-forwarding here. kubectl port - forward - n argo - events webhook - gateway - YOUR - GATEWAY - POD 12000 : 12000","title":"Prerequisite"},{"location":"triggers/slack-trigger/#create-a-slack-app","text":"We need to create a Slack App which will send messages to your Slack Workspace. We will add OAuth Permissions and add the OAuth token to the k8s cluster via a secret. Create a Slack app by clicking Create New App at the Slack API Page . Name your app and choose your intended Slack Workspace Navigate to your app, then to Features > OAuth & Permissions Scroll down to Scopes and add the scopes channels:join , and chat:write Scroll to the top of the OAuth & Permissions page and click Install App to Workspace and follow the install Wizard You should land back on the OAuth & Permissions page. Copy your app's OAuth Access Token. This will allow the trigger to act on behalf of your newly created Slack app. Encode your OAuth token in base64. This can done easily with the command line echo - n \"YOUR-OAUTH-TOKEN\" | base64 Create a kubernetes secret file slack-secret.yaml with your OAuth token in the following format apiVersion : v1 kind : Secret metadata : name : slack - secret data : token : YOUR - BASE64 - ENCODED - OAUTH - TOKEN Apply the kubernetes secret kubectl - n argo - events apply - f slack - secret . yaml","title":"Create a Slack App"},{"location":"triggers/slack-trigger/#slack-trigger_1","text":"We will set up a basic slack trigger and send a default message, and then a dynamic custom message. Create a sensor with Slack trigger. We will discuss the trigger details in the following sections. kubectl - n argo - events apply - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / sensors / slack - trigger . yaml Send a http request to the newly setup gateway to fire the Slack trigger. curl - d '{\"text\":\"Hello, World!\"}' - H \"Content-Type: application/json\" - X POST http : // localhost : 12000 / example Note : The default slack-trigger will send the message \"hello world\" to the #general channel. You may change the default message and channel in slack-trigger.yaml under triggers.slack.channel and triggers.slack.message. 3. Alternatively, you can dynamically determine the channel and message based on parameterization of your event. curl - d ' {\"channel\":\"random\",\"message\":\"test message\"} ' - H \" Content-Type: application/json \" - X POST http : // localhost : 12000 / example Great! But how did the sensor use the event to customize the message and channel from the http request? We will see that in next section.","title":"Slack Trigger"},{"location":"triggers/slack-trigger/#parameterization","text":"The slack trigger parameters have the following structure, parameters : - src : dependencyName : test - dep dataKey : body . channel dest : slack . channel - src : dependencyName : test - dep contextKey : body . message dest : slack . message The src is the source of event. It contains, dependencyName : name of the event dependency to extract the event from. dataKey : to extract a particular key-value from event's data. contextKey : to extract a particular key-value from event' context. The dest is the destination key within the result payload. So, the above trigger paramters will generate a request payload as, { \"channel\" : \"channel_to_send_message\" , \"message\" : \"message_to_send_to_channel\" } Note : If you define both the contextKey and dataKey within a paramter item, then the dataKey takes the precedence. You can create any paramater structure you want. To get more info on how to generate complex event payloads, take a look at this library . The complete specification of Slack trigger is available here .","title":"Parameterization"},{"location":"tutorials/01-introduction/","text":"Introduction \u00b6 In the tutorials, we will cover every aspect of Argo Events and demonstrate how you can leverage these features to build an event driven workflow pipeline. All the concepts you will learn in this tutorial and subsequent ones can be applied to any type of gateway. Prerequisites \u00b6 Follow the installation guide to set up the Argo Events. Make sure to configure Argo Workflow controller to listen to workflow objects created in argo-events namespace. Make sure to read the concepts behind gateway , sensor , event source . Get Started \u00b6 Lets set up a basic webhook gateway and sensor that listens to events over HTTP and executes an Argo workflow. Create the webhook event source. kubectl - n argo - events apply - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / event - sources / webhook . yaml Create the webhook gateway. kubectl - n argo - events apply - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / gateways / webhook . yaml Create the webhook sensor. kubectl - n argo - events apply - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / sensors / webhook . yaml If the commands are executed successfully, the gateway and sensor pods will get created. You will also notice that a service is created for both the gateway and sensor. Expose the gateway pod via Ingress, OpenShift Route or port forward to consume requests over HTTP. kubectl - n argo - events port - forward < gateway - pod - name > 12000 : 12000 Use either Curl or Postman to send a post request to the http://localhost:12000/example curl - d '{\"message\":\"this is my first webhook\"}' - H \"Content-Type: application/json\" - X POST http : // localhost : 12000 / example Now, you should see an Argo workflow being created. kubectl - n argo - events get wf Make sure the workflow pod ran successfully. _________________________________________ / { \"context\" : { \"type\" : \"webhook\" , \"specVersi \\ | on\" : \"0.3\" , \"source\" : \"webhook-gateway\" , \"e | | ventID\" : \"38376665363064642d343336352d34 | | 3035372d393766662d366234326130656232343 | | 337\" , \"time\" : \"2020-01-11T16:55:42.996636 | | Z\" , \"dataContentType\" : \"application/json\" | | , \"subject\" : \"example\" } , \"data\" : \"eyJoZWFkZ | | XIiOnsiQWNjZXB0IjpbIiovKiJdLCJDb250ZW50 | | LUxlbmd0aCI6WyIzOCJdLCJDb250ZW50LVR5cGU | | iOlsiYXBwbGljYXRpb24vanNvbiJdLCJVc2VyLU | | FnZW50IjpbImN1cmwvNy41NC4wIl19LCJib2R5I | | jp7Im1lc3NhZ2UiOiJ0aGlzIGlzIG15IGZpcnN0 | \\ IHdlYmhvb2sifX0=\" } / ----------------------------------------- \\ \\ \\ ## . ## ## ## == ## ## ## ## === / \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" ___ / === ~~~ { ~~ ~~~~ ~~~ ~~~~ ~~ ~ / ===- ~~~ \\ ______ o __ / \\ \\ __ / \\ ____ \\ ______ / Note: You will see the message printed in the workflow logs contains both the event context and data, with data being base64 encoded. In later sections, we will see how to extract particular key-value from event context or data and pass it to the workflow as arguments. Troubleshoot \u00b6 If you don't see the gateway and sensor pod in argo-events namespace, Make sure the correct Role and RoleBindings are applied to the service account and there are no errors in both gateway and sensor controller. Make sure gateway and sensor controller configmap has namespace set to argo-events . Check the logs of gateway and sensor controller. Make sure the controllers have processed the gateway and sensor objects and there are no errors. Look for any error in gateway or sensor pod. Inspect the gateway, kubectl - n argo - event gateway - object - name - o yaml Inspect the sensor, kubectl - n argo - events sensor - object - name - o yaml and look for any errors within the Status . Raise an issue on GitHub or post a question on argo-events slack channel.","title":"Introduction"},{"location":"tutorials/01-introduction/#introduction","text":"In the tutorials, we will cover every aspect of Argo Events and demonstrate how you can leverage these features to build an event driven workflow pipeline. All the concepts you will learn in this tutorial and subsequent ones can be applied to any type of gateway.","title":"Introduction"},{"location":"tutorials/01-introduction/#prerequisites","text":"Follow the installation guide to set up the Argo Events. Make sure to configure Argo Workflow controller to listen to workflow objects created in argo-events namespace. Make sure to read the concepts behind gateway , sensor , event source .","title":"Prerequisites"},{"location":"tutorials/01-introduction/#get-started","text":"Lets set up a basic webhook gateway and sensor that listens to events over HTTP and executes an Argo workflow. Create the webhook event source. kubectl - n argo - events apply - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / event - sources / webhook . yaml Create the webhook gateway. kubectl - n argo - events apply - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / gateways / webhook . yaml Create the webhook sensor. kubectl - n argo - events apply - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / sensors / webhook . yaml If the commands are executed successfully, the gateway and sensor pods will get created. You will also notice that a service is created for both the gateway and sensor. Expose the gateway pod via Ingress, OpenShift Route or port forward to consume requests over HTTP. kubectl - n argo - events port - forward < gateway - pod - name > 12000 : 12000 Use either Curl or Postman to send a post request to the http://localhost:12000/example curl - d '{\"message\":\"this is my first webhook\"}' - H \"Content-Type: application/json\" - X POST http : // localhost : 12000 / example Now, you should see an Argo workflow being created. kubectl - n argo - events get wf Make sure the workflow pod ran successfully. _________________________________________ / { \"context\" : { \"type\" : \"webhook\" , \"specVersi \\ | on\" : \"0.3\" , \"source\" : \"webhook-gateway\" , \"e | | ventID\" : \"38376665363064642d343336352d34 | | 3035372d393766662d366234326130656232343 | | 337\" , \"time\" : \"2020-01-11T16:55:42.996636 | | Z\" , \"dataContentType\" : \"application/json\" | | , \"subject\" : \"example\" } , \"data\" : \"eyJoZWFkZ | | XIiOnsiQWNjZXB0IjpbIiovKiJdLCJDb250ZW50 | | LUxlbmd0aCI6WyIzOCJdLCJDb250ZW50LVR5cGU | | iOlsiYXBwbGljYXRpb24vanNvbiJdLCJVc2VyLU | | FnZW50IjpbImN1cmwvNy41NC4wIl19LCJib2R5I | | jp7Im1lc3NhZ2UiOiJ0aGlzIGlzIG15IGZpcnN0 | \\ IHdlYmhvb2sifX0=\" } / ----------------------------------------- \\ \\ \\ ## . ## ## ## == ## ## ## ## === / \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" ___ / === ~~~ { ~~ ~~~~ ~~~ ~~~~ ~~ ~ / ===- ~~~ \\ ______ o __ / \\ \\ __ / \\ ____ \\ ______ / Note: You will see the message printed in the workflow logs contains both the event context and data, with data being base64 encoded. In later sections, we will see how to extract particular key-value from event context or data and pass it to the workflow as arguments.","title":"Get Started"},{"location":"tutorials/01-introduction/#troubleshoot","text":"If you don't see the gateway and sensor pod in argo-events namespace, Make sure the correct Role and RoleBindings are applied to the service account and there are no errors in both gateway and sensor controller. Make sure gateway and sensor controller configmap has namespace set to argo-events . Check the logs of gateway and sensor controller. Make sure the controllers have processed the gateway and sensor objects and there are no errors. Look for any error in gateway or sensor pod. Inspect the gateway, kubectl - n argo - event gateway - object - name - o yaml Inspect the sensor, kubectl - n argo - events sensor - object - name - o yaml and look for any errors within the Status . Raise an issue on GitHub or post a question on argo-events slack channel.","title":"Troubleshoot"},{"location":"tutorials/02-parameterization/","text":"Parameterization \u00b6 In previous section, we saw how to set up a basic webhook gateway and sensor, and trigger an Argo workflow. The trigger template had parameters set in the sensor obejct and the workflow was able to print the event payload. In this tutorial, we will dig deeper into different types of parameterization, how to extract particular key-value from event payload and how to use default values if certain key is not available within event payload. Trigger Resource Parameterization \u00b6 If you take a closer look at the Sensor object, you will notice it contains a list of triggers. Each Trigger contains the template that defines the context of the trigger and actual resource that we expect the sensor to execute. In previous section, the resource within the trigger template was an Argo workflow. This subsection deals with how to parameterize the resource within trigger template with the event payload. Prerequisites \u00b6 Make sure to have the basic webhook gateway and sensor set up. Follow the introduction tutorial if haven't done already. Webhook Event Payload \u00b6 Webhook gateway consumes events through HTTP requests and transforms them into CloudEvents. The structure of the event the Webhook sensor receives from the gateway looks like following, { \"context\" : { \"type\" : \"type_of_gateway\" , \"specVersion\" : \"cloud_events_version\" , \"source\" : \"name_of_the_gateway\" , \"eventID\" : \"unique_event_id\" , \"time\" : \"event_time\" , \"dataContentType\" : \"type_of_data\" , \"subject\" : \"name_of_the_event_within_event_source\" } , \"data\" : { \"header\" : {} , \"body\" : {} , } } Context : This is the CloudEvent context and it is populated by the gateway regardless of type of HTTP request. Data : Data contains following fields, Header : The header within event data contains the headers in the HTTP request that was dispatched to the gateway. The gateway extracts the headers from the request and put it in the the header within event data . Body : This is the request payload from the HTTP request. Event Context \u00b6 Now that we have an understanding of the structure of the event the webhook sensor receives from the gateway, lets see how we can use the event context to parameterize the Argo workflow. Update the Webhook Sensor and add the contextKey for the parameter at index 0. kubectl - n argo - events apply - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / tutorials / 02 - parameterization / sensor - 01 . yaml Send a HTTP request to the gateway. curl - d '{\"message\":\"this is my first webhook\"}' - H \"Content-Type: application/json\" - X POST http : // localhost : 12000 / example Inspect the output of the Argo workflow that was created. argo logs name_of_the_workflow You will see the following output, _________ < webhook > --------- \\ \\ \\ ## . ## ## ## == ## ## ## ## === / \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" ___ / === ~~~ { ~~ ~~~~ ~~~ ~~~~ ~~ ~ / ===- ~~~ \\ ______ o __ / \\ \\ __ / \\ ____ \\ ______ / We have successfully extracted the type key within the event context and parameterized the workflow to print the value of the type . Event Data \u00b6 Now, it is time to use the event data and parameterize the Argo workflow trigger. We will extract the message from request payload and get the Argo workflow to print the message. Update the Webhook Sensor and add the dataKey in the parameter at index 0. kubectl - n argo - events apply - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / tutorials / 02 - parameterization / sensor - 02 . yaml Send a HTTP request to the gateway. curl - d '{\"message\":\"this is my first webhook\"}' - H \"Content-Type: application/json\" - X POST http : // localhost : 12000 / example Inspect the output of the Argo workflow that was created. argo logs name_of_the_workflow You will see the following output, __________________________ < this is my first webhook > -------------------------- \\ \\ \\ ## . ## ## ## == ## ## ## ## === / \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" ___ / === ~~~ { ~~ ~~~~ ~~~ ~~~~ ~~ ~ / ===- ~~~ \\ ______ o __ / \\ \\ __ / \\ ____ \\ ______ / Yay!! The Argo workflow printed the message. You can add however many number of parameters to update the trigger resource on the fly. Note : If you define both the contextKey and dataKey within a parameter, then the dataKey takes the precedence. Default Values \u00b6 Each parameter comes with an option to configure the default value. This is specially important when the key you defined in the parameter doesn't exist in the event. Update the Webhook Sensor and add the value for the parameter at index 0. We will also update the dataKey to an unknown event key. kubectl - n argo - events apply - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / tutorials / 02 - parameterization / sensor - 03 . yaml Send a HTTP request to the gateway. curl - d '{\"message\":\"this is my first webhook\"}' - H \"Content-Type: application/json\" - X POST http : // localhost : 12000 / example Inspect the output of the Argo workflow that was created. argo logs name_of_the_workflow You will see the following output, _______________________ < wow ! a default value . > ----------------------- \\ \\ \\ ## . ## ## ## == ## ## ## ## === / \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" ___ / === ~~~ { ~~ ~~~~ ~~~ ~~~~ ~~ ~ / ===- ~~~ \\ ______ o __ / \\ \\ __ / \\ ____ \\ ______ / Sprig Templates \u00b6 The sprig template exposed through contextTemplate and dataTemplate lets you alter the event context and event data before it gets applied to the trigger via parameters . Take a look at the example defined here , it contains the parameters as follows, parameters: # Retrieve the 'message' key from the payload - src: dependencyName: test-dep dataTemplate: \" {{ .Input.body.message | title }} \" dest: spec.arguments.parameters.0.value # Title case the context subject - src: dependencyName: test-dep contextTemplate: \" {{ .Input.subject | title }} \" dest: spec.arguments.parameters.1.value # Retrieve the 'name' key from the payload, remove all whitespace and lowercase it. - src: dependencyName: test-dep dataTemplate: \" {{ .Input.body.name | nospace | lower }} -\" dest: metadata.generateName operation: append Consider the event the sensor received has format like, { \"context\" : { \"type\" : \"type_of_gateway\" , \"specVersion\" : \"cloud_events_version\" , \"source\" : \"name_of_the_gateway\" , \"eventID\" : \"unique_event_id\" , \"time\" : \"event_time\" , \"dataContentType\" : \"type_of_data\" , \"subject\" : \"name_of_the_event_within_event_source\" } , \"data\" : { \"body\" : { \"name\" : \"foo bar\" , \"message\" : \"hello there!!\" } , } } The parameters are transformed as, The first parameter extracts the body.message from event data and applies title filter which basically capitalizes the first letter and replaces the spec.arguments.parameters.0.value . The second parameter extracts the subject from the event context and again applies title filter and replaces the spec.arguments.parameters.1.value . The third parameter extracts the body.name from the event data, applies nospace filter which removes all white spaces and then lower filter which lowercases the text and appends it to metadata.generateName . Send a curl request to webhook-gateway as follows, curl - d '{\"name\":\"foo bar\", \"message\": \"hello there!!\"}' - H \"Content-Type: application/json\" - X POST http : // localhost : 12000 / example and you will an Argo workflow being sprung with name like webhook-foobar-xxxxx . Check the output of workflow, it should print something like, ____________________________ < Hello There !! from Example > ---------------------------- \\ \\ \\ ## . ## ## ## == ## ## ## ## === / \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" ___ / === ~~~ { ~~ ~~~~ ~~~ ~~~~ ~~ ~ / ===- ~~~ \\ ______ o __ / \\ \\ __ / \\ ____ \\ ______ / Operations \u00b6 Sometimes you need the ability to append or prepend a parameter value to an existing value in trigger resource. This is where the operation field within a parameter comes handy. Update the Webhook Sensor and add the operation in the parameter at index 0. We will prepend the message to an existing value. kubectl - n argo - events apply - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / tutorials / 02 - parameterization / sensor - 04 . yaml Send a HTTP request to the gateway. curl - d '{\"message\":\"hey!!\"}' - H \"Content-Type: application/json\" - X POST http : // localhost : 12000 / example Inspect the output of the Argo workflow that was created. argo logs name_of_the_workflow You will see the following output, __________________ < hey !! hello world > ------------------ \\ \\ \\ ## . ## ## ## == ## ## ## ## === / \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" ___ / === ~~~ { ~~ ~~~~ ~~~ ~~~~ ~~ ~ / ===- ~~~ \\ ______ o __ / \\ \\ __ / \\ ____ \\ ______ / Trigger Template Parameterization \u00b6 The parameterization you saw above deals with the trigger resource, but sometimes you need to parameterize the trigger template itself. This comes handy when you have the trigger resource stored on some external source like S3, Git, etc. and you need to replace the url of the source on the fly in trigger template. Imagine a scenario where you want to parameterize the parameters of trigger to parameterize the trigger resource. What?... The sensor you have been using in this tutorial has one parameter defined in the trigger resource under k8s . We will parameterize that parameter by applying a parameter at the trigger template level. Update the Webhook Sensor and add parameters at trigger level. kubectl - n argo - events apply - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / tutorials / 02 - parameterization / sensor - 05 . yaml Send a HTTP request to the gateway. curl - d '{\"dependencyName\":\"test-dep\", \"dataKey\": \"body.message\", \"dest\": \"spec.arguments.parameters.0.value\", \"message\": \"amazing!!\"}' - H \"Content-Type: application/json\" - X POST http : // localhost : 12000 / example Inspect the output of the Argo workflow that was created. argo logs name_of_the_workflow You will see the following output, ___________ < amazing !! > ----------- \\ \\ \\ ## . ## ## ## == ## ## ## ## === / \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" ___ / === ~~~ { ~~ ~~~~ ~~~ ~~~~ ~~ ~ / ===- ~~~ \\ ______ o __ / \\ \\ __ / \\ ____ \\ ______ / Great!! You have now learned how to apply parameters at trigger resource and template level. Keep in mind that you can apply default values and operations like prepend and append for trigger template parameters as well.","title":"Parameterization"},{"location":"tutorials/02-parameterization/#parameterization","text":"In previous section, we saw how to set up a basic webhook gateway and sensor, and trigger an Argo workflow. The trigger template had parameters set in the sensor obejct and the workflow was able to print the event payload. In this tutorial, we will dig deeper into different types of parameterization, how to extract particular key-value from event payload and how to use default values if certain key is not available within event payload.","title":"Parameterization"},{"location":"tutorials/02-parameterization/#trigger-resource-parameterization","text":"If you take a closer look at the Sensor object, you will notice it contains a list of triggers. Each Trigger contains the template that defines the context of the trigger and actual resource that we expect the sensor to execute. In previous section, the resource within the trigger template was an Argo workflow. This subsection deals with how to parameterize the resource within trigger template with the event payload.","title":"Trigger Resource Parameterization"},{"location":"tutorials/02-parameterization/#prerequisites","text":"Make sure to have the basic webhook gateway and sensor set up. Follow the introduction tutorial if haven't done already.","title":"Prerequisites"},{"location":"tutorials/02-parameterization/#webhook-event-payload","text":"Webhook gateway consumes events through HTTP requests and transforms them into CloudEvents. The structure of the event the Webhook sensor receives from the gateway looks like following, { \"context\" : { \"type\" : \"type_of_gateway\" , \"specVersion\" : \"cloud_events_version\" , \"source\" : \"name_of_the_gateway\" , \"eventID\" : \"unique_event_id\" , \"time\" : \"event_time\" , \"dataContentType\" : \"type_of_data\" , \"subject\" : \"name_of_the_event_within_event_source\" } , \"data\" : { \"header\" : {} , \"body\" : {} , } } Context : This is the CloudEvent context and it is populated by the gateway regardless of type of HTTP request. Data : Data contains following fields, Header : The header within event data contains the headers in the HTTP request that was dispatched to the gateway. The gateway extracts the headers from the request and put it in the the header within event data . Body : This is the request payload from the HTTP request.","title":"Webhook Event Payload"},{"location":"tutorials/02-parameterization/#event-context","text":"Now that we have an understanding of the structure of the event the webhook sensor receives from the gateway, lets see how we can use the event context to parameterize the Argo workflow. Update the Webhook Sensor and add the contextKey for the parameter at index 0. kubectl - n argo - events apply - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / tutorials / 02 - parameterization / sensor - 01 . yaml Send a HTTP request to the gateway. curl - d '{\"message\":\"this is my first webhook\"}' - H \"Content-Type: application/json\" - X POST http : // localhost : 12000 / example Inspect the output of the Argo workflow that was created. argo logs name_of_the_workflow You will see the following output, _________ < webhook > --------- \\ \\ \\ ## . ## ## ## == ## ## ## ## === / \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" ___ / === ~~~ { ~~ ~~~~ ~~~ ~~~~ ~~ ~ / ===- ~~~ \\ ______ o __ / \\ \\ __ / \\ ____ \\ ______ / We have successfully extracted the type key within the event context and parameterized the workflow to print the value of the type .","title":"Event Context"},{"location":"tutorials/02-parameterization/#event-data","text":"Now, it is time to use the event data and parameterize the Argo workflow trigger. We will extract the message from request payload and get the Argo workflow to print the message. Update the Webhook Sensor and add the dataKey in the parameter at index 0. kubectl - n argo - events apply - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / tutorials / 02 - parameterization / sensor - 02 . yaml Send a HTTP request to the gateway. curl - d '{\"message\":\"this is my first webhook\"}' - H \"Content-Type: application/json\" - X POST http : // localhost : 12000 / example Inspect the output of the Argo workflow that was created. argo logs name_of_the_workflow You will see the following output, __________________________ < this is my first webhook > -------------------------- \\ \\ \\ ## . ## ## ## == ## ## ## ## === / \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" ___ / === ~~~ { ~~ ~~~~ ~~~ ~~~~ ~~ ~ / ===- ~~~ \\ ______ o __ / \\ \\ __ / \\ ____ \\ ______ / Yay!! The Argo workflow printed the message. You can add however many number of parameters to update the trigger resource on the fly. Note : If you define both the contextKey and dataKey within a parameter, then the dataKey takes the precedence.","title":"Event Data"},{"location":"tutorials/02-parameterization/#default-values","text":"Each parameter comes with an option to configure the default value. This is specially important when the key you defined in the parameter doesn't exist in the event. Update the Webhook Sensor and add the value for the parameter at index 0. We will also update the dataKey to an unknown event key. kubectl - n argo - events apply - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / tutorials / 02 - parameterization / sensor - 03 . yaml Send a HTTP request to the gateway. curl - d '{\"message\":\"this is my first webhook\"}' - H \"Content-Type: application/json\" - X POST http : // localhost : 12000 / example Inspect the output of the Argo workflow that was created. argo logs name_of_the_workflow You will see the following output, _______________________ < wow ! a default value . > ----------------------- \\ \\ \\ ## . ## ## ## == ## ## ## ## === / \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" ___ / === ~~~ { ~~ ~~~~ ~~~ ~~~~ ~~ ~ / ===- ~~~ \\ ______ o __ / \\ \\ __ / \\ ____ \\ ______ /","title":"Default Values"},{"location":"tutorials/02-parameterization/#sprig-templates","text":"The sprig template exposed through contextTemplate and dataTemplate lets you alter the event context and event data before it gets applied to the trigger via parameters . Take a look at the example defined here , it contains the parameters as follows, parameters: # Retrieve the 'message' key from the payload - src: dependencyName: test-dep dataTemplate: \" {{ .Input.body.message | title }} \" dest: spec.arguments.parameters.0.value # Title case the context subject - src: dependencyName: test-dep contextTemplate: \" {{ .Input.subject | title }} \" dest: spec.arguments.parameters.1.value # Retrieve the 'name' key from the payload, remove all whitespace and lowercase it. - src: dependencyName: test-dep dataTemplate: \" {{ .Input.body.name | nospace | lower }} -\" dest: metadata.generateName operation: append Consider the event the sensor received has format like, { \"context\" : { \"type\" : \"type_of_gateway\" , \"specVersion\" : \"cloud_events_version\" , \"source\" : \"name_of_the_gateway\" , \"eventID\" : \"unique_event_id\" , \"time\" : \"event_time\" , \"dataContentType\" : \"type_of_data\" , \"subject\" : \"name_of_the_event_within_event_source\" } , \"data\" : { \"body\" : { \"name\" : \"foo bar\" , \"message\" : \"hello there!!\" } , } } The parameters are transformed as, The first parameter extracts the body.message from event data and applies title filter which basically capitalizes the first letter and replaces the spec.arguments.parameters.0.value . The second parameter extracts the subject from the event context and again applies title filter and replaces the spec.arguments.parameters.1.value . The third parameter extracts the body.name from the event data, applies nospace filter which removes all white spaces and then lower filter which lowercases the text and appends it to metadata.generateName . Send a curl request to webhook-gateway as follows, curl - d '{\"name\":\"foo bar\", \"message\": \"hello there!!\"}' - H \"Content-Type: application/json\" - X POST http : // localhost : 12000 / example and you will an Argo workflow being sprung with name like webhook-foobar-xxxxx . Check the output of workflow, it should print something like, ____________________________ < Hello There !! from Example > ---------------------------- \\ \\ \\ ## . ## ## ## == ## ## ## ## === / \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" ___ / === ~~~ { ~~ ~~~~ ~~~ ~~~~ ~~ ~ / ===- ~~~ \\ ______ o __ / \\ \\ __ / \\ ____ \\ ______ /","title":"Sprig Templates"},{"location":"tutorials/02-parameterization/#operations","text":"Sometimes you need the ability to append or prepend a parameter value to an existing value in trigger resource. This is where the operation field within a parameter comes handy. Update the Webhook Sensor and add the operation in the parameter at index 0. We will prepend the message to an existing value. kubectl - n argo - events apply - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / tutorials / 02 - parameterization / sensor - 04 . yaml Send a HTTP request to the gateway. curl - d '{\"message\":\"hey!!\"}' - H \"Content-Type: application/json\" - X POST http : // localhost : 12000 / example Inspect the output of the Argo workflow that was created. argo logs name_of_the_workflow You will see the following output, __________________ < hey !! hello world > ------------------ \\ \\ \\ ## . ## ## ## == ## ## ## ## === / \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" ___ / === ~~~ { ~~ ~~~~ ~~~ ~~~~ ~~ ~ / ===- ~~~ \\ ______ o __ / \\ \\ __ / \\ ____ \\ ______ /","title":"Operations"},{"location":"tutorials/02-parameterization/#trigger-template-parameterization","text":"The parameterization you saw above deals with the trigger resource, but sometimes you need to parameterize the trigger template itself. This comes handy when you have the trigger resource stored on some external source like S3, Git, etc. and you need to replace the url of the source on the fly in trigger template. Imagine a scenario where you want to parameterize the parameters of trigger to parameterize the trigger resource. What?... The sensor you have been using in this tutorial has one parameter defined in the trigger resource under k8s . We will parameterize that parameter by applying a parameter at the trigger template level. Update the Webhook Sensor and add parameters at trigger level. kubectl - n argo - events apply - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / tutorials / 02 - parameterization / sensor - 05 . yaml Send a HTTP request to the gateway. curl - d '{\"dependencyName\":\"test-dep\", \"dataKey\": \"body.message\", \"dest\": \"spec.arguments.parameters.0.value\", \"message\": \"amazing!!\"}' - H \"Content-Type: application/json\" - X POST http : // localhost : 12000 / example Inspect the output of the Argo workflow that was created. argo logs name_of_the_workflow You will see the following output, ___________ < amazing !! > ----------- \\ \\ \\ ## . ## ## ## == ## ## ## ## === / \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" ___ / === ~~~ { ~~ ~~~~ ~~~ ~~~~ ~~ ~ / ===- ~~~ \\ ______ o __ / \\ \\ __ / \\ ____ \\ ______ / Great!! You have now learned how to apply parameters at trigger resource and template level. Keep in mind that you can apply default values and operations like prepend and append for trigger template parameters as well.","title":"Trigger Template Parameterization"},{"location":"tutorials/03-trigger-sources/","text":"Trigger Sources \u00b6 A trigger source is the source of trigger resource. It can be either external source such as Git , S3 , K8s Configmap , File , any valid URL that hosts the resource or an internal resource which is defined in the sensor object itself like Inline or Resource . In the previous sections, you have been dealing with the Resource trigger source. In this tutorial, we will explore other trigger sources. Prerequisites \u00b6 The Webhook gateway is already set up. Git \u00b6 Git trigger source refers to K8s trigger refers to the K8s resource stored in Git. The specification for the Git source is available here . In order to fetch data from git, you need to set up the private SSH key in sensor. If you don't have ssh keys available, create them following this guide Create a K8s secret that holds the SSH keys kubectl - n argo - events create secret generic git - ssh --from-file=key=.ssh/<YOUR_SSH_KEY_FILE_NAME> Create a K8s secret that holds known hosts. kubectl - n argo - events create secret generic git - known - hosts --from-file=ssh_known_hosts=.ssh/known_hosts Create a sensor with the git trigger source and refer it to the hello world workflow stored on the Argo Git project kubectl - n argo - events apply - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / tutorials / 03 - trigger - sources / sensor - git . yaml Use either Curl or Postman to send a post request to the http://localhost:12000/example curl - d '{\"message\":\"ok\"}' - H \"Content-Type: application/json\" - X POST http : // localhost : 12000 / example Now, you should see an Argo workflow being created. kubectl - n argo - events get wf S3 \u00b6 You can refer to the K8s resource stored on S3 complaint store as the trigger source. For this tutorial, lets set up a minio server which is S3 compliant store. Create a K8s secret called artifacts-minio that holds your minio access key and secret key. The access key must be stored under accesskey key and secret key must be stored under secretkey . Follow steps described here to set up the minio server. Make sure a service is available to expose the minio server. Create a bucket called workflows and store a basic hello world Argo workflow with key name hello-world.yaml . Create the sensor with trigger source as S3. kubectl - n argo - events apply - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / tutorials / 03 - trigger - sources / sensor - minio . yaml Use either Curl or Postman to send a post request to the http://localhost:12000/example curl - d '{\"message\":\"ok\"}' - H \"Content-Type: application/json\" - X POST http : // localhost : 12000 / example Now, you should see an Argo workflow being created. kubectl - n argo - events get wf K8s Configmap \u00b6 K8s configmap can be treated as trigger source if needed. Lets create a configmap called trigger-store . kubectl - n argo - events apply - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / tutorials / 03 - trigger - sources / trigger - store . yaml Create a sensor with trigger source as configmap and refer it to the trigger-store . kubectl - n argo - events apply - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / tutorials / 03 - trigger - sources / sensor - cm . yaml Use either Curl or Postman to send a post request to the http://localhost:12000/example curl - d '{\"message\":\"ok\"}' - H \"Content-Type: application/json\" - X POST http : // localhost : 12000 / example Now, you should see an Argo workflow being created. kubectl - n argo - events get wf File & URL \u00b6 File and URL trigger sources are pretty self explanatory. The example sensors are available under examples/sensors folder.","title":"Trigger Sources"},{"location":"tutorials/03-trigger-sources/#trigger-sources","text":"A trigger source is the source of trigger resource. It can be either external source such as Git , S3 , K8s Configmap , File , any valid URL that hosts the resource or an internal resource which is defined in the sensor object itself like Inline or Resource . In the previous sections, you have been dealing with the Resource trigger source. In this tutorial, we will explore other trigger sources.","title":"Trigger Sources"},{"location":"tutorials/03-trigger-sources/#prerequisites","text":"The Webhook gateway is already set up.","title":"Prerequisites"},{"location":"tutorials/03-trigger-sources/#git","text":"Git trigger source refers to K8s trigger refers to the K8s resource stored in Git. The specification for the Git source is available here . In order to fetch data from git, you need to set up the private SSH key in sensor. If you don't have ssh keys available, create them following this guide Create a K8s secret that holds the SSH keys kubectl - n argo - events create secret generic git - ssh --from-file=key=.ssh/<YOUR_SSH_KEY_FILE_NAME> Create a K8s secret that holds known hosts. kubectl - n argo - events create secret generic git - known - hosts --from-file=ssh_known_hosts=.ssh/known_hosts Create a sensor with the git trigger source and refer it to the hello world workflow stored on the Argo Git project kubectl - n argo - events apply - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / tutorials / 03 - trigger - sources / sensor - git . yaml Use either Curl or Postman to send a post request to the http://localhost:12000/example curl - d '{\"message\":\"ok\"}' - H \"Content-Type: application/json\" - X POST http : // localhost : 12000 / example Now, you should see an Argo workflow being created. kubectl - n argo - events get wf","title":"Git"},{"location":"tutorials/03-trigger-sources/#s3","text":"You can refer to the K8s resource stored on S3 complaint store as the trigger source. For this tutorial, lets set up a minio server which is S3 compliant store. Create a K8s secret called artifacts-minio that holds your minio access key and secret key. The access key must be stored under accesskey key and secret key must be stored under secretkey . Follow steps described here to set up the minio server. Make sure a service is available to expose the minio server. Create a bucket called workflows and store a basic hello world Argo workflow with key name hello-world.yaml . Create the sensor with trigger source as S3. kubectl - n argo - events apply - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / tutorials / 03 - trigger - sources / sensor - minio . yaml Use either Curl or Postman to send a post request to the http://localhost:12000/example curl - d '{\"message\":\"ok\"}' - H \"Content-Type: application/json\" - X POST http : // localhost : 12000 / example Now, you should see an Argo workflow being created. kubectl - n argo - events get wf","title":"S3"},{"location":"tutorials/03-trigger-sources/#k8s-configmap","text":"K8s configmap can be treated as trigger source if needed. Lets create a configmap called trigger-store . kubectl - n argo - events apply - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / tutorials / 03 - trigger - sources / trigger - store . yaml Create a sensor with trigger source as configmap and refer it to the trigger-store . kubectl - n argo - events apply - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / tutorials / 03 - trigger - sources / sensor - cm . yaml Use either Curl or Postman to send a post request to the http://localhost:12000/example curl - d '{\"message\":\"ok\"}' - H \"Content-Type: application/json\" - X POST http : // localhost : 12000 / example Now, you should see an Argo workflow being created. kubectl - n argo - events get wf","title":"K8s Configmap"},{"location":"tutorials/03-trigger-sources/#file-url","text":"File and URL trigger sources are pretty self explanatory. The example sensors are available under examples/sensors folder.","title":"File &amp; URL"},{"location":"tutorials/04-standard-k8s-resources/","text":"Trigger Standard K8s Resources \u00b6 In the previous sections, you saw how to trigger the Argo workflows. In this tutorial, you will see how to trigger Pod and Deployment. Note: You can trigger any standard Kubernetes object. Having the ability to trigger standard Kubernetes resources is quite powerful as provides an avenue to set up pipelines for existing workloads. Prerequisites \u00b6 Make sure that argo-events-sa service account has necessary permissions to create the Kubernetes resource of your choice. The Webhook gateway is already set up. Pod \u00b6 Create a sensor with K8s trigger. Pay close attention to the group , version and kind keys within the trigger resource. These keys determine the type of kubernetes object. You will notice that the group key is empty, that means we want to use core group. For any other groups, you need to specify the group key. kubectl - n argo - events apply - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / tutorials / 04 - standard - k8s - resources / sensor - pod . yaml Use either Curl or Postman to send a post request to the http://localhost:12000/example curl - d '{\"message\":\"ok\"}' - H \"Content-Type: application/json\" - X POST http : // localhost : 12000 / example Now, you should see a pod being created. kubectl - n argo - events get po Output _________________________________________ / { \"context\" : { \"type\" : \"webhook\" , \"specVersi \\ | on\" : \"0.3\" , \"source\" : \"webhook-gateway\" , \"e | | ventID\" : \"30306463666539362d346666642d34 | | 3336332d383861312d336538363333613564313 | | 932\" , \"time\" : \"2020-01-11T21:23:07.682961 | | Z\" , \"dataContentType\" : \"application/json\" | | , \"subject\" : \"example\" } , \"data\" : \"eyJoZWFkZ | | XIiOnsiQWNjZXB0IjpbIiovKiJdLCJDb250ZW50 | | LUxlbmd0aCI6WyIxOSJdLCJDb250ZW50LVR5cGU | | iOlsiYXBwbGljYXRpb24vanNvbiJdLCJVc2VyLU | | FnZW50IjpbImN1cmwvNy41NC4wIl19LCJib2R5I | \\ jp7Im1lc3NhZ2UiOiJoZXkhISJ9fQ==\" } / ----------------------------------------- \\ \\ \\ ## . ## ## ## == ## ## ## ## === / \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" ___ / === ~~~ { ~~ ~~~~ ~~~ ~~~~ ~~ ~ / ===- ~~~ \\ ______ o __ / \\ \\ __ / \\ ____ \\ ______ / Deployment \u00b6 Lets create a sensor with a K8s deployment as trigger. kubectl - n argo - events apply - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / tutorials / 04 - standard - k8s - resources / sensor - deployment . yaml Use either Curl or Postman to send a post request to the http://localhost:12000/example curl - d '{\"message\":\"ok\"}' - H \"Content-Type: application/json\" - X POST http : // localhost : 12000 / example Now, you should see a deployment being created. Get the corresponding pod. kubectl - n argo - events get deployments Output _________________________________________ / { \"context\" : { \"type\" : \"webhook\" , \"specVersi \\ | on\" : \"0.3\" , \"source\" : \"webhook-gateway\" , \"e | | ventID\" : \"30306463666539362d346666642d34 | | 3336332d383861312d336538363333613564313 | | 932\" , \"time\" : \"2020-01-11T21:23:07.682961 | | Z\" , \"dataContentType\" : \"application/json\" | | , \"subject\" : \"example\" } , \"data\" : \"eyJoZWFkZ | | XIiOnsiQWNjZXB0IjpbIiovKiJdLCJDb250ZW50 | | LUxlbmd0aCI6WyIxOSJdLCJDb250ZW50LVR5cGU | | iOlsiYXBwbGljYXRpb24vanNvbiJdLCJVc2VyLU | | FnZW50IjpbImN1cmwvNy41NC4wIl19LCJib2R5I | \\ jp7Im1lc3NhZ2UiOiJoZXkhISJ9fQ==\" } / ----------------------------------------- \\ \\ \\ ## . ## ## ## == ## ## ## ## === / \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" ___ / === ~~~ { ~~ ~~~~ ~~~ ~~~~ ~~ ~ / ===- ~~~ \\ ______ o __ / \\ \\ __ / \\ ____ \\ ______ /","title":"Trigger Standard K8s Resources"},{"location":"tutorials/04-standard-k8s-resources/#trigger-standard-k8s-resources","text":"In the previous sections, you saw how to trigger the Argo workflows. In this tutorial, you will see how to trigger Pod and Deployment. Note: You can trigger any standard Kubernetes object. Having the ability to trigger standard Kubernetes resources is quite powerful as provides an avenue to set up pipelines for existing workloads.","title":"Trigger Standard K8s Resources"},{"location":"tutorials/04-standard-k8s-resources/#prerequisites","text":"Make sure that argo-events-sa service account has necessary permissions to create the Kubernetes resource of your choice. The Webhook gateway is already set up.","title":"Prerequisites"},{"location":"tutorials/04-standard-k8s-resources/#pod","text":"Create a sensor with K8s trigger. Pay close attention to the group , version and kind keys within the trigger resource. These keys determine the type of kubernetes object. You will notice that the group key is empty, that means we want to use core group. For any other groups, you need to specify the group key. kubectl - n argo - events apply - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / tutorials / 04 - standard - k8s - resources / sensor - pod . yaml Use either Curl or Postman to send a post request to the http://localhost:12000/example curl - d '{\"message\":\"ok\"}' - H \"Content-Type: application/json\" - X POST http : // localhost : 12000 / example Now, you should see a pod being created. kubectl - n argo - events get po Output _________________________________________ / { \"context\" : { \"type\" : \"webhook\" , \"specVersi \\ | on\" : \"0.3\" , \"source\" : \"webhook-gateway\" , \"e | | ventID\" : \"30306463666539362d346666642d34 | | 3336332d383861312d336538363333613564313 | | 932\" , \"time\" : \"2020-01-11T21:23:07.682961 | | Z\" , \"dataContentType\" : \"application/json\" | | , \"subject\" : \"example\" } , \"data\" : \"eyJoZWFkZ | | XIiOnsiQWNjZXB0IjpbIiovKiJdLCJDb250ZW50 | | LUxlbmd0aCI6WyIxOSJdLCJDb250ZW50LVR5cGU | | iOlsiYXBwbGljYXRpb24vanNvbiJdLCJVc2VyLU | | FnZW50IjpbImN1cmwvNy41NC4wIl19LCJib2R5I | \\ jp7Im1lc3NhZ2UiOiJoZXkhISJ9fQ==\" } / ----------------------------------------- \\ \\ \\ ## . ## ## ## == ## ## ## ## === / \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" ___ / === ~~~ { ~~ ~~~~ ~~~ ~~~~ ~~ ~ / ===- ~~~ \\ ______ o __ / \\ \\ __ / \\ ____ \\ ______ /","title":"Pod"},{"location":"tutorials/04-standard-k8s-resources/#deployment","text":"Lets create a sensor with a K8s deployment as trigger. kubectl - n argo - events apply - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / tutorials / 04 - standard - k8s - resources / sensor - deployment . yaml Use either Curl or Postman to send a post request to the http://localhost:12000/example curl - d '{\"message\":\"ok\"}' - H \"Content-Type: application/json\" - X POST http : // localhost : 12000 / example Now, you should see a deployment being created. Get the corresponding pod. kubectl - n argo - events get deployments Output _________________________________________ / { \"context\" : { \"type\" : \"webhook\" , \"specVersi \\ | on\" : \"0.3\" , \"source\" : \"webhook-gateway\" , \"e | | ventID\" : \"30306463666539362d346666642d34 | | 3336332d383861312d336538363333613564313 | | 932\" , \"time\" : \"2020-01-11T21:23:07.682961 | | Z\" , \"dataContentType\" : \"application/json\" | | , \"subject\" : \"example\" } , \"data\" : \"eyJoZWFkZ | | XIiOnsiQWNjZXB0IjpbIiovKiJdLCJDb250ZW50 | | LUxlbmd0aCI6WyIxOSJdLCJDb250ZW50LVR5cGU | | iOlsiYXBwbGljYXRpb24vanNvbiJdLCJVc2VyLU | | FnZW50IjpbImN1cmwvNy41NC4wIl19LCJib2R5I | \\ jp7Im1lc3NhZ2UiOiJoZXkhISJ9fQ==\" } / ----------------------------------------- \\ \\ \\ ## . ## ## ## == ## ## ## ## === / \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" ___ / === ~~~ { ~~ ~~~~ ~~~ ~~~~ ~~ ~ / ===- ~~~ \\ ______ o __ / \\ \\ __ / \\ ____ \\ ______ /","title":"Deployment"},{"location":"tutorials/05-trigger-custom-resources/","text":"Trigger Custom Resources \u00b6 Argo Events supports Argo workflows, standard K8s objects, Sensor and Gateway as K8s triggers. In order to add your custom resource as trigger to Argo Events, you will need to register the scheme of resource in Argo Events. If you feel Argo Events should support your custom resource out of box, create an issue on GitHub and provide the details. Steps to build sensor image with your CR \u00b6 Fork the Argo Events project. Go to store.go in store package. Import your custom resource api package. In init method, add the scheme to your custom resource api. Make sure there are no errors. Rebuild the sensor binary using make sensor To build the image, first change IMAGE_NAMESPACE in Makefile to your docker registry and then run make sensor-image.","title":"Trigger Custom Resources"},{"location":"tutorials/05-trigger-custom-resources/#trigger-custom-resources","text":"Argo Events supports Argo workflows, standard K8s objects, Sensor and Gateway as K8s triggers. In order to add your custom resource as trigger to Argo Events, you will need to register the scheme of resource in Argo Events. If you feel Argo Events should support your custom resource out of box, create an issue on GitHub and provide the details.","title":"Trigger Custom Resources"},{"location":"tutorials/05-trigger-custom-resources/#steps-to-build-sensor-image-with-your-cr","text":"Fork the Argo Events project. Go to store.go in store package. Import your custom resource api package. In init method, add the scheme to your custom resource api. Make sure there are no errors. Rebuild the sensor binary using make sensor To build the image, first change IMAGE_NAMESPACE in Makefile to your docker registry and then run make sensor-image.","title":"Steps to build sensor image with your CR"},{"location":"tutorials/06-circuit-and-switch/","text":"Circuit and Switch \u00b6 In previous sections, you have been dealing with just a single dependency. But in many cases, you want to wait for multiple events to occur and then trigger a resource which means you need a mechanism to determine which triggers to execute based on set of different event dependencies. This mechanism is supported through Circuit and Switch . Note : Whenever you define multiple dependencies in a sensor, the sensor applies a AND operation, meaning, it will wait for all dependencies to resolve before it executes triggers. Circuit and Switch can modify that behavior. Prerequisite \u00b6 Minio server must be set up in the argo-events namespace with a bucket called test and it should be available at minio-service.argo-events:9000 . Circuit \u00b6 A circuit is a boolean expression. To create a circuit, you just need to define event dependencies in groups and the sensor will apply the circuit logic on those groups. If the logic results in true value, the sensor will execute the triggers else it won't. Switch \u00b6 A switch is the conditional execution gate for a trigger. Consider a scenario where you have a Webhook and Minio gateway, and you want to trigger an Argo workflow if the sensor receives an event from the Webhook gateway, but, another workflow if it receives an event from the Minio gateway. Create the webhook event source and gateway. The gateway listens to HTTP requests on port 12000 kubectl - n argo - events apply - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / tutorials / 05 - circuit - and - switches / webhook - event - source . yaml kubectl - n argo - events apply - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / tutorials / 05 - circuit - and - switches / webhook - gateway . yaml Create the minio event source and gateway. The gateway listens to events of type PUT and DELETE for objects in bucket test . kubectl - n argo - events apply - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / tutorials / 05 - circuit - and - switches / minio - event - source . yaml kubectl - n argo - events apply - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / tutorials / 05 - circuit - and - switches / minio - gateway . yaml Make sure there are no errors in any of the gateways and all event sources are active. Lets create the sensor. If you take a closer look at the trigger templates, you will notice that it contains switch key with all condition, meaning, execute this trigger when every group defined in all is resolved. In the sensor definition, there is only one group under all in both trigger templates. So, as soon as the group is resolved, the corresponding trigger will be executed. kubectl - n argo - events apply - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / tutorials / 06 - circuit - and - switches / sensor - 01 . yaml Send a HTTP request to Webhook gateway, curl - d '{\"message\":\"this is my first webhook\"}' - H \"Content-Type: application/json\" - X POST http : // localhost : 12000 / example You will notice an Argo worklfow with name group-1-xxxx is created with following output, __________________________ < this is my first webhook > -------------------------- \\ \\ \\ ## . ## ## ## == ## ## ## ## === / \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" ___ / === ~~~ { ~~ ~~~~ ~~~ ~~~~ ~~ ~ / ===- ~~~ \\ ______ o __ / \\ \\ __ / \\ ____ \\ ______ / Now, lets generate a Minio event so that we can run group-2-xxxx workflow. Drop a file onto test bucket. The workflow that will get created will print the name of the bucket as follow, ______ < test > ------ \\ \\ \\ ## . ## ## ## == ## ## ## ## === / \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" ___ / === ~~~ { ~~ ~~~~ ~~~ ~~~~ ~~ ~ / ===- ~~~ \\ ______ o __ / \\ \\ __ / \\ ____ \\ ______ / Great!! You have now learned how to use a circuit and switch . Lets update the sensor with a trigger that waits for both groups to resolve. This is the normal sensor behavior if circuit is not defined. kubectl - n argo - events apply - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / tutorials / 06 - circuit - and - switches / sensor - 02 . yaml Send a HTTP request and perform a file drop on Minio bucket as done above. You should following output, _______________________________ < this is my first webhook test > ------------------------------- \\ \\ \\ ## . ## ## ## == ## ## ## ## === / \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" ___ / === ~~~ { ~~ ~~~~ ~~~ ~~~~ ~~ ~ / ===- ~~~ \\ ______ o __ / \\ \\ __ / \\ ____ \\ ______ /","title":"Circuit and Switch"},{"location":"tutorials/06-circuit-and-switch/#circuit-and-switch","text":"In previous sections, you have been dealing with just a single dependency. But in many cases, you want to wait for multiple events to occur and then trigger a resource which means you need a mechanism to determine which triggers to execute based on set of different event dependencies. This mechanism is supported through Circuit and Switch . Note : Whenever you define multiple dependencies in a sensor, the sensor applies a AND operation, meaning, it will wait for all dependencies to resolve before it executes triggers. Circuit and Switch can modify that behavior.","title":"Circuit and Switch"},{"location":"tutorials/06-circuit-and-switch/#prerequisite","text":"Minio server must be set up in the argo-events namespace with a bucket called test and it should be available at minio-service.argo-events:9000 .","title":"Prerequisite"},{"location":"tutorials/06-circuit-and-switch/#circuit","text":"A circuit is a boolean expression. To create a circuit, you just need to define event dependencies in groups and the sensor will apply the circuit logic on those groups. If the logic results in true value, the sensor will execute the triggers else it won't.","title":"Circuit"},{"location":"tutorials/06-circuit-and-switch/#switch","text":"A switch is the conditional execution gate for a trigger. Consider a scenario where you have a Webhook and Minio gateway, and you want to trigger an Argo workflow if the sensor receives an event from the Webhook gateway, but, another workflow if it receives an event from the Minio gateway. Create the webhook event source and gateway. The gateway listens to HTTP requests on port 12000 kubectl - n argo - events apply - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / tutorials / 05 - circuit - and - switches / webhook - event - source . yaml kubectl - n argo - events apply - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / tutorials / 05 - circuit - and - switches / webhook - gateway . yaml Create the minio event source and gateway. The gateway listens to events of type PUT and DELETE for objects in bucket test . kubectl - n argo - events apply - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / tutorials / 05 - circuit - and - switches / minio - event - source . yaml kubectl - n argo - events apply - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / tutorials / 05 - circuit - and - switches / minio - gateway . yaml Make sure there are no errors in any of the gateways and all event sources are active. Lets create the sensor. If you take a closer look at the trigger templates, you will notice that it contains switch key with all condition, meaning, execute this trigger when every group defined in all is resolved. In the sensor definition, there is only one group under all in both trigger templates. So, as soon as the group is resolved, the corresponding trigger will be executed. kubectl - n argo - events apply - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / tutorials / 06 - circuit - and - switches / sensor - 01 . yaml Send a HTTP request to Webhook gateway, curl - d '{\"message\":\"this is my first webhook\"}' - H \"Content-Type: application/json\" - X POST http : // localhost : 12000 / example You will notice an Argo worklfow with name group-1-xxxx is created with following output, __________________________ < this is my first webhook > -------------------------- \\ \\ \\ ## . ## ## ## == ## ## ## ## === / \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" ___ / === ~~~ { ~~ ~~~~ ~~~ ~~~~ ~~ ~ / ===- ~~~ \\ ______ o __ / \\ \\ __ / \\ ____ \\ ______ / Now, lets generate a Minio event so that we can run group-2-xxxx workflow. Drop a file onto test bucket. The workflow that will get created will print the name of the bucket as follow, ______ < test > ------ \\ \\ \\ ## . ## ## ## == ## ## ## ## === / \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" ___ / === ~~~ { ~~ ~~~~ ~~~ ~~~~ ~~ ~ / ===- ~~~ \\ ______ o __ / \\ \\ __ / \\ ____ \\ ______ / Great!! You have now learned how to use a circuit and switch . Lets update the sensor with a trigger that waits for both groups to resolve. This is the normal sensor behavior if circuit is not defined. kubectl - n argo - events apply - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / tutorials / 06 - circuit - and - switches / sensor - 02 . yaml Send a HTTP request and perform a file drop on Minio bucket as done above. You should following output, _______________________________ < this is my first webhook test > ------------------------------- \\ \\ \\ ## . ## ## ## == ## ## ## ## === / \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" ___ / === ~~~ { ~~ ~~~~ ~~~ ~~~~ ~~ ~ / ===- ~~~ \\ ______ o __ / \\ \\ __ / \\ ____ \\ ______ /","title":"Switch"},{"location":"tutorials/07-filters/","text":"Filters \u00b6 In previous sections, you have seen how to trigger an Argo workflow based on events. In this tutorial, you will learn how to apply filters on event data and context. Filters provide a powerful mechanism to apply constraints on the events in order to determine a validity. Argo Events offers 3 types of filters: Data Filter Context Filter Time Filter Prerequisite \u00b6 Webhook gateway must be set up. Data Filter \u00b6 Data filter as the name suggests are applied on the event data. A CloudEvent from Webhook gateway has payload structure as, { \"context\" : { \"type\" : \"type_of_gateway\" , \"specVersion\" : \"cloud_events_version\" , \"source\" : \"name_of_the_gateway\" , \"eventID\" : \"unique_event_id\" , \"time\" : \"event_time\" , \"dataContentType\" : \"type_of_data\" , \"subject\" : \"name_of_the_event_within_event_source\" } , \"data\" : { \"header\" : {} , \"body\" : {} , } } Data Filter are applied on data within the payload. We will make a simple HTTP request to webhook gateway with request data as {\"message\":\"this is my first webhook\"} and apply data filter on message . A data filter has following fields, data : - path : path_within_event_data type : types_of_the_data value : - list_of_possible_values Comparator \u00b6 The data filter offers comparator \u201c>=\u201d, \u201c>\u201d, \u201c=\u201d, \u201c<\u201d, or \u201c<=\u201d. e.g., filters : name : data - filter data : - path : body . value type : number comparator : \">\" value : - \"50.0\" Note : If data type is a string , then you can pass either an exact value or a regex. If data types is bool or float, then you need to pass the exact value. Lets create a webhook sensor with data filter. kubectl - n argo - events apply - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / tutorials / 07 - filters / sensor - data - filters . yaml Send a HTTP request to gateway curl - d '{\"message\":\"this is my first webhook\"}' - H \"Content-Type: application/json\" - X POST http : // localhost : 12000 / example You will notice that the sensor logs prints the event is invalid as the sensor expects for either hello or hey as the value of body.message . Send a valid HTTP request to gateway curl - d '{\"message\":\"hello\"}' - H \"Content-Type: application/json\" - X POST http : // localhost : 12000 / example Watch for a workflow with name data-workflow-xxxx . Context Filter \u00b6 Similar to the data filter, you can apply a filter on the context of the event. Change the subscriber in the webhook gateway to point it to context-filter sensor's URL. Lets create a webhook sensor with context filter. kubectl - n argo - events apply - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / tutorials / 07 - filters / sensor - context - filter . yaml Send a HTTP request to gateway curl - d '{\"message\":\"this is my first webhook\"}' - H \"Content-Type: application/json\" - X POST http : // localhost : 12000 / example You will notice that the sensor logs prints the event is invalid as the sensor expects for either custom-webhook as the value of the source . Time Filter \u00b6 Time filter is specially helpful when you need to make sure an event occurs between a certain time-frame. Time filter takes a start and stop time but you can also define just the start time, meaning, there is no end time constraint or just the stop time, meaning, there is no start time constraint. An example of time filter is available under examples/sensors .","title":"Filters"},{"location":"tutorials/07-filters/#filters","text":"In previous sections, you have seen how to trigger an Argo workflow based on events. In this tutorial, you will learn how to apply filters on event data and context. Filters provide a powerful mechanism to apply constraints on the events in order to determine a validity. Argo Events offers 3 types of filters: Data Filter Context Filter Time Filter","title":"Filters"},{"location":"tutorials/07-filters/#prerequisite","text":"Webhook gateway must be set up.","title":"Prerequisite"},{"location":"tutorials/07-filters/#data-filter","text":"Data filter as the name suggests are applied on the event data. A CloudEvent from Webhook gateway has payload structure as, { \"context\" : { \"type\" : \"type_of_gateway\" , \"specVersion\" : \"cloud_events_version\" , \"source\" : \"name_of_the_gateway\" , \"eventID\" : \"unique_event_id\" , \"time\" : \"event_time\" , \"dataContentType\" : \"type_of_data\" , \"subject\" : \"name_of_the_event_within_event_source\" } , \"data\" : { \"header\" : {} , \"body\" : {} , } } Data Filter are applied on data within the payload. We will make a simple HTTP request to webhook gateway with request data as {\"message\":\"this is my first webhook\"} and apply data filter on message . A data filter has following fields, data : - path : path_within_event_data type : types_of_the_data value : - list_of_possible_values","title":"Data Filter"},{"location":"tutorials/07-filters/#comparator","text":"The data filter offers comparator \u201c>=\u201d, \u201c>\u201d, \u201c=\u201d, \u201c<\u201d, or \u201c<=\u201d. e.g., filters : name : data - filter data : - path : body . value type : number comparator : \">\" value : - \"50.0\" Note : If data type is a string , then you can pass either an exact value or a regex. If data types is bool or float, then you need to pass the exact value. Lets create a webhook sensor with data filter. kubectl - n argo - events apply - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / tutorials / 07 - filters / sensor - data - filters . yaml Send a HTTP request to gateway curl - d '{\"message\":\"this is my first webhook\"}' - H \"Content-Type: application/json\" - X POST http : // localhost : 12000 / example You will notice that the sensor logs prints the event is invalid as the sensor expects for either hello or hey as the value of body.message . Send a valid HTTP request to gateway curl - d '{\"message\":\"hello\"}' - H \"Content-Type: application/json\" - X POST http : // localhost : 12000 / example Watch for a workflow with name data-workflow-xxxx .","title":"Comparator"},{"location":"tutorials/07-filters/#context-filter","text":"Similar to the data filter, you can apply a filter on the context of the event. Change the subscriber in the webhook gateway to point it to context-filter sensor's URL. Lets create a webhook sensor with context filter. kubectl - n argo - events apply - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / tutorials / 07 - filters / sensor - context - filter . yaml Send a HTTP request to gateway curl - d '{\"message\":\"this is my first webhook\"}' - H \"Content-Type: application/json\" - X POST http : // localhost : 12000 / example You will notice that the sensor logs prints the event is invalid as the sensor expects for either custom-webhook as the value of the source .","title":"Context Filter"},{"location":"tutorials/07-filters/#time-filter","text":"Time filter is specially helpful when you need to make sure an event occurs between a certain time-frame. Time filter takes a start and stop time but you can also define just the start time, meaning, there is no end time constraint or just the stop time, meaning, there is no start time constraint. An example of time filter is available under examples/sensors .","title":"Time Filter"},{"location":"tutorials/08-policy/","text":"Policy \u00b6 A policy for a trigger determines whether the trigger resulted in success or failure. Currently, Argo Events supports 2 types of policies: Policy based on the K8s resource labels. Policy based on the response status for triggers like HTTP request, AWS Lambda, etc. Resource Labels Policy \u00b6 This type of policy determines whether trigger completed successfully based on the labels set on the trigger resource. Consider a sensor which has an Argo workflow as the trigger. When an Argo workflow completes successfully, the workflow controller sets a label on the resource as workflows.argoproj.io/completed: 'true' . So, in order for sensor to determine whether the trigger workflow completed successfully, you just need to set the policy labels as workflows.argoproj.io/completed: 'true' under trigger template. In addition to labels, you can also define a backoff and option to error out if sensor is unable to determine status of the trigger after the backoff completes. Check out the specification of resource labels policy here . Status Policy \u00b6 For triggers like HTTP request or AWS Lambda, you can apply the Status Policy to determine the trigger status. The Status Policy supports list of expected response statuses. If the status of the HTTP request or Lamda is within the statuses defined in the policy, then the trigger is considered successful. Complete specification is available here .","title":"Policy"},{"location":"tutorials/08-policy/#policy","text":"A policy for a trigger determines whether the trigger resulted in success or failure. Currently, Argo Events supports 2 types of policies: Policy based on the K8s resource labels. Policy based on the response status for triggers like HTTP request, AWS Lambda, etc.","title":"Policy"},{"location":"tutorials/08-policy/#resource-labels-policy","text":"This type of policy determines whether trigger completed successfully based on the labels set on the trigger resource. Consider a sensor which has an Argo workflow as the trigger. When an Argo workflow completes successfully, the workflow controller sets a label on the resource as workflows.argoproj.io/completed: 'true' . So, in order for sensor to determine whether the trigger workflow completed successfully, you just need to set the policy labels as workflows.argoproj.io/completed: 'true' under trigger template. In addition to labels, you can also define a backoff and option to error out if sensor is unable to determine status of the trigger after the backoff completes. Check out the specification of resource labels policy here .","title":"Resource Labels Policy"},{"location":"tutorials/08-policy/#status-policy","text":"For triggers like HTTP request or AWS Lambda, you can apply the Status Policy to determine the trigger status. The Status Policy supports list of expected response statuses. If the status of the HTTP request or Lamda is within the statuses defined in the policy, then the trigger is considered successful. Complete specification is available here .","title":"Status Policy"},{"location":"tutorials/09-events-over-nats/","text":"Events Over NATS \u00b6 Up until now, you have seen the gateway dispatch events to sensor over HTTP. In this section, we will see how to dispatch events over NATS . Prerequisites \u00b6 NATS cluster must be up and running. If you are looking to set up a test cluster, apiVersion : v1 kind : Service metadata : name : nats namespace : argo - events labels : component : nats spec : selector : component : nats type : ClusterIP ports : - name : client port : 4222 - name : cluster port : 6222 - name : monitor port : 8222 --- apiVersion : apps / v1beta1 kind : StatefulSet metadata : name : nats namespace : argo - events labels : component : nats spec : serviceName : nats replicas : 1 template : metadata : labels : component : nats spec : serviceAccountName : argo - events - sa containers : - name : nats image : nats : latest ports : - containerPort : 4222 name : client - containerPort : 6222 name : cluster - containerPort : 8222 name : monitor livenessProbe : httpGet : path : / port : 8222 initialDelaySeconds : 10 timeoutSeconds : 5 Setup \u00b6 Create a webhook gateway as below, apiVersion : argoproj . io / v1alpha1 kind : Gateway metadata : name : webhook - gateway - multi - subscribers labels : # gateway controller with instanceId \"argo-events\" will process this gateway gateways . argoproj . io / gateway - controller - instanceid : argo - events spec : replica : 1 type : webhook eventSourceRef : name : webhook - event - source template : metadata : name : webhook - gateway labels : gateway - name : webhook - gateway spec : containers : - name : gateway - client image : argoproj / gateway - client : v0 . 14.0 imagePullPolicy : Always command : [ \"/bin/gateway-client\" ] - name : webhook - events image : argoproj / webhook - gateway : v0 . 14.0 imagePullPolicy : Always command : [ \"/bin/webhook-gateway\" ] serviceAccountName : argo - events - sa service : metadata : name : webhook - gateway - svc spec : selector : gateway - name : webhook - gateway ports : - port : 12000 targetPort : 12000 type : LoadBalancer subscribers : nats : - name : webhook - sensor serverURL : nats :// nats . argo - events . svc : 4222 subject : webhook - events Take a closer look at subscribers. Instead of http , the we have declared nats as protocol for event transmission under subscribers . subscribers : nats : - name : webhook - sensor serverURL : nats :// nats . argo - events . svc : 4222 subject : webhook - sensor - subject Each subscriber has three keys, 1 . name : a unique name for the subscriber . 2 . serverURL : NATS server URL 3 . subject : name of the NATS subject to publish events . Now, create the sensor as follows, apiVersion : argoproj . io / v1alpha1 kind : Sensor metadata : name : webhook - sensor labels : sensors . argoproj . io / sensor - controller - instanceid : argo - events spec : template : spec : containers : - name : sensor image : argoproj / sensor : v0 .14.0 imagePullPolicy : Always serviceAccountName : argo - events - sa dependencies : - name : test - dep gatewayName : webhook - gateway eventName : example subscription : nats : serverURL : nats : // nats . argo - events . svc : 4222 subject : webhook - events triggers : - template : name : webhook - workflow - trigger k8s : group : argoproj . io version : v1alpha1 resource : workflows operation : create source : resource : apiVersion : argoproj . io / v1alpha1 kind : Workflow metadata : generateName : webhook - spec : entrypoint : whalesay arguments : parameters : - name : message # the value will get overridden by event payload from test - dep value : hello world templates : - name : whalesay inputs : parameters : - name : message container : image : docker / whalesay : latest command : [ cowsay ] args : [ \"{{inputs.parameters.message}}\" ] parameters : - src : dependencyName : test - dep dest : spec . arguments . parameters .0 . value The subscription under sensor spec defines NATS as protocol for event processing. subscription : nats : serverURL : nats :// nats . argo - events . svc : 4222 subject : webhook - events Create event source for webhook as follows, kubectl - n argo - events apply - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / event - sources / webhook . yaml Expose the gateway pod via Ingress, OpenShift Route or port forward to consume requests over HTTP. kubectl - n argo - events port - forward < gateway - pod - name > 12000 : 12000 Use either Curl or Postman to send a post request to the http://localhost:12000/example curl - d '{\"message\":\"this is my first webhook\"}' - H \"Content-Type: application/json\" - X POST http : // localhost : 12000 / example You should see an Argo workflow being created. kubectl - n argo - events get wf Events over HTTP and NATS \u00b6 You can easily set up a gateway to send events over both HTTP and NATS, apiVersion : argoproj . io / v1alpha1 kind : Gateway metadata : name : webhook - gateway - multi - subscribers labels : # gateway controller with instanceId \"argo-events\" will process this gateway gateways . argoproj . io / gateway - controller - instanceid : argo - events spec : replica : 1 type : webhook eventSourceRef : name : webhook - event - source template : metadata : name : webhook - gateway labels : gateway - name : webhook - gateway spec : containers : - name : gateway - client image : argoproj / gateway - client : v0 . 14 . 0 imagePullPolicy : Always command : [ \"/bin/gateway-client\" ] - name : webhook - events image : argoproj / webhook - gateway : v0 . 14 . 0 imagePullPolicy : Always command : [ \"/bin/webhook-gateway\" ] serviceAccountName : argo - events - sa service : metadata : name : webhook - gateway - svc spec : selector : gateway - name : webhook - gateway ports : - port : 12000 targetPort : 12000 type : LoadBalancer subscribers : http : - \"http://webhook-time-filter-sensor.argo-events.svc:9300/\" nats : - name : webhook - sensor serverURL : nats : // nats . argo - events . svc : 4222 subject : webhook - events The subscribers list both http and nats subscribers. The sensor can also have both http and nats subscription as follows, apiVersion : argoproj . io / v1alpha1 kind : Sensor metadata : name : webhook - sensor - over - http - and - nats labels : sensors . argoproj . io / sensor - controller - instanceid : argo - events spec : template : spec : containers : - name : sensor image : argoproj / sensor : v0 .14.0 imagePullPolicy : Always serviceAccountName : argo - events - sa dependencies : - name : test - dep gatewayName : webhook - gateway eventName : example subscription : http : port : 9300 nats : serverURL : nats : // nats . argo - events . svc : 4222 subject : webhook - events triggers : - template : name : webhook - workflow - trigger k8s : group : argoproj . io version : v1alpha1 resource : workflows operation : create source : resource : apiVersion : argoproj . io / v1alpha1 kind : Workflow metadata : generateName : webhook - spec : entrypoint : whalesay arguments : parameters : - name : message # the value will get overridden by event payload from test - dep value : hello world templates : - name : whalesay inputs : parameters : - name : message container : image : docker / whalesay : latest command : [ cowsay ] args : [ \"{{inputs.parameters.message}}\" ] parameters : - src : dependencyName : test - dep dest : spec . arguments . parameters .0 . value Make sure that you don't define same subscriber in both http and nats under gateway subscribers. Otherwise, the gateway will end up dispatching same event twice, once over HTTP and once over NATS.","title":"Events Over NATS"},{"location":"tutorials/09-events-over-nats/#events-over-nats","text":"Up until now, you have seen the gateway dispatch events to sensor over HTTP. In this section, we will see how to dispatch events over NATS .","title":"Events Over NATS"},{"location":"tutorials/09-events-over-nats/#prerequisites","text":"NATS cluster must be up and running. If you are looking to set up a test cluster, apiVersion : v1 kind : Service metadata : name : nats namespace : argo - events labels : component : nats spec : selector : component : nats type : ClusterIP ports : - name : client port : 4222 - name : cluster port : 6222 - name : monitor port : 8222 --- apiVersion : apps / v1beta1 kind : StatefulSet metadata : name : nats namespace : argo - events labels : component : nats spec : serviceName : nats replicas : 1 template : metadata : labels : component : nats spec : serviceAccountName : argo - events - sa containers : - name : nats image : nats : latest ports : - containerPort : 4222 name : client - containerPort : 6222 name : cluster - containerPort : 8222 name : monitor livenessProbe : httpGet : path : / port : 8222 initialDelaySeconds : 10 timeoutSeconds : 5","title":"Prerequisites"},{"location":"tutorials/09-events-over-nats/#setup","text":"Create a webhook gateway as below, apiVersion : argoproj . io / v1alpha1 kind : Gateway metadata : name : webhook - gateway - multi - subscribers labels : # gateway controller with instanceId \"argo-events\" will process this gateway gateways . argoproj . io / gateway - controller - instanceid : argo - events spec : replica : 1 type : webhook eventSourceRef : name : webhook - event - source template : metadata : name : webhook - gateway labels : gateway - name : webhook - gateway spec : containers : - name : gateway - client image : argoproj / gateway - client : v0 . 14.0 imagePullPolicy : Always command : [ \"/bin/gateway-client\" ] - name : webhook - events image : argoproj / webhook - gateway : v0 . 14.0 imagePullPolicy : Always command : [ \"/bin/webhook-gateway\" ] serviceAccountName : argo - events - sa service : metadata : name : webhook - gateway - svc spec : selector : gateway - name : webhook - gateway ports : - port : 12000 targetPort : 12000 type : LoadBalancer subscribers : nats : - name : webhook - sensor serverURL : nats :// nats . argo - events . svc : 4222 subject : webhook - events Take a closer look at subscribers. Instead of http , the we have declared nats as protocol for event transmission under subscribers . subscribers : nats : - name : webhook - sensor serverURL : nats :// nats . argo - events . svc : 4222 subject : webhook - sensor - subject Each subscriber has three keys, 1 . name : a unique name for the subscriber . 2 . serverURL : NATS server URL 3 . subject : name of the NATS subject to publish events . Now, create the sensor as follows, apiVersion : argoproj . io / v1alpha1 kind : Sensor metadata : name : webhook - sensor labels : sensors . argoproj . io / sensor - controller - instanceid : argo - events spec : template : spec : containers : - name : sensor image : argoproj / sensor : v0 .14.0 imagePullPolicy : Always serviceAccountName : argo - events - sa dependencies : - name : test - dep gatewayName : webhook - gateway eventName : example subscription : nats : serverURL : nats : // nats . argo - events . svc : 4222 subject : webhook - events triggers : - template : name : webhook - workflow - trigger k8s : group : argoproj . io version : v1alpha1 resource : workflows operation : create source : resource : apiVersion : argoproj . io / v1alpha1 kind : Workflow metadata : generateName : webhook - spec : entrypoint : whalesay arguments : parameters : - name : message # the value will get overridden by event payload from test - dep value : hello world templates : - name : whalesay inputs : parameters : - name : message container : image : docker / whalesay : latest command : [ cowsay ] args : [ \"{{inputs.parameters.message}}\" ] parameters : - src : dependencyName : test - dep dest : spec . arguments . parameters .0 . value The subscription under sensor spec defines NATS as protocol for event processing. subscription : nats : serverURL : nats :// nats . argo - events . svc : 4222 subject : webhook - events Create event source for webhook as follows, kubectl - n argo - events apply - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / event - sources / webhook . yaml Expose the gateway pod via Ingress, OpenShift Route or port forward to consume requests over HTTP. kubectl - n argo - events port - forward < gateway - pod - name > 12000 : 12000 Use either Curl or Postman to send a post request to the http://localhost:12000/example curl - d '{\"message\":\"this is my first webhook\"}' - H \"Content-Type: application/json\" - X POST http : // localhost : 12000 / example You should see an Argo workflow being created. kubectl - n argo - events get wf","title":"Setup"},{"location":"tutorials/09-events-over-nats/#events-over-http-and-nats","text":"You can easily set up a gateway to send events over both HTTP and NATS, apiVersion : argoproj . io / v1alpha1 kind : Gateway metadata : name : webhook - gateway - multi - subscribers labels : # gateway controller with instanceId \"argo-events\" will process this gateway gateways . argoproj . io / gateway - controller - instanceid : argo - events spec : replica : 1 type : webhook eventSourceRef : name : webhook - event - source template : metadata : name : webhook - gateway labels : gateway - name : webhook - gateway spec : containers : - name : gateway - client image : argoproj / gateway - client : v0 . 14 . 0 imagePullPolicy : Always command : [ \"/bin/gateway-client\" ] - name : webhook - events image : argoproj / webhook - gateway : v0 . 14 . 0 imagePullPolicy : Always command : [ \"/bin/webhook-gateway\" ] serviceAccountName : argo - events - sa service : metadata : name : webhook - gateway - svc spec : selector : gateway - name : webhook - gateway ports : - port : 12000 targetPort : 12000 type : LoadBalancer subscribers : http : - \"http://webhook-time-filter-sensor.argo-events.svc:9300/\" nats : - name : webhook - sensor serverURL : nats : // nats . argo - events . svc : 4222 subject : webhook - events The subscribers list both http and nats subscribers. The sensor can also have both http and nats subscription as follows, apiVersion : argoproj . io / v1alpha1 kind : Sensor metadata : name : webhook - sensor - over - http - and - nats labels : sensors . argoproj . io / sensor - controller - instanceid : argo - events spec : template : spec : containers : - name : sensor image : argoproj / sensor : v0 .14.0 imagePullPolicy : Always serviceAccountName : argo - events - sa dependencies : - name : test - dep gatewayName : webhook - gateway eventName : example subscription : http : port : 9300 nats : serverURL : nats : // nats . argo - events . svc : 4222 subject : webhook - events triggers : - template : name : webhook - workflow - trigger k8s : group : argoproj . io version : v1alpha1 resource : workflows operation : create source : resource : apiVersion : argoproj . io / v1alpha1 kind : Workflow metadata : generateName : webhook - spec : entrypoint : whalesay arguments : parameters : - name : message # the value will get overridden by event payload from test - dep value : hello world templates : - name : whalesay inputs : parameters : - name : message container : image : docker / whalesay : latest command : [ cowsay ] args : [ \"{{inputs.parameters.message}}\" ] parameters : - src : dependencyName : test - dep dest : spec . arguments . parameters .0 . value Make sure that you don't define same subscriber in both http and nats under gateway subscribers. Otherwise, the gateway will end up dispatching same event twice, once over HTTP and once over NATS.","title":"Events over HTTP and NATS"}]}