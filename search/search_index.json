{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Argo Events \u00b6 What is Argo Events? \u00b6 Argo Events is an event-based dependency manager for Kubernetes which helps you define multiple dependencies from a variety of event sources like webhook, s3, schedules, streams etc. and trigger Kubernetes objects after successful event dependencies resolution. Features \u00b6 Manage dependencies from 20+ event sources. Ability to customize business-level constraint logic for event dependencies resolution. Manage everything from simple, linear, real-time dependencies to complex, multi-source, batch job dependencies. Supports AWS Lambda and OpenFaas as triggers. Supports integration of existing API servers with 20+ event sources. CloudEvents compliant. Event Sources \u00b6 AMQP AWS SNS AWS SQS Cron Schedules GCP PubSub GitHub GitLab HDFS File Based Events Kafka Minio NATS MQTT K8s Resources Slack NetApp StorageGrid Webhooks Stripe NSQ Emitter Redis Azure Events Hub Installation \u00b6 Follow the setup to install Argo Events. Quick Start \u00b6 Check out the quick start guide to trigger Argo workflows on webhook events. Deep Dive \u00b6 Explore the tutorial to dive deep into Argo Events features.","title":"Overview"},{"location":"#argo-events","text":"","title":"Argo Events"},{"location":"#what-is-argo-events","text":"Argo Events is an event-based dependency manager for Kubernetes which helps you define multiple dependencies from a variety of event sources like webhook, s3, schedules, streams etc. and trigger Kubernetes objects after successful event dependencies resolution.","title":"What is Argo Events?"},{"location":"#features","text":"Manage dependencies from 20+ event sources. Ability to customize business-level constraint logic for event dependencies resolution. Manage everything from simple, linear, real-time dependencies to complex, multi-source, batch job dependencies. Supports AWS Lambda and OpenFaas as triggers. Supports integration of existing API servers with 20+ event sources. CloudEvents compliant.","title":"Features"},{"location":"#event-sources","text":"AMQP AWS SNS AWS SQS Cron Schedules GCP PubSub GitHub GitLab HDFS File Based Events Kafka Minio NATS MQTT K8s Resources Slack NetApp StorageGrid Webhooks Stripe NSQ Emitter Redis Azure Events Hub","title":"Event Sources"},{"location":"#installation","text":"Follow the setup to install Argo Events.","title":"Installation"},{"location":"#quick-start","text":"Check out the quick start guide to trigger Argo workflows on webhook events.","title":"Quick Start"},{"location":"#deep-dive","text":"Explore the tutorial to dive deep into Argo Events features.","title":"Deep Dive"},{"location":"FAQ/","text":"FAQs \u00b6 Q. How to get started with Argo Events? **A Recommended way to get started with Argo Events is, 1. Read the basic concepts about Gateway , Sensor and Event Source . 2. Install the setup as outlined here . 3. Read the tutorials available here . Q. Can I deploy gateway and sensor in a namespace different that argo-events ? A . Yes. If you want to deploy the gateway in a different namespace that argo-events , then please update the gateway definition with desired namespace and service account. Make sure to grant the service account the necessary roles. Also note that the gateway and sensor controllers are configured to process the gateway and sensor resources in argo-events namespace with instance-id argo-events . You can change the configuration by updating the appropriate controller configmap. Q. Gateway is receiving the events but nothing happens. A . First, check the sensor resource is deployed and a pod is created for the resource. If sensor pod is running, check the subscribers list in the gateway resource. The sensor service url must be registered as a subscriber in order to receive events from gateway. The gateway-client container should also log an error related to this situation. Second, if the gateway was able to send an event to sensor, then check the sensor logs, either the sensor event resolution circuitry has rejected the event or the sensor failed to execute the trigger due to an error. Q. Helm chart installation does not work. A. The helm chart for argo events is maintained by the community and can be out of sync with latest release version. The official installation file is available here . If you notice the helm chart is outdated, we encourage you to contribute to the argo-helm . Q. Kustomization file doesn't have a X resource. A. The kustomization.yaml file is maintained by the community. If you notice that it is out of sync with the official installation file, please raise a PR. Q. Can I use Minio gateway for AWS S3 notifications? A. No. Minio gateway is exclusive for the Minio server. If you want to trigger workloads on AWS S3 bucket notification, then set up the AWS SNS gateway. Q. If I have multiple event dependencies and triggers in a single sensor, can I execute a specific trigger upon a specific event? A. Yes, this is precisely the functionality the sensor event resolution circuitry offers. Please take a look at the Circuit and Switch . Q. The latest image tag does not point to latest release tag? A. When it comes to image tags, the golden rule is do not trust the latest tag. Always use the pinned version of the images. We will try to keep the latest in sync with the latest release version. Q. Where can I find the event structure for a particular gateway? A. Please refer https://github.com/argoproj/argo-events/blob/master/pkg/apis/events/event-data.go to understand the structure of different types of events dispatched by gateways.","title":"FAQs"},{"location":"FAQ/#faqs","text":"Q. How to get started with Argo Events? **A Recommended way to get started with Argo Events is, 1. Read the basic concepts about Gateway , Sensor and Event Source . 2. Install the setup as outlined here . 3. Read the tutorials available here . Q. Can I deploy gateway and sensor in a namespace different that argo-events ? A . Yes. If you want to deploy the gateway in a different namespace that argo-events , then please update the gateway definition with desired namespace and service account. Make sure to grant the service account the necessary roles. Also note that the gateway and sensor controllers are configured to process the gateway and sensor resources in argo-events namespace with instance-id argo-events . You can change the configuration by updating the appropriate controller configmap. Q. Gateway is receiving the events but nothing happens. A . First, check the sensor resource is deployed and a pod is created for the resource. If sensor pod is running, check the subscribers list in the gateway resource. The sensor service url must be registered as a subscriber in order to receive events from gateway. The gateway-client container should also log an error related to this situation. Second, if the gateway was able to send an event to sensor, then check the sensor logs, either the sensor event resolution circuitry has rejected the event or the sensor failed to execute the trigger due to an error. Q. Helm chart installation does not work. A. The helm chart for argo events is maintained by the community and can be out of sync with latest release version. The official installation file is available here . If you notice the helm chart is outdated, we encourage you to contribute to the argo-helm . Q. Kustomization file doesn't have a X resource. A. The kustomization.yaml file is maintained by the community. If you notice that it is out of sync with the official installation file, please raise a PR. Q. Can I use Minio gateway for AWS S3 notifications? A. No. Minio gateway is exclusive for the Minio server. If you want to trigger workloads on AWS S3 bucket notification, then set up the AWS SNS gateway. Q. If I have multiple event dependencies and triggers in a single sensor, can I execute a specific trigger upon a specific event? A. Yes, this is precisely the functionality the sensor event resolution circuitry offers. Please take a look at the Circuit and Switch . Q. The latest image tag does not point to latest release tag? A. When it comes to image tags, the golden rule is do not trust the latest tag. Always use the pinned version of the images. We will try to keep the latest in sync with the latest release version. Q. Where can I find the event structure for a particular gateway? A. Please refer https://github.com/argoproj/argo-events/blob/master/pkg/apis/events/event-data.go to understand the structure of different types of events dispatched by gateways.","title":"FAQs"},{"location":"controllers/","text":"Controllers \u00b6 Sensor and Gateway controllers are the components which manage Sensor and Gateway objects respectively. Sensor and Gateway are Kubernetes Custom Resources. For more information on K8 CRDs visit here. Controller Configmap \u00b6 Defines the instance-id and the namespace for the controller. e.g. # The gateway-controller configmap includes configuration information for the gateway-controller apiVersion : v1 kind : ConfigMap metadata : name : gateway-controller-configmap data : config : | instanceID: argo-events # mandatory namespace: my-custom-namespace # optional namespace : If you don't provide namespace, controller will watch all namespaces for gateway resource. instanceID : it is used to map a gateway or sensor object to a controller. e.g. when you create a gateway with label gateways.argoproj.io/gateway-controller-instanceid: argo-events , a controller with label argo-events will process that gateway. instanceID is used to horizontally scale controllers, so you won't end up overwhelming a single controller with large number of gateways or sensors. Also keep in mind that instanceID has nothing to do with namespace where you are deploying controllers and gateways/sensors objects.","title":"Controllers"},{"location":"controllers/#controllers","text":"Sensor and Gateway controllers are the components which manage Sensor and Gateway objects respectively. Sensor and Gateway are Kubernetes Custom Resources. For more information on K8 CRDs visit here.","title":"Controllers"},{"location":"controllers/#controller-configmap","text":"Defines the instance-id and the namespace for the controller. e.g. # The gateway-controller configmap includes configuration information for the gateway-controller apiVersion : v1 kind : ConfigMap metadata : name : gateway-controller-configmap data : config : | instanceID: argo-events # mandatory namespace: my-custom-namespace # optional namespace : If you don't provide namespace, controller will watch all namespaces for gateway resource. instanceID : it is used to map a gateway or sensor object to a controller. e.g. when you create a gateway with label gateways.argoproj.io/gateway-controller-instanceid: argo-events , a controller with label argo-events will process that gateway. instanceID is used to horizontally scale controllers, so you won't end up overwhelming a single controller with large number of gateways or sensors. Also keep in mind that instanceID has nothing to do with namespace where you are deploying controllers and gateways/sensors objects.","title":"Controller Configmap"},{"location":"developer_guide/","text":"Developer Guide \u00b6 Setup your DEV environment \u00b6 Argo Events is native to Kubernetes so you'll need a running Kubernetes cluster. This guide includes steps for Minikube for local development, but if you have another cluster you can ignore the Minikube specific step 3. Requirements \u00b6 Golang 1.11 Docker dep Installation & Setup \u00b6 1. Get the project \u00b6 go get github . com / argoproj / argo - events cd $ GOPATH / src / github . com / argoproj / argo - events 2. Vendor dependencies \u00b6 dep ensure - vendor - only 3. Start Minikube and point Docker Client to Minikube's Docker Daemon \u00b6 minikube start eval $ ( minikube docker - env ) 5. Build the project \u00b6 make all Changing Types \u00b6 If you're making a change to the pkg/apis package, please ensure you re-run the K8 code-generator scripts found in the /hack folder. First, ensure you have the generate-groups.sh script at the path: vendor/k8s.io/code-generator/ . Next run the following commands in order: $ make codegen How to write a custom gateway? \u00b6 To implement a custom gateway, you need to create a gRPC server and implement the service defined below. The framework code acts as a gRPC client consuming event stream from gateway server. Proto Definition \u00b6 The proto file is located here If you choose to implement the gateway in Go , then you can find generated client stubs here To create stubs in other languages, head over to gRPC website Service, /** * Service for handling event sources. */ service Eventing { // StartEventSource starts an event source and returns stream of events . rpc StartEventSource ( EventSource ) returns ( stream Event ) ; // ValidateEventSource validates an event source . rpc ValidateEventSource ( EventSource ) returns ( ValidEventSource ) ; } Available Environment Variables to Server \u00b6 Field Description GATEWAY_NAMESPACE K8s namespace of the gateway GATEWAY_EVENT_SOURCE_CONFIG_MAP K8s configmap containing event source GATEWAY_NAME name of the gateway GATEWAY_CONTROLLER_INSTANCE_ID gateway controller instance id GATEWAY_CONTROLLER_NAME gateway controller name GATEWAY_SERVER_PORT Port on which the gateway gRPC server should run","title":"Developer Guide"},{"location":"developer_guide/#developer-guide","text":"","title":"Developer Guide"},{"location":"developer_guide/#setup-your-dev-environment","text":"Argo Events is native to Kubernetes so you'll need a running Kubernetes cluster. This guide includes steps for Minikube for local development, but if you have another cluster you can ignore the Minikube specific step 3.","title":"Setup your DEV environment"},{"location":"developer_guide/#requirements","text":"Golang 1.11 Docker dep","title":"Requirements"},{"location":"developer_guide/#installation-setup","text":"","title":"Installation &amp; Setup"},{"location":"developer_guide/#1-get-the-project","text":"go get github . com / argoproj / argo - events cd $ GOPATH / src / github . com / argoproj / argo - events","title":"1. Get the project"},{"location":"developer_guide/#2-vendor-dependencies","text":"dep ensure - vendor - only","title":"2. Vendor dependencies"},{"location":"developer_guide/#3-start-minikube-and-point-docker-client-to-minikubes-docker-daemon","text":"minikube start eval $ ( minikube docker - env )","title":"3. Start Minikube and point Docker Client to Minikube's Docker Daemon"},{"location":"developer_guide/#5-build-the-project","text":"make all","title":"5. Build the project"},{"location":"developer_guide/#changing-types","text":"If you're making a change to the pkg/apis package, please ensure you re-run the K8 code-generator scripts found in the /hack folder. First, ensure you have the generate-groups.sh script at the path: vendor/k8s.io/code-generator/ . Next run the following commands in order: $ make codegen","title":"Changing Types"},{"location":"developer_guide/#how-to-write-a-custom-gateway","text":"To implement a custom gateway, you need to create a gRPC server and implement the service defined below. The framework code acts as a gRPC client consuming event stream from gateway server.","title":"How to write a custom gateway?"},{"location":"developer_guide/#proto-definition","text":"The proto file is located here If you choose to implement the gateway in Go , then you can find generated client stubs here To create stubs in other languages, head over to gRPC website Service, /** * Service for handling event sources. */ service Eventing { // StartEventSource starts an event source and returns stream of events . rpc StartEventSource ( EventSource ) returns ( stream Event ) ; // ValidateEventSource validates an event source . rpc ValidateEventSource ( EventSource ) returns ( ValidEventSource ) ; }","title":"Proto Definition"},{"location":"developer_guide/#available-environment-variables-to-server","text":"Field Description GATEWAY_NAMESPACE K8s namespace of the gateway GATEWAY_EVENT_SOURCE_CONFIG_MAP K8s configmap containing event source GATEWAY_NAME name of the gateway GATEWAY_CONTROLLER_INSTANCE_ID gateway controller instance id GATEWAY_CONTROLLER_NAME gateway controller name GATEWAY_SERVER_PORT Port on which the gateway gRPC server should run","title":"Available Environment Variables to Server"},{"location":"installation/","text":"Installation \u00b6 Requirements \u00b6 Kubernetes cluster >v1.9 Installed the kubectl command-line tool >v1.9.0 Using Helm Chart \u00b6 Note: This method does not work with Helm 3, only Helm 2. Make sure you have helm client installed and Tiller server is running. To install helm, follow the link. Add argoproj repository helm repo add argo https : // argoproj . github . io / argo - helm Install argo-events chart helm install argo - events argo / argo - events Using kubectl \u00b6 One Command Installation \u00b6 Deploy Argo Events SA, Roles, ConfigMap, Sensor Controller and Gateway Controller kubectl apply - f https : // raw . githubusercontent . com / argoproj / argo - events / master / hack / k8s / manifests / installation . yaml NOTE: On GKE, you may need to grant your account the ability to create new clusterroles kubectl create clusterrolebinding YOURNAME - cluster - admin - binding --clusterrole=cluster-admin --user=YOUREMAIL@gmail.com Step-by-Step Installation \u00b6 Create the namespace kubectl create namespace argo - events Create the service account kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / hack / k8s / manifests / argo - events - sa . yaml Create the cluster roles kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / hack / k8s / manifests / argo - events - cluster - roles . yaml Install the sensor custom resource definition kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / hack / k8s / manifests / sensor - crd . yaml Install the gateway custom resource definition kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / hack / k8s / manifests / gateway - crd . yaml Install the event source custom resource definition kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / hack / k8s / manifests / event - source - crd . yaml Create the confimap for sensor controller kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / hack / k8s / manifests / sensor - controller - configmap . yaml Create the configmap for gateway controller kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / hack / k8s / manifests / gateway - controller - configmap . yaml Deploy the sensor controller kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / hack / k8s / manifests / sensor - controller - deployment . yaml Deploy the gateway controller kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / hack / k8s / manifests / gateway - controller - deployment . yaml Deploy at cluster level \u00b6 To deploy Argo-Events controllers at cluster level where the controllers will be able to process gateway and sensor objects created in any namespace, Make sure to apply cluster role and binding to the service account, kubectl apply - f https : // raw . githubusercontent . com / argoproj / argo - events / master / hack / k8s / manifests / argo - events - cluster - roles . yaml Update the configmap for both gateway and sensor and remove the namespace key from it. Deploy both gateway and sensor controllers and watch the magic.","title":"Installation"},{"location":"installation/#installation","text":"","title":"Installation"},{"location":"installation/#requirements","text":"Kubernetes cluster >v1.9 Installed the kubectl command-line tool >v1.9.0","title":"Requirements"},{"location":"installation/#using-helm-chart","text":"Note: This method does not work with Helm 3, only Helm 2. Make sure you have helm client installed and Tiller server is running. To install helm, follow the link. Add argoproj repository helm repo add argo https : // argoproj . github . io / argo - helm Install argo-events chart helm install argo - events argo / argo - events","title":"Using Helm Chart"},{"location":"installation/#using-kubectl","text":"","title":"Using kubectl"},{"location":"installation/#one-command-installation","text":"Deploy Argo Events SA, Roles, ConfigMap, Sensor Controller and Gateway Controller kubectl apply - f https : // raw . githubusercontent . com / argoproj / argo - events / master / hack / k8s / manifests / installation . yaml NOTE: On GKE, you may need to grant your account the ability to create new clusterroles kubectl create clusterrolebinding YOURNAME - cluster - admin - binding --clusterrole=cluster-admin --user=YOUREMAIL@gmail.com","title":"One Command Installation"},{"location":"installation/#step-by-step-installation","text":"Create the namespace kubectl create namespace argo - events Create the service account kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / hack / k8s / manifests / argo - events - sa . yaml Create the cluster roles kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / hack / k8s / manifests / argo - events - cluster - roles . yaml Install the sensor custom resource definition kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / hack / k8s / manifests / sensor - crd . yaml Install the gateway custom resource definition kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / hack / k8s / manifests / gateway - crd . yaml Install the event source custom resource definition kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / hack / k8s / manifests / event - source - crd . yaml Create the confimap for sensor controller kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / hack / k8s / manifests / sensor - controller - configmap . yaml Create the configmap for gateway controller kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / hack / k8s / manifests / gateway - controller - configmap . yaml Deploy the sensor controller kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / hack / k8s / manifests / sensor - controller - deployment . yaml Deploy the gateway controller kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / hack / k8s / manifests / gateway - controller - deployment . yaml","title":"Step-by-Step Installation"},{"location":"installation/#deploy-at-cluster-level","text":"To deploy Argo-Events controllers at cluster level where the controllers will be able to process gateway and sensor objects created in any namespace, Make sure to apply cluster role and binding to the service account, kubectl apply - f https : // raw . githubusercontent . com / argoproj / argo - events / master / hack / k8s / manifests / argo - events - cluster - roles . yaml Update the configmap for both gateway and sensor and remove the namespace key from it. Deploy both gateway and sensor controllers and watch the magic.","title":"Deploy at cluster level"},{"location":"quick_start/","text":"Getting Started \u00b6 We are going to set up a gateway, sensor and event-source for webhook. The goal is to trigger an Argo workflow upon a HTTP Post request. Note: You will need to have Argo Workflows installed for this tutorial. First, we need to setup event sources for gateway to listen. kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / event - sources / webhook . yaml The event-source drives the configuration required for a gateway to consume events from external sources. Create webhook gateway, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / gateways / webhook . yaml After running above command, gateway controller will create corresponding a pod and service. Create webhook sensor, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / sensors / webhook . yaml Once sensor object is created, sensor controller will create corresponding pod and service. Once the gateway and sensor pods are running, dispatch a HTTP POST request to /example endpoint. Note: the WEBHOOK_SERVICE_URL will differ based on the Kubernetes cluster. export WEBHOOK_SERVICE_URL = $ ( minikube service - n argo - events --url webhook-gateway-svc) echo $ WEBHOOK_SERVICE_URL curl - d '{\"message\":\"this is my first webhook\"}' - H \"Content-Type: application/json\" - X POST $ WEBHOOK_SERVICE_URL / example Note : If you are facing an issue getting service url by running minikube service - n argo - events --url webhook-gateway-svc You can use port forwarding to access the service as well, kubectl port - forward Open another terminal window and enter kubectl port - forward - n argo - events < name_of_the_webhook_gateway_pod > 12000 : 12000 You can now use localhost:12000 to query webhook gateway Verify that an Argo workflow was triggered. kubectl - n argo - events get workflows | grep \"webhook\"","title":"Getting Started"},{"location":"quick_start/#getting-started","text":"We are going to set up a gateway, sensor and event-source for webhook. The goal is to trigger an Argo workflow upon a HTTP Post request. Note: You will need to have Argo Workflows installed for this tutorial. First, we need to setup event sources for gateway to listen. kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / event - sources / webhook . yaml The event-source drives the configuration required for a gateway to consume events from external sources. Create webhook gateway, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / gateways / webhook . yaml After running above command, gateway controller will create corresponding a pod and service. Create webhook sensor, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / sensors / webhook . yaml Once sensor object is created, sensor controller will create corresponding pod and service. Once the gateway and sensor pods are running, dispatch a HTTP POST request to /example endpoint. Note: the WEBHOOK_SERVICE_URL will differ based on the Kubernetes cluster. export WEBHOOK_SERVICE_URL = $ ( minikube service - n argo - events --url webhook-gateway-svc) echo $ WEBHOOK_SERVICE_URL curl - d '{\"message\":\"this is my first webhook\"}' - H \"Content-Type: application/json\" - X POST $ WEBHOOK_SERVICE_URL / example Note : If you are facing an issue getting service url by running minikube service - n argo - events --url webhook-gateway-svc You can use port forwarding to access the service as well, kubectl port - forward Open another terminal window and enter kubectl port - forward - n argo - events < name_of_the_webhook_gateway_pod > 12000 : 12000 You can now use localhost:12000 to query webhook gateway Verify that an Argo workflow was triggered. kubectl - n argo - events get workflows | grep \"webhook\"","title":"Getting Started"},{"location":"concepts/event_source/","text":"Event Source \u00b6 Event Sources are the configuration store for gateways. The configuration stored in an Event Source is used by a gateway to consume events from external entities like AWS SNS, SQS, GCP PubSub, Webhooks etc. Specification \u00b6 Complete specification is available here . Examples \u00b6 Examples are located under examples/event-sources .","title":"Event Source"},{"location":"concepts/event_source/#event-source","text":"Event Sources are the configuration store for gateways. The configuration stored in an Event Source is used by a gateway to consume events from external entities like AWS SNS, SQS, GCP PubSub, Webhooks etc.","title":"Event Source"},{"location":"concepts/event_source/#specification","text":"Complete specification is available here .","title":"Specification"},{"location":"concepts/event_source/#examples","text":"Examples are located under examples/event-sources .","title":"Examples"},{"location":"concepts/gateway/","text":"Gateway \u00b6 What is a gateway? \u00b6 A gateway consumes events from outside entities, transforms them into the cloudevents specification compliant events and dispatches them to sensors. There are two components for a gateway, Gateway Client \u00b6 Gateway client manages the event source for the gateway. Its responsibilities are, 1. Monitor and manage the event sources. 2. Monitor and manage the subscribers. 3. Convert the events received from the gateway server into CloudEvents. 4. Dispatch the cloudevents to subscribers. Gateway Server \u00b6 Gateway server listens to events from event sources. Its responsibilities are, 1. Validate an event source. 2. Implement the logic for consuming events from an event source. 3. Dispatch events to gateway client. Gateway & Event Source \u00b6 Event Source are event configuration store for a gateway. The configuration stored in an Event Source is used by a gateway to consume events from external entities like AWS SNS, SQS, GCP PubSub, Webhooks etc. Specification \u00b6 Complete specification is available here . Examples \u00b6 Examples are located under examples/gateways .","title":"Gateway"},{"location":"concepts/gateway/#gateway","text":"","title":"Gateway"},{"location":"concepts/gateway/#what-is-a-gateway","text":"A gateway consumes events from outside entities, transforms them into the cloudevents specification compliant events and dispatches them to sensors. There are two components for a gateway,","title":"What is a gateway?"},{"location":"concepts/gateway/#gateway-client","text":"Gateway client manages the event source for the gateway. Its responsibilities are, 1. Monitor and manage the event sources. 2. Monitor and manage the subscribers. 3. Convert the events received from the gateway server into CloudEvents. 4. Dispatch the cloudevents to subscribers.","title":"Gateway Client"},{"location":"concepts/gateway/#gateway-server","text":"Gateway server listens to events from event sources. Its responsibilities are, 1. Validate an event source. 2. Implement the logic for consuming events from an event source. 3. Dispatch events to gateway client.","title":"Gateway Server"},{"location":"concepts/gateway/#gateway-event-source","text":"Event Source are event configuration store for a gateway. The configuration stored in an Event Source is used by a gateway to consume events from external entities like AWS SNS, SQS, GCP PubSub, Webhooks etc.","title":"Gateway &amp; Event Source"},{"location":"concepts/gateway/#specification","text":"Complete specification is available here .","title":"Specification"},{"location":"concepts/gateway/#examples","text":"Examples are located under examples/gateways .","title":"Examples"},{"location":"concepts/sensor/","text":"Sensor \u00b6 Sensor defines a set of event dependencies (inputs) and triggers (outputs). It listens to events from one or more gateways and act as an event dependency manager. Event dependency \u00b6 A dependency is an event the sensor is waiting to happen. Trigger \u00b6 A Trigger is the resource executed by sensor once the event dependencies are resolved.. Specification \u00b6 Complete specification is available here . Examples \u00b6 Examples are located under examples/sensors .","title":"Sensor"},{"location":"concepts/sensor/#sensor","text":"Sensor defines a set of event dependencies (inputs) and triggers (outputs). It listens to events from one or more gateways and act as an event dependency manager.","title":"Sensor"},{"location":"concepts/sensor/#event-dependency","text":"A dependency is an event the sensor is waiting to happen.","title":"Event dependency"},{"location":"concepts/sensor/#trigger","text":"A Trigger is the resource executed by sensor once the event dependencies are resolved..","title":"Trigger"},{"location":"concepts/sensor/#specification","text":"Complete specification is available here .","title":"Specification"},{"location":"concepts/sensor/#examples","text":"Examples are located under examples/sensors .","title":"Examples"},{"location":"setup/amqp/","text":"AMQP \u00b6 AMQP gateway listens to messages on the MQ and helps sensor trigger the workloads. Event Structure \u00b6 The structure of an event dispatched by the gateway to the sensor looks like following, { \"context\" : { \"type\" : \"type_of_gateway\" , \"specVersion\" : \"cloud_events_version\" , \"source\" : \"name_of_the_gateway\" , \"eventID\" : \"unique_event_id\" , \"time\" : \"event_time\" , \"dataContentType\" : \"type_of_data\" , \"subject\" : \"name_of_the_event_within_event_source\" } , \"data\" : { \"contentType\" : \"ContentType is the MIME content type\" , \"contentEncoding\" : \"ContentEncoding is the MIME content encoding\" , \"deliveryMode\" : \"Delivery mode can be either - non-persistent (1) or persistent (2)\" , \"priority\" : \"Priority refers to the use - 0 to 9\" , \"correlationId\" : \"CorrelationId is the correlation identifier\" , \"replyTo\" : \"ReplyTo is the address to reply to (ex: RPC)\" , \"expiration\" : \"Expiration refers to message expiration spec\" , \"messageId\" : \"MessageId is message identifier\" , \"timestamp\" : \"Timestamp refers to the message timestamp\" , \"type\" : \"Type refers to the message type name\" , \"appId\" : \"AppId refers to the application id\" , \"exchange\" : \"Exchange is basic.publish exchange\" , \"routingKey\" : \"RoutingKey is basic.publish routing key\" , \"body\" : \"Body represents the messsage body\" , } } Setup \u00b6 Lets set up RabbitMQ locally, apiVersion : v1 kind : Service metadata : labels : component : rabbitmq name : rabbitmq - service spec : ports : - port : 5672 selector : app : taskQueue component : rabbitmq --- apiVersion : v1 kind : ReplicationController metadata : labels : component : rabbitmq name : rabbitmq - controller spec : replicas : 1 template : metadata : labels : app : taskQueue component : rabbitmq spec : containers : - image : rabbitmq name : rabbitmq ports : - containerPort : 5672 resources : limits : cpu : 100 m Make sure the RabbitMQ controller pod is up and running before proceeding further. Expose the RabbitMQ server to local publisher using port-forward , kubectl - n argo - events port - forward < rabbitmq - pod - name > 5672 : 5672 Create the event source by running the following command. kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / event - sources / amqp . yaml Create the gateway by running the following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / gateways / amqp . yaml Inspect the gateway pod logs to make sure the gateway was able to subscribe to the exchange specified in the event source to consume messages. Create the sensor by running the following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / sensors / amqp . yaml Lets set up a rabbitmq publisher. If you don't have pika installed, run, python - m pip install pika --upgrade Open a python REPL and run following code to publish a message on exhange called test . import pika connection = pika . BlockingConnection ( pika . ConnectionParameters ( 'localhost' )) channel = connection . channel () channel . basic_publish ( exchange = 'test' , routing_key = 'hello' , body = '{\"message\": \"hello\"}' ) As soon as you publish a message, sensor will trigger an Argo workflow. Run argo list to find the workflow. Troubleshoot \u00b6 Please read the FAQ .","title":"AMQP"},{"location":"setup/amqp/#amqp","text":"AMQP gateway listens to messages on the MQ and helps sensor trigger the workloads.","title":"AMQP"},{"location":"setup/amqp/#event-structure","text":"The structure of an event dispatched by the gateway to the sensor looks like following, { \"context\" : { \"type\" : \"type_of_gateway\" , \"specVersion\" : \"cloud_events_version\" , \"source\" : \"name_of_the_gateway\" , \"eventID\" : \"unique_event_id\" , \"time\" : \"event_time\" , \"dataContentType\" : \"type_of_data\" , \"subject\" : \"name_of_the_event_within_event_source\" } , \"data\" : { \"contentType\" : \"ContentType is the MIME content type\" , \"contentEncoding\" : \"ContentEncoding is the MIME content encoding\" , \"deliveryMode\" : \"Delivery mode can be either - non-persistent (1) or persistent (2)\" , \"priority\" : \"Priority refers to the use - 0 to 9\" , \"correlationId\" : \"CorrelationId is the correlation identifier\" , \"replyTo\" : \"ReplyTo is the address to reply to (ex: RPC)\" , \"expiration\" : \"Expiration refers to message expiration spec\" , \"messageId\" : \"MessageId is message identifier\" , \"timestamp\" : \"Timestamp refers to the message timestamp\" , \"type\" : \"Type refers to the message type name\" , \"appId\" : \"AppId refers to the application id\" , \"exchange\" : \"Exchange is basic.publish exchange\" , \"routingKey\" : \"RoutingKey is basic.publish routing key\" , \"body\" : \"Body represents the messsage body\" , } }","title":"Event Structure"},{"location":"setup/amqp/#setup","text":"Lets set up RabbitMQ locally, apiVersion : v1 kind : Service metadata : labels : component : rabbitmq name : rabbitmq - service spec : ports : - port : 5672 selector : app : taskQueue component : rabbitmq --- apiVersion : v1 kind : ReplicationController metadata : labels : component : rabbitmq name : rabbitmq - controller spec : replicas : 1 template : metadata : labels : app : taskQueue component : rabbitmq spec : containers : - image : rabbitmq name : rabbitmq ports : - containerPort : 5672 resources : limits : cpu : 100 m Make sure the RabbitMQ controller pod is up and running before proceeding further. Expose the RabbitMQ server to local publisher using port-forward , kubectl - n argo - events port - forward < rabbitmq - pod - name > 5672 : 5672 Create the event source by running the following command. kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / event - sources / amqp . yaml Create the gateway by running the following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / gateways / amqp . yaml Inspect the gateway pod logs to make sure the gateway was able to subscribe to the exchange specified in the event source to consume messages. Create the sensor by running the following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / sensors / amqp . yaml Lets set up a rabbitmq publisher. If you don't have pika installed, run, python - m pip install pika --upgrade Open a python REPL and run following code to publish a message on exhange called test . import pika connection = pika . BlockingConnection ( pika . ConnectionParameters ( 'localhost' )) channel = connection . channel () channel . basic_publish ( exchange = 'test' , routing_key = 'hello' , body = '{\"message\": \"hello\"}' ) As soon as you publish a message, sensor will trigger an Argo workflow. Run argo list to find the workflow.","title":"Setup"},{"location":"setup/amqp/#troubleshoot","text":"Please read the FAQ .","title":"Troubleshoot"},{"location":"setup/aws-sns/","text":"AWS SNS \u00b6 SNS gateway subscribes to AWS SNS topics, listens events and helps sensor trigger workloads. Event Structure \u00b6 The structure of an event dispatched by the gateway to the sensor looks like following, { \"context\" : { \"type\" : \"type_of_gateway\" , \"specVersion\" : \"cloud_events_version\" , \"source\" : \"name_of_the_gateway\" , \"eventID\" : \"unique_event_id\" , \"time\" : \"event_time\" , \"dataContentType\" : \"type_of_data\" , \"subject\" : \"name_of_the_event_within_event_source\" } , \"data\" : { \"header\" : \"sns headers\" , \"body\" : \"body refers to the sns notification data\" , } } Setup \u00b6 Create a topic called test using aws cli or AWS SNS console. Fetch your access and secret key for AWS account and base64 encode them. Create a secret called aws-secret as follows, apiVersion : v1 kind : Secret metadata : name : aws - secret type : Opaque data : accesskey : < base64 - access - key > secretkey : < base64 - secret - key > Deploy the secret kubectl - n argo - events apply - f aws - secret . yaml Create the gateway by running the following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / gateways / aws - sns . yaml Wait for gateway pod to get into the running state. Create an Ingress or Openshift Route for the gateway service to that it can be reached from AWS. You can find more information on Ingress or Route online. Get the event source stored at https://raw.githubusercontent.com/argoproj/argo-events/master/examples/event-sources/aws-sns.yaml Change the topicArn and url under webhook to your gateway service url created in a previous step. Make sure this url is reachable from AWS. Create the event source by running the following command. kubectl apply - n argo - events - f < event - source - file - updated - in - previous - step > Go to SNS settings on AWS and verify the webhook is registered. You can also do the same by looking at the gateway pod logs. Create the sensor by running the following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / sensors / aws - sns . yaml Publish a message to the SNS topic and it will trigger an argo workflow. Run argo list to find the workflow. Troubleshoot \u00b6 Please read the FAQ .","title":"AWS SNS"},{"location":"setup/aws-sns/#aws-sns","text":"SNS gateway subscribes to AWS SNS topics, listens events and helps sensor trigger workloads.","title":"AWS SNS"},{"location":"setup/aws-sns/#event-structure","text":"The structure of an event dispatched by the gateway to the sensor looks like following, { \"context\" : { \"type\" : \"type_of_gateway\" , \"specVersion\" : \"cloud_events_version\" , \"source\" : \"name_of_the_gateway\" , \"eventID\" : \"unique_event_id\" , \"time\" : \"event_time\" , \"dataContentType\" : \"type_of_data\" , \"subject\" : \"name_of_the_event_within_event_source\" } , \"data\" : { \"header\" : \"sns headers\" , \"body\" : \"body refers to the sns notification data\" , } }","title":"Event Structure"},{"location":"setup/aws-sns/#setup","text":"Create a topic called test using aws cli or AWS SNS console. Fetch your access and secret key for AWS account and base64 encode them. Create a secret called aws-secret as follows, apiVersion : v1 kind : Secret metadata : name : aws - secret type : Opaque data : accesskey : < base64 - access - key > secretkey : < base64 - secret - key > Deploy the secret kubectl - n argo - events apply - f aws - secret . yaml Create the gateway by running the following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / gateways / aws - sns . yaml Wait for gateway pod to get into the running state. Create an Ingress or Openshift Route for the gateway service to that it can be reached from AWS. You can find more information on Ingress or Route online. Get the event source stored at https://raw.githubusercontent.com/argoproj/argo-events/master/examples/event-sources/aws-sns.yaml Change the topicArn and url under webhook to your gateway service url created in a previous step. Make sure this url is reachable from AWS. Create the event source by running the following command. kubectl apply - n argo - events - f < event - source - file - updated - in - previous - step > Go to SNS settings on AWS and verify the webhook is registered. You can also do the same by looking at the gateway pod logs. Create the sensor by running the following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / sensors / aws - sns . yaml Publish a message to the SNS topic and it will trigger an argo workflow. Run argo list to find the workflow.","title":"Setup"},{"location":"setup/aws-sns/#troubleshoot","text":"Please read the FAQ .","title":"Troubleshoot"},{"location":"setup/aws-sqs/","text":"AWS SQS \u00b6 SQS gateway listens to messages on AWS SQS queue and helps sensor trigger workloads. Event Structure \u00b6 The structure of an event dispatched by the gateway to the sensor looks like following, { \" context \" : { \" type \" : \" type_of_gateway \" , \" specVersion \" : \" cloud_events_version \" , \" source \" : \" name_of_the_gateway \" , \" eventID \" : \" unique_event_id \" , \" time \" : \" event_time \" , \" dataContentType \" : \" type_of_data \" , \" subject \" : \" name_of_the_event_within_event_source \" }, \" data \" : { \" messageId \" : \" message id \" , // Each message attribute consists of a Name , Type , and Value . For more information , // see Amazon SQS Message Attributes // ( https : // docs . aws . amazon . com / AWSSimpleQueueService / latest / SQSDeveloperGuide / sqs - message - attributes . html ) // in the Amazon Simple Queue Service Developer Guide . \" messageAttributes \" : \" message attributes \" , \" body \" : \" Body is the message data \" , } } Setup \u00b6 Create a queue called test either using aws cli or AWS SQS management console. Fetch your access and secret key for AWS account and base64 encode them. Create a secret called aws-secret as follows, apiVersion : v1 kind : Secret metadata : name : aws - secret type : Opaque data : accesskey : < base64 - access - key > secretkey : < base64 - secret - key > Deploy the secret kubectl - n argo - events apply - f aws - secret . yaml Create the event source by running the following command. kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / event - sources / aws - sqs . yaml Create the gateway by running the following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / gateways / aws - sqs . yaml Inspect the gateway pod logs to make sure the gateway was able to subscribe to the queue specified in the event source to consume messages. Create the sensor by running the following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / sensors / aws - sqs . yaml Dispatch a message on sqs queue, aws sqs send - message -- queue - url https : // sqs . us - east - 1 . amazonaws . com / XXXXX / test -- message - body ' {\"message\": \"hello\"} ' Once a message is published, an argo workflow will be triggered. Run argo list to find the workflow. Troubleshoot \u00b6 Please read the FAQ .","title":"AWS SQS"},{"location":"setup/aws-sqs/#aws-sqs","text":"SQS gateway listens to messages on AWS SQS queue and helps sensor trigger workloads.","title":"AWS SQS"},{"location":"setup/aws-sqs/#event-structure","text":"The structure of an event dispatched by the gateway to the sensor looks like following, { \" context \" : { \" type \" : \" type_of_gateway \" , \" specVersion \" : \" cloud_events_version \" , \" source \" : \" name_of_the_gateway \" , \" eventID \" : \" unique_event_id \" , \" time \" : \" event_time \" , \" dataContentType \" : \" type_of_data \" , \" subject \" : \" name_of_the_event_within_event_source \" }, \" data \" : { \" messageId \" : \" message id \" , // Each message attribute consists of a Name , Type , and Value . For more information , // see Amazon SQS Message Attributes // ( https : // docs . aws . amazon . com / AWSSimpleQueueService / latest / SQSDeveloperGuide / sqs - message - attributes . html ) // in the Amazon Simple Queue Service Developer Guide . \" messageAttributes \" : \" message attributes \" , \" body \" : \" Body is the message data \" , } }","title":"Event Structure"},{"location":"setup/aws-sqs/#setup","text":"Create a queue called test either using aws cli or AWS SQS management console. Fetch your access and secret key for AWS account and base64 encode them. Create a secret called aws-secret as follows, apiVersion : v1 kind : Secret metadata : name : aws - secret type : Opaque data : accesskey : < base64 - access - key > secretkey : < base64 - secret - key > Deploy the secret kubectl - n argo - events apply - f aws - secret . yaml Create the event source by running the following command. kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / event - sources / aws - sqs . yaml Create the gateway by running the following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / gateways / aws - sqs . yaml Inspect the gateway pod logs to make sure the gateway was able to subscribe to the queue specified in the event source to consume messages. Create the sensor by running the following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / sensors / aws - sqs . yaml Dispatch a message on sqs queue, aws sqs send - message -- queue - url https : // sqs . us - east - 1 . amazonaws . com / XXXXX / test -- message - body ' {\"message\": \"hello\"} ' Once a message is published, an argo workflow will be triggered. Run argo list to find the workflow.","title":"Setup"},{"location":"setup/aws-sqs/#troubleshoot","text":"Please read the FAQ .","title":"Troubleshoot"},{"location":"setup/calendar/","text":"Calendar \u00b6 Calendar gateway generates events on either a cron schedule or an interval and help trigger workloads. Event Structure \u00b6 The structure of an event dispatched by the gateway to the sensor looks like following, { \"context\" : { \"type\" : \"type_of_gateway\" , \"specVersion\" : \"cloud_events_version\" , \"source\" : \"name_of_the_gateway\" , \"eventID\" : \"unique_event_id\" , \"time\" : \"event_time\" , \"dataContentType\" : \"type_of_data\" , \"subject\" : \"name_of_the_event_within_event_source\" } , \"data\" : { \"eventTime\" : { /* UTC time of the event */ } , \"userPayload\" : { /* static payload available in the event source */ } , } } Setup \u00b6 Install gateway in the argo-events namespace using following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / gateways / calendar . yaml Once the gateway resource is created, the gateway controller will process it and create a pod. If you don't see the pod in argo-events namespace, check the gateway controller logs for errors. If you inspect the gateway resource definition, you will notice it points to the event source called calendar-event-source . Lets install event source in the argo-events namespace, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / event - sources / calendar . yaml Check the gateway logs to make sure the gateway has processed the event source. The gateway will generate events at every 10 seconds. Lets create the sensor, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / sensors / calendar . yaml Once the sensor pod is in running state, wait for next interval to occur. Troubleshoot \u00b6 Please read the FAQ .","title":"Calendar"},{"location":"setup/calendar/#calendar","text":"Calendar gateway generates events on either a cron schedule or an interval and help trigger workloads.","title":"Calendar"},{"location":"setup/calendar/#event-structure","text":"The structure of an event dispatched by the gateway to the sensor looks like following, { \"context\" : { \"type\" : \"type_of_gateway\" , \"specVersion\" : \"cloud_events_version\" , \"source\" : \"name_of_the_gateway\" , \"eventID\" : \"unique_event_id\" , \"time\" : \"event_time\" , \"dataContentType\" : \"type_of_data\" , \"subject\" : \"name_of_the_event_within_event_source\" } , \"data\" : { \"eventTime\" : { /* UTC time of the event */ } , \"userPayload\" : { /* static payload available in the event source */ } , } }","title":"Event Structure"},{"location":"setup/calendar/#setup","text":"Install gateway in the argo-events namespace using following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / gateways / calendar . yaml Once the gateway resource is created, the gateway controller will process it and create a pod. If you don't see the pod in argo-events namespace, check the gateway controller logs for errors. If you inspect the gateway resource definition, you will notice it points to the event source called calendar-event-source . Lets install event source in the argo-events namespace, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / event - sources / calendar . yaml Check the gateway logs to make sure the gateway has processed the event source. The gateway will generate events at every 10 seconds. Lets create the sensor, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / sensors / calendar . yaml Once the sensor pod is in running state, wait for next interval to occur.","title":"Setup"},{"location":"setup/calendar/#troubleshoot","text":"Please read the FAQ .","title":"Troubleshoot"},{"location":"setup/emitter/","text":"Emitter \u00b6 Emitter gateway subscribes to a channel and helps sensor trigger the workloads. Event Structure \u00b6 The structure of an event dispatched by the gateway to the sensor looks like following, { \"context\" : { \"type\" : \"type_of_gateway\" , \"specVersion\" : \"cloud_events_version\" , \"source\" : \"name_of_the_gateway\" , \"eventID\" : \"unique_event_id\" , \"time\" : \"event_time\" , \"dataContentType\" : \"type_of_data\" , \"subject\" : \"name_of_the_event_within_event_source\" } , \"data\" : { \"topic\" : \"name_of_the_topic\" , \"body\" : \"message_payload\" } } Setup \u00b6 Deploy emitter in your local K8s cluster, --- apiVersion : v1 kind : Service metadata : name : broker labels : app : broker spec : clusterIP : None ports : - port : 4000 targetPort : 4000 selector : app : broker --- apiVersion : apps / v1 kind : Deployment metadata : name : broker spec : replicas : 1 selector : matchLabels : app : broker template : metadata : labels : app : broker spec : containers : - env : - name : EMITTER_LICENSE value : \" zT83oDV0DWY5_JysbSTPTDr8KB0AAAAAAAAAAAAAAAI \" # This is a test license , DO NOT USE IN PRODUCTION ! - name : EMITTER_CLUSTER_SEED value : \" broker \" - name : EMITTER_CLUSTER_ADVERTISE value : \" private:4000 \" name : broker image : emitter / server : latest ports : - containerPort : 8080 - containerPort : 443 - containerPort : 4000 volumeMounts : - name : broker - volume mountPath : / data volumes : - name : broker - volume hostPath : path : / emitter # directory on host Create the event source by running the following command. kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / event - sources / emitter . yaml Create the gateway by running the following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / gateways / emitter . yaml Inspect the gateway pod logs to make sure the gateway was able to subscribe to the topic specified in the event source to consume messages. Create the sensor by running the following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / sensors / emitter . yaml Send message on emitter channel using one of the clients https://emitter.io/develop/golang/ Once a message is published, an argo workflow will be triggered. Run argo list to find the workflow. Troubleshoot \u00b6 Please read the FAQ .","title":"Emitter"},{"location":"setup/emitter/#emitter","text":"Emitter gateway subscribes to a channel and helps sensor trigger the workloads.","title":"Emitter"},{"location":"setup/emitter/#event-structure","text":"The structure of an event dispatched by the gateway to the sensor looks like following, { \"context\" : { \"type\" : \"type_of_gateway\" , \"specVersion\" : \"cloud_events_version\" , \"source\" : \"name_of_the_gateway\" , \"eventID\" : \"unique_event_id\" , \"time\" : \"event_time\" , \"dataContentType\" : \"type_of_data\" , \"subject\" : \"name_of_the_event_within_event_source\" } , \"data\" : { \"topic\" : \"name_of_the_topic\" , \"body\" : \"message_payload\" } }","title":"Event Structure"},{"location":"setup/emitter/#setup","text":"Deploy emitter in your local K8s cluster, --- apiVersion : v1 kind : Service metadata : name : broker labels : app : broker spec : clusterIP : None ports : - port : 4000 targetPort : 4000 selector : app : broker --- apiVersion : apps / v1 kind : Deployment metadata : name : broker spec : replicas : 1 selector : matchLabels : app : broker template : metadata : labels : app : broker spec : containers : - env : - name : EMITTER_LICENSE value : \" zT83oDV0DWY5_JysbSTPTDr8KB0AAAAAAAAAAAAAAAI \" # This is a test license , DO NOT USE IN PRODUCTION ! - name : EMITTER_CLUSTER_SEED value : \" broker \" - name : EMITTER_CLUSTER_ADVERTISE value : \" private:4000 \" name : broker image : emitter / server : latest ports : - containerPort : 8080 - containerPort : 443 - containerPort : 4000 volumeMounts : - name : broker - volume mountPath : / data volumes : - name : broker - volume hostPath : path : / emitter # directory on host Create the event source by running the following command. kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / event - sources / emitter . yaml Create the gateway by running the following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / gateways / emitter . yaml Inspect the gateway pod logs to make sure the gateway was able to subscribe to the topic specified in the event source to consume messages. Create the sensor by running the following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / sensors / emitter . yaml Send message on emitter channel using one of the clients https://emitter.io/develop/golang/ Once a message is published, an argo workflow will be triggered. Run argo list to find the workflow.","title":"Setup"},{"location":"setup/emitter/#troubleshoot","text":"Please read the FAQ .","title":"Troubleshoot"},{"location":"setup/file/","text":"File \u00b6 File gateway listens to file system events and helps sensor trigger workloads. Event Structure \u00b6 The structure of an event dispatched by the gateway to the sensor looks like following, { \"context\" : { \"type\" : \"type_of_gateway\" , \"specVersion\" : \"cloud_events_version\" , \"source\" : \"name_of_the_gateway\" , \"eventID\" : \"unique_event_id\" , \"time\" : \"event_time\" , \"dataContentType\" : \"type_of_data\" , \"subject\" : \"name_of_the_event_within_event_source\" } , \"data\" : { \"name\" : \"Relative path to the file or directory\" , \"op\" : \"File operation that triggered the event\" // Create , Write , Remove , Rename , Chmod } } Setup \u00b6 Create the event source by running the following command. kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / event - sources / file . yaml The event source has configuration to listen to file system events for test-data directory and file called x.txt . Create the gateway by running the following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / gateways / file . yaml Create the sensor by running the following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / sensors / file . yaml Make sure there are no errors in either gateway or sensor pod. Log into the gateway pod by running following command, kubectl - n argo - events exec - it < file - gateway - pod - name > - c file - events -- / bin / bash Lets create a file called x.txt under test-data directory in gateway pod. cd test - data cat << EOF > x . txt hello EOF Once you create file x.txt , the sensor will trigger argo workflow. Run argo list to find the workflow. For real-world use cases, you should user PersistentVolume and PersistentVolumeClaim. Troubleshoot \u00b6 Please read the FAQ .","title":"File"},{"location":"setup/file/#file","text":"File gateway listens to file system events and helps sensor trigger workloads.","title":"File"},{"location":"setup/file/#event-structure","text":"The structure of an event dispatched by the gateway to the sensor looks like following, { \"context\" : { \"type\" : \"type_of_gateway\" , \"specVersion\" : \"cloud_events_version\" , \"source\" : \"name_of_the_gateway\" , \"eventID\" : \"unique_event_id\" , \"time\" : \"event_time\" , \"dataContentType\" : \"type_of_data\" , \"subject\" : \"name_of_the_event_within_event_source\" } , \"data\" : { \"name\" : \"Relative path to the file or directory\" , \"op\" : \"File operation that triggered the event\" // Create , Write , Remove , Rename , Chmod } }","title":"Event Structure"},{"location":"setup/file/#setup","text":"Create the event source by running the following command. kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / event - sources / file . yaml The event source has configuration to listen to file system events for test-data directory and file called x.txt . Create the gateway by running the following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / gateways / file . yaml Create the sensor by running the following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / sensors / file . yaml Make sure there are no errors in either gateway or sensor pod. Log into the gateway pod by running following command, kubectl - n argo - events exec - it < file - gateway - pod - name > - c file - events -- / bin / bash Lets create a file called x.txt under test-data directory in gateway pod. cd test - data cat << EOF > x . txt hello EOF Once you create file x.txt , the sensor will trigger argo workflow. Run argo list to find the workflow. For real-world use cases, you should user PersistentVolume and PersistentVolumeClaim.","title":"Setup"},{"location":"setup/file/#troubleshoot","text":"Please read the FAQ .","title":"Troubleshoot"},{"location":"setup/gcp-pub-sub/","text":"GCP PubSub \u00b6 GCP PubSub gateway subscribes to messages published by GCP publisher and helps sensor trigger workloads. Event Structure \u00b6 The structure of an event dispatched by the gateway to the sensor looks like following, { \"context\" : { \"type\" : \"type_of_gateway\" , \"specVersion\" : \"cloud_events_version\" , \"source\" : \"name_of_the_gateway\" , \"eventID\" : \"unique_event_id\" , \"time\" : \"event_time\" , \"dataContentType\" : \"type_of_data\" , \"subject\" : \"name_of_the_event_within_event_source\" } , \"data\" : { \"id\" : \"message id\" , // Attributes represents the key - value pairs the current message // is labelled with . \"attributes\" : \"key-values\" , \"publishTime\" : \"// The time at which the message was published\" , \"body\" : \"body refers to the message data\" , } } Setup \u00b6 Fetch the project credentials JSON file from GCP console. Create a K8s secret called gcp-credentials to store the credentials file apiVersion : v1 data : key . json : < YOUR_CREDENTIALS_STRING_FROM_JSON_FILE > kind : Secret metadata : name : gcp - credentials namespace : argo - events type : Opaque Create the gateway by running the following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / gateways / gcp - pubsub . yaml Create the event source by running the following command. kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / event - sources / gcp - pubsub . yaml Inspect the gateway pod logs to make sure the gateway was able to subscribe to the topic specified in the event source to consume messages. Create the sensor by running the following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / sensors / gcp - pubsub . yaml Publish a message from GCP PubSub console. Once a message is published, an argo workflow will be triggered. Run argo list to find the workflow. Troubleshoot \u00b6 Please read the FAQ .","title":"GCP PubSub"},{"location":"setup/gcp-pub-sub/#gcp-pubsub","text":"GCP PubSub gateway subscribes to messages published by GCP publisher and helps sensor trigger workloads.","title":"GCP PubSub"},{"location":"setup/gcp-pub-sub/#event-structure","text":"The structure of an event dispatched by the gateway to the sensor looks like following, { \"context\" : { \"type\" : \"type_of_gateway\" , \"specVersion\" : \"cloud_events_version\" , \"source\" : \"name_of_the_gateway\" , \"eventID\" : \"unique_event_id\" , \"time\" : \"event_time\" , \"dataContentType\" : \"type_of_data\" , \"subject\" : \"name_of_the_event_within_event_source\" } , \"data\" : { \"id\" : \"message id\" , // Attributes represents the key - value pairs the current message // is labelled with . \"attributes\" : \"key-values\" , \"publishTime\" : \"// The time at which the message was published\" , \"body\" : \"body refers to the message data\" , } }","title":"Event Structure"},{"location":"setup/gcp-pub-sub/#setup","text":"Fetch the project credentials JSON file from GCP console. Create a K8s secret called gcp-credentials to store the credentials file apiVersion : v1 data : key . json : < YOUR_CREDENTIALS_STRING_FROM_JSON_FILE > kind : Secret metadata : name : gcp - credentials namespace : argo - events type : Opaque Create the gateway by running the following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / gateways / gcp - pubsub . yaml Create the event source by running the following command. kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / event - sources / gcp - pubsub . yaml Inspect the gateway pod logs to make sure the gateway was able to subscribe to the topic specified in the event source to consume messages. Create the sensor by running the following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / sensors / gcp - pubsub . yaml Publish a message from GCP PubSub console. Once a message is published, an argo workflow will be triggered. Run argo list to find the workflow.","title":"Setup"},{"location":"setup/gcp-pub-sub/#troubleshoot","text":"Please read the FAQ .","title":"Troubleshoot"},{"location":"setup/github/","text":"GitHub \u00b6 GitHub gateway programatically configures webhooks for projects on GitHub and helps sensor trigger the workloads upon events. Event Structure \u00b6 The structure of an event dispatched by the gateway to the sensor looks like following, { \"context\" : { \"type\" : \"type_of_gateway\" , \"specVersion\" : \"cloud_events_version\" , \"source\" : \"name_of_the_gateway\" , \"eventID\" : \"unique_event_id\" , \"time\" : \"event_time\" , \"dataContentType\" : \"type_of_data\" , \"subject\" : \"name_of_the_event_within_event_source\" } , \"data\" : { \"body\" : \"Body is the github event data\" , \"headers\" : \"Headers from the Gitlab event\" , } } Setup \u00b6 Create an API token if you don't have one. Follow instructions to create a new GitHub API Token. Grant it the repo_hook permissions. Base64 encode your api token key, echo - n < api - token - key > | base64 Create a secret called github-access . apiVersion : v1 kind : Secret metadata : name : github - access type : Opaque data : access : < base64 - encoded - api - token - from - previous - step > Deploy the secret into K8s cluster kubectl - n argo - events apply - f github - access . yaml Create the gateway by running the following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / gateways / github . yaml Wait for gateway pod to get into the running state. Create an Ingress or Openshift Route for the gateway service to that it can be reached from GitHub. You can find more information on Ingress or Route online. Get the event source stored at https://raw.githubusercontent.com/argoproj/argo-events/master/examples/event-sources/github.yaml Change the url under webhook to your gateway service url created in a previous step. Make sure this url is reachable from GitHub. Create the event source by running the following command. kubectl apply - n argo - events - f < event - source - file - updated - in - previous - step > Go to Webhooks under your project settings on GitHub and verify the webhook is registered. You can also do the same by looking at the gateway pod logs. Create the sensor by running the following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / sensors / github . yaml Make a change to one of your project files and commit. It will trigger an argo workflow. Run argo list to find the workflow. Troubleshoot \u00b6 Please read the FAQ .","title":"GitHub"},{"location":"setup/github/#github","text":"GitHub gateway programatically configures webhooks for projects on GitHub and helps sensor trigger the workloads upon events.","title":"GitHub"},{"location":"setup/github/#event-structure","text":"The structure of an event dispatched by the gateway to the sensor looks like following, { \"context\" : { \"type\" : \"type_of_gateway\" , \"specVersion\" : \"cloud_events_version\" , \"source\" : \"name_of_the_gateway\" , \"eventID\" : \"unique_event_id\" , \"time\" : \"event_time\" , \"dataContentType\" : \"type_of_data\" , \"subject\" : \"name_of_the_event_within_event_source\" } , \"data\" : { \"body\" : \"Body is the github event data\" , \"headers\" : \"Headers from the Gitlab event\" , } }","title":"Event Structure"},{"location":"setup/github/#setup","text":"Create an API token if you don't have one. Follow instructions to create a new GitHub API Token. Grant it the repo_hook permissions. Base64 encode your api token key, echo - n < api - token - key > | base64 Create a secret called github-access . apiVersion : v1 kind : Secret metadata : name : github - access type : Opaque data : access : < base64 - encoded - api - token - from - previous - step > Deploy the secret into K8s cluster kubectl - n argo - events apply - f github - access . yaml Create the gateway by running the following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / gateways / github . yaml Wait for gateway pod to get into the running state. Create an Ingress or Openshift Route for the gateway service to that it can be reached from GitHub. You can find more information on Ingress or Route online. Get the event source stored at https://raw.githubusercontent.com/argoproj/argo-events/master/examples/event-sources/github.yaml Change the url under webhook to your gateway service url created in a previous step. Make sure this url is reachable from GitHub. Create the event source by running the following command. kubectl apply - n argo - events - f < event - source - file - updated - in - previous - step > Go to Webhooks under your project settings on GitHub and verify the webhook is registered. You can also do the same by looking at the gateway pod logs. Create the sensor by running the following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / sensors / github . yaml Make a change to one of your project files and commit. It will trigger an argo workflow. Run argo list to find the workflow.","title":"Setup"},{"location":"setup/github/#troubleshoot","text":"Please read the FAQ .","title":"Troubleshoot"},{"location":"setup/gitlab/","text":"GitLab \u00b6 GitLab gateway programatically configures webhooks for projects on GitLab and helps sensor trigger the workloads upon events. Event Structure \u00b6 The structure of an event dispatched by the gateway to the sensor looks like following, { \"context\" : { \"type\" : \"type_of_gateway\" , \"specVersion\" : \"cloud_events_version\" , \"source\" : \"name_of_the_gateway\" , \"eventID\" : \"unique_event_id\" , \"time\" : \"event_time\" , \"dataContentType\" : \"type_of_data\" , \"subject\" : \"name_of_the_event_within_event_source\" } , \"data\" : { \"body\" : \"Body is the gitlab event data\" , \"headers\" : \"Headers from the Gitlab event\" , } } Setup \u00b6 Create an API token if you don't have one. Follow instructions to create a new GitLab API Token. Grant it the api permissions. Base64 encode your api token key, echo - n < api - token - key > | base64 Create a secret called gitlab-access . apiVersion : v1 kind : Secret metadata : name : gitlab - access type : Opaque data : access : < base64 - encoded - api - token - from - previous - step > Deploy the secret into K8s cluster kubectl - n argo - events apply - f gitlab - access . yaml Create the gateway by running the following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / gateways / gitlab . yaml Wait for gateway pod to get into the running state. Create an Ingress or Openshift Route for the gateway service to that it can be reached from GitLab. You can find more information on Ingress or Route online. Get the event source stored at https://raw.githubusercontent.com/argoproj/argo-events/master/examples/event-sources/gitlab.yaml Change the url under webhook to your gateway service url created in a previous step. Make sure this url is reachable from GitLab. Create the event source by running the following command. kubectl apply - n argo - events - f < event - source - file - updated - in - previous - step > Go to Webhooks under your project settings on GitLab and verify the webhook is registered. You can also do the same by looking at the gateway pod logs. Create the sensor by running the following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / sensors / gitlab . yaml Make a change to one of your project files and commit. It will trigger an argo workflow. Run argo list to find the workflow. Troubleshoot \u00b6 Please read the FAQ .","title":"GitLab"},{"location":"setup/gitlab/#gitlab","text":"GitLab gateway programatically configures webhooks for projects on GitLab and helps sensor trigger the workloads upon events.","title":"GitLab"},{"location":"setup/gitlab/#event-structure","text":"The structure of an event dispatched by the gateway to the sensor looks like following, { \"context\" : { \"type\" : \"type_of_gateway\" , \"specVersion\" : \"cloud_events_version\" , \"source\" : \"name_of_the_gateway\" , \"eventID\" : \"unique_event_id\" , \"time\" : \"event_time\" , \"dataContentType\" : \"type_of_data\" , \"subject\" : \"name_of_the_event_within_event_source\" } , \"data\" : { \"body\" : \"Body is the gitlab event data\" , \"headers\" : \"Headers from the Gitlab event\" , } }","title":"Event Structure"},{"location":"setup/gitlab/#setup","text":"Create an API token if you don't have one. Follow instructions to create a new GitLab API Token. Grant it the api permissions. Base64 encode your api token key, echo - n < api - token - key > | base64 Create a secret called gitlab-access . apiVersion : v1 kind : Secret metadata : name : gitlab - access type : Opaque data : access : < base64 - encoded - api - token - from - previous - step > Deploy the secret into K8s cluster kubectl - n argo - events apply - f gitlab - access . yaml Create the gateway by running the following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / gateways / gitlab . yaml Wait for gateway pod to get into the running state. Create an Ingress or Openshift Route for the gateway service to that it can be reached from GitLab. You can find more information on Ingress or Route online. Get the event source stored at https://raw.githubusercontent.com/argoproj/argo-events/master/examples/event-sources/gitlab.yaml Change the url under webhook to your gateway service url created in a previous step. Make sure this url is reachable from GitLab. Create the event source by running the following command. kubectl apply - n argo - events - f < event - source - file - updated - in - previous - step > Go to Webhooks under your project settings on GitLab and verify the webhook is registered. You can also do the same by looking at the gateway pod logs. Create the sensor by running the following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / sensors / gitlab . yaml Make a change to one of your project files and commit. It will trigger an argo workflow. Run argo list to find the workflow.","title":"Setup"},{"location":"setup/gitlab/#troubleshoot","text":"Please read the FAQ .","title":"Troubleshoot"},{"location":"setup/kafka/","text":"Kafka \u00b6 Event Structure \u00b6 The structure of an event dispatched by the gateway to the sensor looks like following, { \"context\" : { \"type\" : \"type_of_gateway\" , \"specVersion\" : \"cloud_events_version\" , \"source\" : \"name_of_the_gateway\" , \"eventID\" : \"unique_event_id\" , \"time\" : \"event_time\" , \"dataContentType\" : \"type_of_data\" , \"subject\" : \"name_of_the_event_within_event_source\" } , \"data\" : { \"topic\" : \"kafka_topic\" , \"partition\" : \"partition_number\" , \"body\" : \"message_body\" , \"timestamp\" : \"timestamp_of_the_message\" } } Setup \u00b6 Make sure to setup the Kafka cluster in Kubernetes if you don't already have one. You can refer to https://github.com/Yolean/kubernetes-kafka for installation instructions. Create the event source by running the following command. Make sure to update the appropriate fields. kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / event - sources / kafka . yaml Create the gateway by running the following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / gateways / kafka . yaml Inspect the gateway pod logs to make sure the gateway was able to subscribe to the topic specified in the event source to consume messages. Create the sensor by running the following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / sensors / kafka . yaml Send message by using Kafka client. More info on how to send message at https://kafka.apache.org/quickstart Once a message is published, an argo workflow will be triggered. Run argo list to find the workflow. Troubleshoot \u00b6 Please read the FAQ .","title":"Kafka"},{"location":"setup/kafka/#kafka","text":"","title":"Kafka"},{"location":"setup/kafka/#event-structure","text":"The structure of an event dispatched by the gateway to the sensor looks like following, { \"context\" : { \"type\" : \"type_of_gateway\" , \"specVersion\" : \"cloud_events_version\" , \"source\" : \"name_of_the_gateway\" , \"eventID\" : \"unique_event_id\" , \"time\" : \"event_time\" , \"dataContentType\" : \"type_of_data\" , \"subject\" : \"name_of_the_event_within_event_source\" } , \"data\" : { \"topic\" : \"kafka_topic\" , \"partition\" : \"partition_number\" , \"body\" : \"message_body\" , \"timestamp\" : \"timestamp_of_the_message\" } }","title":"Event Structure"},{"location":"setup/kafka/#setup","text":"Make sure to setup the Kafka cluster in Kubernetes if you don't already have one. You can refer to https://github.com/Yolean/kubernetes-kafka for installation instructions. Create the event source by running the following command. Make sure to update the appropriate fields. kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / event - sources / kafka . yaml Create the gateway by running the following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / gateways / kafka . yaml Inspect the gateway pod logs to make sure the gateway was able to subscribe to the topic specified in the event source to consume messages. Create the sensor by running the following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / sensors / kafka . yaml Send message by using Kafka client. More info on how to send message at https://kafka.apache.org/quickstart Once a message is published, an argo workflow will be triggered. Run argo list to find the workflow.","title":"Setup"},{"location":"setup/kafka/#troubleshoot","text":"Please read the FAQ .","title":"Troubleshoot"},{"location":"setup/minio/","text":"Minio \u00b6 Minio gateway listens to minio bucket notifications and helps sensor trigger the workloads. Note : Minio gateway is exclusive for the Minio server. If you want to trigger workloads on AWS S3 bucket notification, please set up the AWS SNS gateway. Event Structure \u00b6 The structure of an event dispatched by the gateway to the sensor looks like following, { \"context\" : { \"type\" : \"type_of_gateway\" , \"specVersion\" : \"cloud_events_version\" , \"source\" : \"name_of_the_gateway\" , \"eventID\" : \"unique_event_id\" , \"time\" : \"event_time\" , \"dataContentType\" : \"type_of_data\" , \"subject\" : \"name_of_the_event_within_event_source\" } , \"data\" : { notification : [ { /* Minio notification. More info is available at https://docs.min.io/docs/minio-bucket-notification-guide.html } ] } } Setup \u00b6 Make sure to have minio server deployed and reachable from the gateway. More info on minio server setup is available at https://github.com/minio/minio/blob/master/docs/orchestration/kubernetes/k8s-yaml.md. If you are running Minio locally, make sure to port-forward to minio pod in order to make the service available outside local K8s cluster. kubectl - n argo - events port - forward < minio - pod - name > 9000 : 9000 Configure the minio client mc . mc config host add minio http : // localhost : 9000 minio minio123 Create a K8s secret that holds the access and secret key. This secret will be referred in the minio event source definition that we are going to install in a later step. apiVersion : v1 data : # base64 of minio accesskey : bWluaW8 = # base64 of minio123 secretkey : bWluaW8xMjM = kind : Secret metadata : name : artifacts - minio namespace : argo - events The event source we are going to use configures notifications for a bucket called input . mc mb minio / input If you inspect the gateway resource definition, you will notice that it refers to the event source minio-event-source . Lets install event source in the argo-events namespace, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / event - sources / minio . yaml Always make sure to first create a bucket on Minio and then refer it in event source. Install gateway in the argo-events namespace using following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / gateways / minio . yaml Once the gateway resource is created, the gateway controller will process it and create a pod. If you don't see the pod in argo-events namespace, check the gateway controller logs for errors. Check the gateway logs to make sure the gateway has processed the event source. Lets create the sensor, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / sensors / minio . yaml Create a file named and hello-world.txt and upload it onto to the input bucket. This will trigger the argo workflow. Run argo list to find the workflow. Troubleshoot \u00b6 Please read the FAQ .","title":"Minio"},{"location":"setup/minio/#minio","text":"Minio gateway listens to minio bucket notifications and helps sensor trigger the workloads. Note : Minio gateway is exclusive for the Minio server. If you want to trigger workloads on AWS S3 bucket notification, please set up the AWS SNS gateway.","title":"Minio"},{"location":"setup/minio/#event-structure","text":"The structure of an event dispatched by the gateway to the sensor looks like following, { \"context\" : { \"type\" : \"type_of_gateway\" , \"specVersion\" : \"cloud_events_version\" , \"source\" : \"name_of_the_gateway\" , \"eventID\" : \"unique_event_id\" , \"time\" : \"event_time\" , \"dataContentType\" : \"type_of_data\" , \"subject\" : \"name_of_the_event_within_event_source\" } , \"data\" : { notification : [ { /* Minio notification. More info is available at https://docs.min.io/docs/minio-bucket-notification-guide.html } ] } }","title":"Event Structure"},{"location":"setup/minio/#setup","text":"Make sure to have minio server deployed and reachable from the gateway. More info on minio server setup is available at https://github.com/minio/minio/blob/master/docs/orchestration/kubernetes/k8s-yaml.md. If you are running Minio locally, make sure to port-forward to minio pod in order to make the service available outside local K8s cluster. kubectl - n argo - events port - forward < minio - pod - name > 9000 : 9000 Configure the minio client mc . mc config host add minio http : // localhost : 9000 minio minio123 Create a K8s secret that holds the access and secret key. This secret will be referred in the minio event source definition that we are going to install in a later step. apiVersion : v1 data : # base64 of minio accesskey : bWluaW8 = # base64 of minio123 secretkey : bWluaW8xMjM = kind : Secret metadata : name : artifacts - minio namespace : argo - events The event source we are going to use configures notifications for a bucket called input . mc mb minio / input If you inspect the gateway resource definition, you will notice that it refers to the event source minio-event-source . Lets install event source in the argo-events namespace, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / event - sources / minio . yaml Always make sure to first create a bucket on Minio and then refer it in event source. Install gateway in the argo-events namespace using following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / gateways / minio . yaml Once the gateway resource is created, the gateway controller will process it and create a pod. If you don't see the pod in argo-events namespace, check the gateway controller logs for errors. Check the gateway logs to make sure the gateway has processed the event source. Lets create the sensor, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / sensors / minio . yaml Create a file named and hello-world.txt and upload it onto to the input bucket. This will trigger the argo workflow. Run argo list to find the workflow.","title":"Setup"},{"location":"setup/minio/#troubleshoot","text":"Please read the FAQ .","title":"Troubleshoot"},{"location":"setup/mqtt/","text":"MQTT \u00b6 MQTT gateway listens to messages on from IoT devices and helps sensor trigger the workloads. Event Structure \u00b6 The structure of an event dispatched by the gateway to the sensor looks like following, { \" context \" : { \" type \" : \" type_of_gateway \" , \" specVersion \" : \" cloud_events_version \" , \" source \" : \" name_of_the_gateway \" , \" eventID \" : \" unique_event_id \" , \" time \" : \" event_time \" , \" dataContentType \" : \" type_of_data \" , \" subject \" : \" name_of_the_event_within_event_source \" }, \" data \" : { \" topic \" : \" Topic refers to the MQTT topic name \" , \" messageId \" : \" MessageId is the unique ID for the message \" , \" body \" : \" Body is the message payload \" } } Setup \u00b6 Make sure to setup the MQTT Broker and Bridge in Kubernetes if you don't already have one. Create the event source by running the following command. Make sure to update the appropriate fields. kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / event - sources / mqtt . yaml Create the gateway by running the following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / gateways / mqtt . yaml Inspect the gateway pod logs to make sure the gateway was able to subscribe to the topic specified in the event source to consume messages. Create the sensor by running the following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / sensors / mqtt . yaml Send message by using MQTT client. Once a message is published, an argo workflow will be triggered. Run argo list to find the workflow. Troubleshoot \u00b6 Please read the FAQ .","title":"MQTT"},{"location":"setup/mqtt/#mqtt","text":"MQTT gateway listens to messages on from IoT devices and helps sensor trigger the workloads.","title":"MQTT"},{"location":"setup/mqtt/#event-structure","text":"The structure of an event dispatched by the gateway to the sensor looks like following, { \" context \" : { \" type \" : \" type_of_gateway \" , \" specVersion \" : \" cloud_events_version \" , \" source \" : \" name_of_the_gateway \" , \" eventID \" : \" unique_event_id \" , \" time \" : \" event_time \" , \" dataContentType \" : \" type_of_data \" , \" subject \" : \" name_of_the_event_within_event_source \" }, \" data \" : { \" topic \" : \" Topic refers to the MQTT topic name \" , \" messageId \" : \" MessageId is the unique ID for the message \" , \" body \" : \" Body is the message payload \" } }","title":"Event Structure"},{"location":"setup/mqtt/#setup","text":"Make sure to setup the MQTT Broker and Bridge in Kubernetes if you don't already have one. Create the event source by running the following command. Make sure to update the appropriate fields. kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / event - sources / mqtt . yaml Create the gateway by running the following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / gateways / mqtt . yaml Inspect the gateway pod logs to make sure the gateway was able to subscribe to the topic specified in the event source to consume messages. Create the sensor by running the following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / sensors / mqtt . yaml Send message by using MQTT client. Once a message is published, an argo workflow will be triggered. Run argo list to find the workflow.","title":"Setup"},{"location":"setup/mqtt/#troubleshoot","text":"Please read the FAQ .","title":"Troubleshoot"},{"location":"setup/nats/","text":"NATS \u00b6 NATS gateway listens to NATS subject notifications and helps sensor trigger the workloads. Event Structure \u00b6 The structure of an event dispatched by the gateway to the sensor looks like following, { \"context\" : { \"type\" : \"type_of_gateway\" , \"specVersion\" : \"cloud_events_version\" , \"source\" : \"name_of_the_gateway\" , \"eventID\" : \"unique_event_id\" , \"time\" : \"event_time\" , \"dataContentType\" : \"type_of_data\" , \"subject\" : \"name_of_the_event_within_event_source\" } , \"data\" : { \"subject\" : \"name_of_the_nats_subject\" , \"body\" : \"message_payload\" } } Setup \u00b6 Make sure to have NATS cluster deployed in the Kubernetes. If you don't have one already installed, please refer https://github.com/nats-io/nats-operator for details. NATS cluster setup for test purposes, apiVersion : v1 kind : Service metadata : name : nats namespace : argo - events labels : component : nats spec : selector : component : nats type : ClusterIP ports : - name : client port : 4222 - name : cluster port : 6222 - name : monitor port : 8222 --- apiVersion : apps / v1beta1 kind : StatefulSet metadata : name : nats namespace : argo - events labels : component : nats spec : serviceName : nats replicas : 1 template : metadata : labels : component : nats spec : serviceAccountName : argo - events - sa containers : - name : nats image : nats : latest ports : - containerPort : 4222 name : client - containerPort : 6222 name : cluster - containerPort : 8222 name : monitor livenessProbe : httpGet : path : / port : 8222 initialDelaySeconds : 10 timeoutSeconds : 5 Create the event source by running the following command. kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / event - sources / nats . yaml Create the gateway by running the following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / gateways / nats . yaml Inspect the gateway pod logs to make sure the gateway was able to subscribe to the subject specified in the event source to consume messages. Create the sensor by running the following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / sensors / nats . yaml If you are running NATS on local K8s cluster, make sure to port-forward to pod, kubectl - n argo - events port - forward < nats - pod - name > 4222 : 4222 Publish a message for the subject specified in the event source. Refer the nats example to publish a message to the subject https://github.com/nats-io/go-nats-examples/tree/master/patterns/publish-subscribe. go run main . go - s localhost foo '{\"message\": \"hello\"}' Once a message is published, an argo workflow will be triggered. Run argo list to find the workflow. Troubleshoot \u00b6 Please read the FAQ .","title":"NATS"},{"location":"setup/nats/#nats","text":"NATS gateway listens to NATS subject notifications and helps sensor trigger the workloads.","title":"NATS"},{"location":"setup/nats/#event-structure","text":"The structure of an event dispatched by the gateway to the sensor looks like following, { \"context\" : { \"type\" : \"type_of_gateway\" , \"specVersion\" : \"cloud_events_version\" , \"source\" : \"name_of_the_gateway\" , \"eventID\" : \"unique_event_id\" , \"time\" : \"event_time\" , \"dataContentType\" : \"type_of_data\" , \"subject\" : \"name_of_the_event_within_event_source\" } , \"data\" : { \"subject\" : \"name_of_the_nats_subject\" , \"body\" : \"message_payload\" } }","title":"Event Structure"},{"location":"setup/nats/#setup","text":"Make sure to have NATS cluster deployed in the Kubernetes. If you don't have one already installed, please refer https://github.com/nats-io/nats-operator for details. NATS cluster setup for test purposes, apiVersion : v1 kind : Service metadata : name : nats namespace : argo - events labels : component : nats spec : selector : component : nats type : ClusterIP ports : - name : client port : 4222 - name : cluster port : 6222 - name : monitor port : 8222 --- apiVersion : apps / v1beta1 kind : StatefulSet metadata : name : nats namespace : argo - events labels : component : nats spec : serviceName : nats replicas : 1 template : metadata : labels : component : nats spec : serviceAccountName : argo - events - sa containers : - name : nats image : nats : latest ports : - containerPort : 4222 name : client - containerPort : 6222 name : cluster - containerPort : 8222 name : monitor livenessProbe : httpGet : path : / port : 8222 initialDelaySeconds : 10 timeoutSeconds : 5 Create the event source by running the following command. kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / event - sources / nats . yaml Create the gateway by running the following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / gateways / nats . yaml Inspect the gateway pod logs to make sure the gateway was able to subscribe to the subject specified in the event source to consume messages. Create the sensor by running the following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / sensors / nats . yaml If you are running NATS on local K8s cluster, make sure to port-forward to pod, kubectl - n argo - events port - forward < nats - pod - name > 4222 : 4222 Publish a message for the subject specified in the event source. Refer the nats example to publish a message to the subject https://github.com/nats-io/go-nats-examples/tree/master/patterns/publish-subscribe. go run main . go - s localhost foo '{\"message\": \"hello\"}' Once a message is published, an argo workflow will be triggered. Run argo list to find the workflow.","title":"Setup"},{"location":"setup/nats/#troubleshoot","text":"Please read the FAQ .","title":"Troubleshoot"},{"location":"setup/nsq/","text":"NSQ \u00b6 NSQ gateway subscribes to nsq pub/sub notifications and helps sensor trigger the workloads. Event Structure \u00b6 The structure of an event dispatched by the gateway to the sensor looks like following, { \"context\" : { \"type\" : \"type_of_gateway\" , \"specVersion\" : \"cloud_events_version\" , \"source\" : \"name_of_the_gateway\" , \"eventID\" : \"unique_event_id\" , \"time\" : \"event_time\" , \"dataContentType\" : \"type_of_data\" , \"subject\" : \"name_of_the_event_within_event_source\" } , \"data\" : { \"body\" : \"Body is the message data\" , \"timestamp\" : \"timestamp of the message\" , \"nsqdAddress\" : \"NSQDAddress is the address of the nsq host\" } } Setup \u00b6 Deploy NSQ on local K8s cluster apiVersion : v1 kind : Service metadata : name : nsqlookupd labels : app : nsq spec : ports : - port : 4160 targetPort : 4160 name : tcp - port : 4161 targetPort : 4161 name : http clusterIP : None selector : app : nsq component : nsqlookupd --- apiVersion : v1 kind : Service metadata : name : nsqd labels : app : nsq spec : ports : - port : 4150 targetPort : 4150 name : tcp - port : 4151 targetPort : 4151 name : http clusterIP : None selector : app : nsq component : nsqd --- apiVersion : v1 kind : Service metadata : name : nsqadmin labels : app : nsq spec : ports : - port : 4170 targetPort : 4170 name : tcp - port : 4171 targetPort : 4171 name : http selector : app : nsq component : nsqadmin --- apiVersion : apps / v1beta1 kind : StatefulSet metadata : name : nsqlookupd spec : serviceName : \"nsqlookupd\" replicas : 1 updateStrategy : type : RollingUpdate template : metadata : labels : app : nsq component : nsqlookupd spec : containers : - name : nsqlookupd image : nsqio / nsq : v1 . 1.0 imagePullPolicy : Always resources : requests : cpu : 30 m memory : 64 Mi ports : - containerPort : 4160 name : tcp - containerPort : 4161 name : http livenessProbe : httpGet : path : / ping port : http initialDelaySeconds : 5 readinessProbe : httpGet : path : / ping port : http initialDelaySeconds : 2 command : - / nsqlookupd terminationGracePeriodSeconds : 5 --- apiVersion : apps / v1beta1 kind : Deployment metadata : name : nsqd spec : replicas : 1 selector : matchLabels : app : nsq component : nsqd template : metadata : labels : app : nsq component : nsqd spec : containers : - name : nsqd image : nsqio / nsq : v1 . 1.0 imagePullPolicy : Always resources : requests : cpu : 30 m memory : 64 Mi ports : - containerPort : 4150 name : tcp - containerPort : 4151 name : http livenessProbe : httpGet : path : / ping port : http initialDelaySeconds : 5 readinessProbe : httpGet : path : / ping port : http initialDelaySeconds : 2 volumeMounts : - name : datadir mountPath : / data command : - / nsqd - - data - path - / data - - lookupd - tcp - address - nsqlookupd . argo - events . svc : 4160 - - broadcast - address - nsqd . argo - events . svc env : - name : HOSTNAME valueFrom : fieldRef : fieldPath : metadata . name terminationGracePeriodSeconds : 5 volumes : - name : datadir emptyDir : {} --- apiVersion : extensions / v1beta1 kind : Deployment metadata : name : nsqadmin spec : replicas : 1 template : metadata : labels : app : nsq component : nsqadmin spec : containers : - name : nsqadmin image : nsqio / nsq : v1 . 1.0 imagePullPolicy : Always resources : requests : cpu : 30 m memory : 64 Mi ports : - containerPort : 4170 name : tcp - containerPort : 4171 name : http livenessProbe : httpGet : path : / ping port : http initialDelaySeconds : 10 readinessProbe : httpGet : path : / ping port : http initialDelaySeconds : 5 command : - / nsqadmin - - lookupd - http - address - nsqlookupd . argo - events . svc : 4161 terminationGracePeriodSeconds : 5 Expose NSQD by kubectl port-forward , kubectl - n argo - events port - forward service / nsqd 4151 : 4151 Create topic hello and channel my-channel curl - X POST 'http://localhost:4151/topic/create?topic=hello' curl - X POST 'http://localhost:4151/channel/create?topic=hello&channel=my-channel' Create the event source by running the following command. kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / event - sources / nsq . yaml Create the gateway by running the following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / gateways / nsq . yaml Inspect the gateway pod logs to make sure the gateway was able to subscribe to the channel specified in the event source to consume messages. Create the sensor by running the following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / sensors / nsq . yaml Publish a message on topic hello and channel my-channel , curl - d '{\"message\": \"hello\"}' 'http://localhost:4151/pub?topic=hello&channel=my-channel' Once a message is published, an argo workflow will be triggered. Run argo list to find the workflow. Troubleshoot \u00b6 Please read the FAQ .","title":"NSQ"},{"location":"setup/nsq/#nsq","text":"NSQ gateway subscribes to nsq pub/sub notifications and helps sensor trigger the workloads.","title":"NSQ"},{"location":"setup/nsq/#event-structure","text":"The structure of an event dispatched by the gateway to the sensor looks like following, { \"context\" : { \"type\" : \"type_of_gateway\" , \"specVersion\" : \"cloud_events_version\" , \"source\" : \"name_of_the_gateway\" , \"eventID\" : \"unique_event_id\" , \"time\" : \"event_time\" , \"dataContentType\" : \"type_of_data\" , \"subject\" : \"name_of_the_event_within_event_source\" } , \"data\" : { \"body\" : \"Body is the message data\" , \"timestamp\" : \"timestamp of the message\" , \"nsqdAddress\" : \"NSQDAddress is the address of the nsq host\" } }","title":"Event Structure"},{"location":"setup/nsq/#setup","text":"Deploy NSQ on local K8s cluster apiVersion : v1 kind : Service metadata : name : nsqlookupd labels : app : nsq spec : ports : - port : 4160 targetPort : 4160 name : tcp - port : 4161 targetPort : 4161 name : http clusterIP : None selector : app : nsq component : nsqlookupd --- apiVersion : v1 kind : Service metadata : name : nsqd labels : app : nsq spec : ports : - port : 4150 targetPort : 4150 name : tcp - port : 4151 targetPort : 4151 name : http clusterIP : None selector : app : nsq component : nsqd --- apiVersion : v1 kind : Service metadata : name : nsqadmin labels : app : nsq spec : ports : - port : 4170 targetPort : 4170 name : tcp - port : 4171 targetPort : 4171 name : http selector : app : nsq component : nsqadmin --- apiVersion : apps / v1beta1 kind : StatefulSet metadata : name : nsqlookupd spec : serviceName : \"nsqlookupd\" replicas : 1 updateStrategy : type : RollingUpdate template : metadata : labels : app : nsq component : nsqlookupd spec : containers : - name : nsqlookupd image : nsqio / nsq : v1 . 1.0 imagePullPolicy : Always resources : requests : cpu : 30 m memory : 64 Mi ports : - containerPort : 4160 name : tcp - containerPort : 4161 name : http livenessProbe : httpGet : path : / ping port : http initialDelaySeconds : 5 readinessProbe : httpGet : path : / ping port : http initialDelaySeconds : 2 command : - / nsqlookupd terminationGracePeriodSeconds : 5 --- apiVersion : apps / v1beta1 kind : Deployment metadata : name : nsqd spec : replicas : 1 selector : matchLabels : app : nsq component : nsqd template : metadata : labels : app : nsq component : nsqd spec : containers : - name : nsqd image : nsqio / nsq : v1 . 1.0 imagePullPolicy : Always resources : requests : cpu : 30 m memory : 64 Mi ports : - containerPort : 4150 name : tcp - containerPort : 4151 name : http livenessProbe : httpGet : path : / ping port : http initialDelaySeconds : 5 readinessProbe : httpGet : path : / ping port : http initialDelaySeconds : 2 volumeMounts : - name : datadir mountPath : / data command : - / nsqd - - data - path - / data - - lookupd - tcp - address - nsqlookupd . argo - events . svc : 4160 - - broadcast - address - nsqd . argo - events . svc env : - name : HOSTNAME valueFrom : fieldRef : fieldPath : metadata . name terminationGracePeriodSeconds : 5 volumes : - name : datadir emptyDir : {} --- apiVersion : extensions / v1beta1 kind : Deployment metadata : name : nsqadmin spec : replicas : 1 template : metadata : labels : app : nsq component : nsqadmin spec : containers : - name : nsqadmin image : nsqio / nsq : v1 . 1.0 imagePullPolicy : Always resources : requests : cpu : 30 m memory : 64 Mi ports : - containerPort : 4170 name : tcp - containerPort : 4171 name : http livenessProbe : httpGet : path : / ping port : http initialDelaySeconds : 10 readinessProbe : httpGet : path : / ping port : http initialDelaySeconds : 5 command : - / nsqadmin - - lookupd - http - address - nsqlookupd . argo - events . svc : 4161 terminationGracePeriodSeconds : 5 Expose NSQD by kubectl port-forward , kubectl - n argo - events port - forward service / nsqd 4151 : 4151 Create topic hello and channel my-channel curl - X POST 'http://localhost:4151/topic/create?topic=hello' curl - X POST 'http://localhost:4151/channel/create?topic=hello&channel=my-channel' Create the event source by running the following command. kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / event - sources / nsq . yaml Create the gateway by running the following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / gateways / nsq . yaml Inspect the gateway pod logs to make sure the gateway was able to subscribe to the channel specified in the event source to consume messages. Create the sensor by running the following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / sensors / nsq . yaml Publish a message on topic hello and channel my-channel , curl - d '{\"message\": \"hello\"}' 'http://localhost:4151/pub?topic=hello&channel=my-channel' Once a message is published, an argo workflow will be triggered. Run argo list to find the workflow.","title":"Setup"},{"location":"setup/nsq/#troubleshoot","text":"Please read the FAQ .","title":"Troubleshoot"},{"location":"setup/redis/","text":"Redis \u00b6 Redis gateway subscribes to Redis publisher and helps sensor trigger workloads. Event Structure \u00b6 The structure of an event dispatched by the gateway to the sensor looks like following, { \"context\" : { \"type\" : \"type_of_gateway\" , \"specVersion\" : \"cloud_events_version\" , \"source\" : \"name_of_the_gateway\" , \"eventID\" : \"unique_event_id\" , \"time\" : \"event_time\" , \"dataContentType\" : \"type_of_data\" , \"subject\" : \"name_of_the_event_within_event_source\" } , \"data\" : { \"channel\" : \"Subscription channel\" , \"pattern\" : \"Message pattern\" , \"body\" : \"message body\" // string } } Setup \u00b6 Follow the documentation to set up Redis database. Create the event source by running the following command. kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / event - sources / redis . yaml Create the gateway by running the following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / gateways / redis . yaml Inspect the gateway pod logs to make sure the gateway was able to subscribe to the channel specified in the event source to consume messages. Create the sensor by running the following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / sensors / redis . yaml Log into redis pod using kubectl , kubectl - n argo - events exec - it < redis - pod - name > - c < redis - container - name > -- / bin / bash Run redis-cli and publish a message on FOO channel. PUBLISH FOO hello Once a message is published, an argo workflow will be triggered. Run argo list to find the workflow. Troubleshoot \u00b6 Please read the FAQ .","title":"Redis"},{"location":"setup/redis/#redis","text":"Redis gateway subscribes to Redis publisher and helps sensor trigger workloads.","title":"Redis"},{"location":"setup/redis/#event-structure","text":"The structure of an event dispatched by the gateway to the sensor looks like following, { \"context\" : { \"type\" : \"type_of_gateway\" , \"specVersion\" : \"cloud_events_version\" , \"source\" : \"name_of_the_gateway\" , \"eventID\" : \"unique_event_id\" , \"time\" : \"event_time\" , \"dataContentType\" : \"type_of_data\" , \"subject\" : \"name_of_the_event_within_event_source\" } , \"data\" : { \"channel\" : \"Subscription channel\" , \"pattern\" : \"Message pattern\" , \"body\" : \"message body\" // string } }","title":"Event Structure"},{"location":"setup/redis/#setup","text":"Follow the documentation to set up Redis database. Create the event source by running the following command. kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / event - sources / redis . yaml Create the gateway by running the following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / gateways / redis . yaml Inspect the gateway pod logs to make sure the gateway was able to subscribe to the channel specified in the event source to consume messages. Create the sensor by running the following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / sensors / redis . yaml Log into redis pod using kubectl , kubectl - n argo - events exec - it < redis - pod - name > - c < redis - container - name > -- / bin / bash Run redis-cli and publish a message on FOO channel. PUBLISH FOO hello Once a message is published, an argo workflow will be triggered. Run argo list to find the workflow.","title":"Setup"},{"location":"setup/redis/#troubleshoot","text":"Please read the FAQ .","title":"Troubleshoot"},{"location":"setup/resource/","text":"Resource \u00b6 Resource gateway watches change notifications for K8s object and helps sensor trigger the workloads. Event Structure \u00b6 The structure of an event dispatched by the gateway to the sensor looks like following, { \"context\" : { \"type\" : \"type_of_gateway\" , \"specVersion\" : \"cloud_events_version\" , \"source\" : \"name_of_the_gateway\" , \"eventID\" : \"unique_event_id\" , \"time\" : \"event_time\" , \"dataContentType\" : \"type_of_data\" , \"subject\" : \"name_of_the_event_within_event_source\" } , \"data\" : { \"type\" : \"type_of_the_event\" , // ADD , UPDATE or DELETE \"body\" : \"resource_body\" , // JSON format \"group\" : \"resource_group_name\" , \"version\" : \"resource_version_name\" , \"resource\" : \"resource_name\" } } Setup \u00b6 Create the event source by running the following command. kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / event - sources / resource . yaml Create the gateway by running the following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / gateways / resource . yaml Create the sensor by running the following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / sensors / resource . yaml The event source we created in step 1 contains configuration which makes the gateway listen to Argo workflows marked with label workflows.argoproj.io/phase: Succeeded and app: my-workflow . Lets create a workflow called my-workflow with label app: my-workflow , so that when it transitions into success state, the gateway will get a notification. apiVersion : argoproj . io / v1alpha1 kind : Workflow metadata : name : my - workflow labels : app : my - workflow spec : entrypoint : whalesay templates : - name : whalesay container : image : docker / whalesay : latest command : [ cowsay ] args : [ \"hello world\" ] Create my-wokflow and wait for the workflow to complete. kubectl - n argo - events create my - worflow . yaml Once the my-workflow completes, the sensor will trigger the workflow. Run argo list to list the triggered workflow. Troubleshoot \u00b6 Please read the FAQ .","title":"Resource"},{"location":"setup/resource/#resource","text":"Resource gateway watches change notifications for K8s object and helps sensor trigger the workloads.","title":"Resource"},{"location":"setup/resource/#event-structure","text":"The structure of an event dispatched by the gateway to the sensor looks like following, { \"context\" : { \"type\" : \"type_of_gateway\" , \"specVersion\" : \"cloud_events_version\" , \"source\" : \"name_of_the_gateway\" , \"eventID\" : \"unique_event_id\" , \"time\" : \"event_time\" , \"dataContentType\" : \"type_of_data\" , \"subject\" : \"name_of_the_event_within_event_source\" } , \"data\" : { \"type\" : \"type_of_the_event\" , // ADD , UPDATE or DELETE \"body\" : \"resource_body\" , // JSON format \"group\" : \"resource_group_name\" , \"version\" : \"resource_version_name\" , \"resource\" : \"resource_name\" } }","title":"Event Structure"},{"location":"setup/resource/#setup","text":"Create the event source by running the following command. kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / event - sources / resource . yaml Create the gateway by running the following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / gateways / resource . yaml Create the sensor by running the following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / sensors / resource . yaml The event source we created in step 1 contains configuration which makes the gateway listen to Argo workflows marked with label workflows.argoproj.io/phase: Succeeded and app: my-workflow . Lets create a workflow called my-workflow with label app: my-workflow , so that when it transitions into success state, the gateway will get a notification. apiVersion : argoproj . io / v1alpha1 kind : Workflow metadata : name : my - workflow labels : app : my - workflow spec : entrypoint : whalesay templates : - name : whalesay container : image : docker / whalesay : latest command : [ cowsay ] args : [ \"hello world\" ] Create my-wokflow and wait for the workflow to complete. kubectl - n argo - events create my - worflow . yaml Once the my-workflow completes, the sensor will trigger the workflow. Run argo list to list the triggered workflow.","title":"Setup"},{"location":"setup/resource/#troubleshoot","text":"Please read the FAQ .","title":"Troubleshoot"},{"location":"setup/webhook/","text":"Webhook \u00b6 Webhook gateway exposes a http server and allows external entities to trigger workloads via http requests. Event Structure \u00b6 The structure of an event dispatched by the gateway to the sensor looks like following, { \"context\" : { \"type\" : \"type_of_gateway\" , \"specVersion\" : \"cloud_events_version\" , \"source\" : \"name_of_the_gateway\" , \"eventID\" : \"unique_event_id\" , \"time\" : \"event_time\" , \"dataContentType\" : \"type_of_data\" , \"subject\" : \"name_of_the_event_within_event_source\" } , \"data\" : { \"header\" : { /* the headers from the request received by the gateway from the external entity */ } , \"body\" : { /* the payload of the request received by the gateway from the external entity */ } , } } Setup \u00b6 Install gateway in the argo-events namespace using following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / gateways / webhook . yaml Once the gateway resource is created, the gateway controller will process it and create a pod and a service. If you don't see the pod and service in argo-events namespace, check the gateway controller logs for errors. If you inspect the gateway resource definition, you will notice it points to the event source called webhook-event-source . Lets install event source in the argo-events namespace, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / event - sources / webhook . yaml Check the gateway logs to make sure the gateway has processed the event source. The gateway is now listening for HTTP requests on port 12000 and endpoint /example . Its time to create the sensor, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / sensors / webhook . yaml Once the sensor pod is in running state, test the setup by sending a POST request to gateway service. Troubleshoot \u00b6 Please read the FAQ .","title":"Webhook"},{"location":"setup/webhook/#webhook","text":"Webhook gateway exposes a http server and allows external entities to trigger workloads via http requests.","title":"Webhook"},{"location":"setup/webhook/#event-structure","text":"The structure of an event dispatched by the gateway to the sensor looks like following, { \"context\" : { \"type\" : \"type_of_gateway\" , \"specVersion\" : \"cloud_events_version\" , \"source\" : \"name_of_the_gateway\" , \"eventID\" : \"unique_event_id\" , \"time\" : \"event_time\" , \"dataContentType\" : \"type_of_data\" , \"subject\" : \"name_of_the_event_within_event_source\" } , \"data\" : { \"header\" : { /* the headers from the request received by the gateway from the external entity */ } , \"body\" : { /* the payload of the request received by the gateway from the external entity */ } , } }","title":"Event Structure"},{"location":"setup/webhook/#setup","text":"Install gateway in the argo-events namespace using following command, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / gateways / webhook . yaml Once the gateway resource is created, the gateway controller will process it and create a pod and a service. If you don't see the pod and service in argo-events namespace, check the gateway controller logs for errors. If you inspect the gateway resource definition, you will notice it points to the event source called webhook-event-source . Lets install event source in the argo-events namespace, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / event - sources / webhook . yaml Check the gateway logs to make sure the gateway has processed the event source. The gateway is now listening for HTTP requests on port 12000 and endpoint /example . Its time to create the sensor, kubectl apply - n argo - events - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / sensors / webhook . yaml Once the sensor pod is in running state, test the setup by sending a POST request to gateway service.","title":"Setup"},{"location":"setup/webhook/#troubleshoot","text":"Please read the FAQ .","title":"Troubleshoot"},{"location":"tutorials/01-introduction/","text":"Introduction \u00b6 In the tutorials, we will cover every aspect of Argo Events and demonstrate how you can leverage these features to build an event driven workflow pipeline. All the concepts you will learn in this tutorial and subsequent ones can be applied to any type of gateway. Prerequisites \u00b6 Follow the installation guide to set up the Argo Events. Make sure to configure Argo Workflow controller to listen to workflow objects created in argo-events namespace. Make sure to read the concepts behind gateway , sensor , event source . Get Started \u00b6 Lets set up a basic webhook gateway and sensor that listens to events over HTTP and executes an Argo workflow. Create the webhook event source. kubectl - n argo - events apply - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / event - sources / webhook . yaml Create the webhook gateway. kubectl - n argo - events apply - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / gateways / webhook . yaml Create the webhook sensor. kubectl - n argo - events apply - f https : // github . com / argoproj / argo - events / tree / master / examples / sensors / webhook . yaml If the commands are executed successfully, the gateway and sensor pods will get created. You will also notice that a service is created for both the gateway and sensor. Expose the gateway pod via Ingress, OpenShift Route or port forward to consume requests over HTTP. kubectl - n argo - events port - forward < gateway - pod - name > 12000 : 12000 Use either Curl or Postman to send a post request to the http://localhost:12000/example curl - d '{\"message\":\"this is my first webhook\"}' - H \"Content-Type: application/json\" - X POST http : // localhost : 12000 / example Now, you should see an Argo workflow being created. kubectl - n argo - events get wf Make sure the workflow pod ran successfully. _________________________________________ / { \"context\" : { \"type\" : \"webhook\" , \"specVersi \\ | on\" : \"0.3\" , \"source\" : \"webhook-gateway\" , \"e | | ventID\" : \"38376665363064642d343336352d34 | | 3035372d393766662d366234326130656232343 | | 337\" , \"time\" : \"2020-01-11T16:55:42.996636 | | Z\" , \"dataContentType\" : \"application/json\" | | , \"subject\" : \"example\" } , \"data\" : \"eyJoZWFkZ | | XIiOnsiQWNjZXB0IjpbIiovKiJdLCJDb250ZW50 | | LUxlbmd0aCI6WyIzOCJdLCJDb250ZW50LVR5cGU | | iOlsiYXBwbGljYXRpb24vanNvbiJdLCJVc2VyLU | | FnZW50IjpbImN1cmwvNy41NC4wIl19LCJib2R5I | | jp7Im1lc3NhZ2UiOiJ0aGlzIGlzIG15IGZpcnN0 | \\ IHdlYmhvb2sifX0=\" } / ----------------------------------------- \\ \\ \\ ## . ## ## ## == ## ## ## ## === / \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" ___ / === ~~~ { ~~ ~~~~ ~~~ ~~~~ ~~ ~ / ===- ~~~ \\ ______ o __ / \\ \\ __ / \\ ____ \\ ______ / Note: You will see the message printed in the workflow logs contains both the event context and data, with data being base64 encoded. In later sections, we will see how to extract particular key-value from event context or data and pass it to the workflow as arguments. Troubleshoot \u00b6 If you don't see the gateway and sensor pod in argo-events namespace, Make sure the correct Role and RoleBindings are applied to the service account and there are no errors in both gateway and sensor controller. Make sure gateway and sensor controller configmap has namespace set to argo-events . Check the logs of gateway and sensor controller. Make sure the controllers have processed the gateway and sensor objects and there are no errors. Look for any error in gateway or sensor pod. Inspect the gateway, kubectl - n argo - event gateway - object - name - o yaml Inspect the sensor, kubectl - n argo - events sensor - object - name - o yaml and look for any errors within the Status . Raise an issue on GitHub or post a question on argo-events slack channel.","title":"Introduction"},{"location":"tutorials/01-introduction/#introduction","text":"In the tutorials, we will cover every aspect of Argo Events and demonstrate how you can leverage these features to build an event driven workflow pipeline. All the concepts you will learn in this tutorial and subsequent ones can be applied to any type of gateway.","title":"Introduction"},{"location":"tutorials/01-introduction/#prerequisites","text":"Follow the installation guide to set up the Argo Events. Make sure to configure Argo Workflow controller to listen to workflow objects created in argo-events namespace. Make sure to read the concepts behind gateway , sensor , event source .","title":"Prerequisites"},{"location":"tutorials/01-introduction/#get-started","text":"Lets set up a basic webhook gateway and sensor that listens to events over HTTP and executes an Argo workflow. Create the webhook event source. kubectl - n argo - events apply - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / event - sources / webhook . yaml Create the webhook gateway. kubectl - n argo - events apply - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / gateways / webhook . yaml Create the webhook sensor. kubectl - n argo - events apply - f https : // github . com / argoproj / argo - events / tree / master / examples / sensors / webhook . yaml If the commands are executed successfully, the gateway and sensor pods will get created. You will also notice that a service is created for both the gateway and sensor. Expose the gateway pod via Ingress, OpenShift Route or port forward to consume requests over HTTP. kubectl - n argo - events port - forward < gateway - pod - name > 12000 : 12000 Use either Curl or Postman to send a post request to the http://localhost:12000/example curl - d '{\"message\":\"this is my first webhook\"}' - H \"Content-Type: application/json\" - X POST http : // localhost : 12000 / example Now, you should see an Argo workflow being created. kubectl - n argo - events get wf Make sure the workflow pod ran successfully. _________________________________________ / { \"context\" : { \"type\" : \"webhook\" , \"specVersi \\ | on\" : \"0.3\" , \"source\" : \"webhook-gateway\" , \"e | | ventID\" : \"38376665363064642d343336352d34 | | 3035372d393766662d366234326130656232343 | | 337\" , \"time\" : \"2020-01-11T16:55:42.996636 | | Z\" , \"dataContentType\" : \"application/json\" | | , \"subject\" : \"example\" } , \"data\" : \"eyJoZWFkZ | | XIiOnsiQWNjZXB0IjpbIiovKiJdLCJDb250ZW50 | | LUxlbmd0aCI6WyIzOCJdLCJDb250ZW50LVR5cGU | | iOlsiYXBwbGljYXRpb24vanNvbiJdLCJVc2VyLU | | FnZW50IjpbImN1cmwvNy41NC4wIl19LCJib2R5I | | jp7Im1lc3NhZ2UiOiJ0aGlzIGlzIG15IGZpcnN0 | \\ IHdlYmhvb2sifX0=\" } / ----------------------------------------- \\ \\ \\ ## . ## ## ## == ## ## ## ## === / \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" ___ / === ~~~ { ~~ ~~~~ ~~~ ~~~~ ~~ ~ / ===- ~~~ \\ ______ o __ / \\ \\ __ / \\ ____ \\ ______ / Note: You will see the message printed in the workflow logs contains both the event context and data, with data being base64 encoded. In later sections, we will see how to extract particular key-value from event context or data and pass it to the workflow as arguments.","title":"Get Started"},{"location":"tutorials/01-introduction/#troubleshoot","text":"If you don't see the gateway and sensor pod in argo-events namespace, Make sure the correct Role and RoleBindings are applied to the service account and there are no errors in both gateway and sensor controller. Make sure gateway and sensor controller configmap has namespace set to argo-events . Check the logs of gateway and sensor controller. Make sure the controllers have processed the gateway and sensor objects and there are no errors. Look for any error in gateway or sensor pod. Inspect the gateway, kubectl - n argo - event gateway - object - name - o yaml Inspect the sensor, kubectl - n argo - events sensor - object - name - o yaml and look for any errors within the Status . Raise an issue on GitHub or post a question on argo-events slack channel.","title":"Troubleshoot"},{"location":"tutorials/02-parameterization/","text":"Parameterization \u00b6 In previous section, we saw how to set up a basic webhook gateway and sensor, and trigger an Argo workflow. The trigger template had parameters set in the sensor obejct and the workflow was able to print the event payload. In this tutorial, we will dig deeper into different types of parameterization, how to extract particular key-value from event payload and how to use default values if certain key is not available within event payload. Trigger Resource Parameterization \u00b6 If you take a closer look at the Sensor object, you will notice it contains a list of triggers. Each Trigger contains the template that defines the context of the trigger and actual resource that we expect the sensor to execute. In previous section, the resource within the trigger template was an Argo workflow. This subsection deals with how to parameterize the resource within trigger template with the event payload. Prerequisites \u00b6 Make sure to have the basic webhook gateway and sensor set up. Follow the introduction tutorial if haven't done already. Webhook Event Payload \u00b6 Webhook gateway consumes events through HTTP requests and transforms them into CloudEvents. The structure of the event the Webhook sensor receives from the gateway looks like following, { \"context\" : { \"type\" : \"type_of_gateway\" , \"specVersion\" : \"cloud_events_version\" , \"source\" : \"name_of_the_gateway\" , \"eventID\" : \"unique_event_id\" , \"time\" : \"event_time\" , \"dataContentType\" : \"type_of_data\" , \"subject\" : \"name_of_the_event_within_event_source\" } , \"data\" : { \"header\" : {} , \"body\" : {} , } } Context : This is the CloudEvent context and it is populated by the gateway regardless of type of HTTP request. Data : Data contains following fields, Header : The header within event data contains the headers in the HTTP request that was dispatched to the gateway. The gateway extracts the headers from the request and put it in the the header within event data . Body : This is the request payload from the HTTP request. Event Context \u00b6 Now that we have an understanding of the structure of the event the webhook sensor receives from the gateway, lets see how we can use the event context to parameterize the Argo workflow. Update the Webhook Sensor and add the contextKey for the parameter at index 0. kubectl - n argo - events apply - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / tutorials / 02 - parameterization / sensor - 01 . yaml Send a HTTP request to the gateway. curl - d '{\"message\":\"this is my first webhook\"}' - H \"Content-Type: application/json\" - X POST http : // localhost : 12000 / example Inspect the output of the Argo workflow that was created. argo logs name_of_the_workflow You will see the following output, _________ < webhook > --------- \\ \\ \\ ## . ## ## ## == ## ## ## ## === / \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" ___ / === ~~~ { ~~ ~~~~ ~~~ ~~~~ ~~ ~ / ===- ~~~ \\ ______ o __ / \\ \\ __ / \\ ____ \\ ______ / We have successfully extracted the type key within the event context and parameterized the workflow to print the value of the type . Event Data \u00b6 Now, it is time to use the event data and parameterize the Argo workflow trigger. We will extract the message from request payload and get the Argo workflow to print the message. Update the Webhook Sensor and add the dataKey in the parameter at index 0. kubectl - n argo - events apply - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / tutorials / 02 - parameterization / sensor - 02 . yaml Send a HTTP request to the gateway. curl - d '{\"message\":\"this is my first webhook\"}' - H \"Content-Type: application/json\" - X POST http : // localhost : 12000 / example Inspect the output of the Argo workflow that was created. argo logs name_of_the_workflow You will see the following output, __________________________ < this is my first webhook > -------------------------- \\ \\ \\ ## . ## ## ## == ## ## ## ## === / \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" ___ / === ~~~ { ~~ ~~~~ ~~~ ~~~~ ~~ ~ / ===- ~~~ \\ ______ o __ / \\ \\ __ / \\ ____ \\ ______ / Yay!! The Argo workflow printed the message. You can add however many number of parameters to update the trigger resource on the fly. Note : If you define both the contextKey and dataKey within a parameter, then the dataKey takes the precedence. Default Values \u00b6 Each parameter comes with an option to configure the default value. This is specially important when the key you defined in the parameter doesn't exist in the event. Update the Webhook Sensor and add the value for the parameter at index 0. We will also update the dataKey to an unknown event key. kubectl - n argo - events apply - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / tutorials / 02 - parameterization / sensor - 03 . yaml Send a HTTP request to the gateway. curl - d '{\"message\":\"this is my first webhook\"}' - H \"Content-Type: application/json\" - X POST http : // localhost : 12000 / example Inspect the output of the Argo workflow that was created. argo logs name_of_the_workflow You will see the following output, _______________________ < wow ! a default value . > ----------------------- \\ \\ \\ ## . ## ## ## == ## ## ## ## === / \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" ___ / === ~~~ { ~~ ~~~~ ~~~ ~~~~ ~~ ~ / ===- ~~~ \\ ______ o __ / \\ \\ __ / \\ ____ \\ ______ / Operations \u00b6 Sometimes you need the ability to append or prepend a parameter value to an existing value in trigger resource. This is where the operation field within a parameter comes handy. Update the Webhook Sensor and add the operation in the parameter at index 0. We will prepend the message to an existing value. kubectl - n argo - events apply - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / tutorials / 02 - parameterization / sensor - 04 . yaml Send a HTTP request to the gateway. curl - d '{\"message\":\"hey!!\"}' - H \"Content-Type: application/json\" - X POST http : // localhost : 12000 / example Inspect the output of the Argo workflow that was created. argo logs name_of_the_workflow You will see the following output, __________________ < hey !! hello world > ------------------ \\ \\ \\ ## . ## ## ## == ## ## ## ## === / \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" ___ / === ~~~ { ~~ ~~~~ ~~~ ~~~~ ~~ ~ / ===- ~~~ \\ ______ o __ / \\ \\ __ / \\ ____ \\ ______ / Trigger Template Parameterization \u00b6 The parameterization you saw above deals with the trigger resource, but sometimes you need to parameterize the trigger template itself. This comes handy when you have the trigger resource stored on some external source like S3, Git, etc. and you need to replace the url of the source on the fly in trigger template. Imagine a scenario where you want to parameterize the parameters of trigger to parameterize the trigger resource. What?... The sensor you have been using in this tutorial has one parameter defined in the trigger resource under k8s . We will parameterize that parameter by applying a parameter at the trigger template level. Update the Webhook Sensor and add parameters at trigger level. kubectl - n argo - events apply - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / tutorials / 02 - parameterization / sensor - 05 . yaml Send a HTTP request to the gateway. curl - d '{\"dependencyName\":\"test-dep\", \"dataKey\": \"body.message\", \"dest\": \"spec.arguments.parameters.0.value\", \"message\": \"amazing!!\"}' - H \"Content-Type: application/json\" - X POST http : // localhost : 12000 / example Inspect the output of the Argo workflow that was created. argo logs name_of_the_workflow You will see the following output, ___________ < amazing !! > ----------- \\ \\ \\ ## . ## ## ## == ## ## ## ## === / \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" ___ / === ~~~ { ~~ ~~~~ ~~~ ~~~~ ~~ ~ / ===- ~~~ \\ ______ o __ / \\ \\ __ / \\ ____ \\ ______ / Great!! You have now learned how to apply parameters at trigger resource and template level. Keep in mind that you can apply default values and operations like prepend and append for trigger template parameters as well.","title":"Parameterization"},{"location":"tutorials/02-parameterization/#parameterization","text":"In previous section, we saw how to set up a basic webhook gateway and sensor, and trigger an Argo workflow. The trigger template had parameters set in the sensor obejct and the workflow was able to print the event payload. In this tutorial, we will dig deeper into different types of parameterization, how to extract particular key-value from event payload and how to use default values if certain key is not available within event payload.","title":"Parameterization"},{"location":"tutorials/02-parameterization/#trigger-resource-parameterization","text":"If you take a closer look at the Sensor object, you will notice it contains a list of triggers. Each Trigger contains the template that defines the context of the trigger and actual resource that we expect the sensor to execute. In previous section, the resource within the trigger template was an Argo workflow. This subsection deals with how to parameterize the resource within trigger template with the event payload.","title":"Trigger Resource Parameterization"},{"location":"tutorials/02-parameterization/#prerequisites","text":"Make sure to have the basic webhook gateway and sensor set up. Follow the introduction tutorial if haven't done already.","title":"Prerequisites"},{"location":"tutorials/02-parameterization/#webhook-event-payload","text":"Webhook gateway consumes events through HTTP requests and transforms them into CloudEvents. The structure of the event the Webhook sensor receives from the gateway looks like following, { \"context\" : { \"type\" : \"type_of_gateway\" , \"specVersion\" : \"cloud_events_version\" , \"source\" : \"name_of_the_gateway\" , \"eventID\" : \"unique_event_id\" , \"time\" : \"event_time\" , \"dataContentType\" : \"type_of_data\" , \"subject\" : \"name_of_the_event_within_event_source\" } , \"data\" : { \"header\" : {} , \"body\" : {} , } } Context : This is the CloudEvent context and it is populated by the gateway regardless of type of HTTP request. Data : Data contains following fields, Header : The header within event data contains the headers in the HTTP request that was dispatched to the gateway. The gateway extracts the headers from the request and put it in the the header within event data . Body : This is the request payload from the HTTP request.","title":"Webhook Event Payload"},{"location":"tutorials/02-parameterization/#event-context","text":"Now that we have an understanding of the structure of the event the webhook sensor receives from the gateway, lets see how we can use the event context to parameterize the Argo workflow. Update the Webhook Sensor and add the contextKey for the parameter at index 0. kubectl - n argo - events apply - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / tutorials / 02 - parameterization / sensor - 01 . yaml Send a HTTP request to the gateway. curl - d '{\"message\":\"this is my first webhook\"}' - H \"Content-Type: application/json\" - X POST http : // localhost : 12000 / example Inspect the output of the Argo workflow that was created. argo logs name_of_the_workflow You will see the following output, _________ < webhook > --------- \\ \\ \\ ## . ## ## ## == ## ## ## ## === / \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" ___ / === ~~~ { ~~ ~~~~ ~~~ ~~~~ ~~ ~ / ===- ~~~ \\ ______ o __ / \\ \\ __ / \\ ____ \\ ______ / We have successfully extracted the type key within the event context and parameterized the workflow to print the value of the type .","title":"Event Context"},{"location":"tutorials/02-parameterization/#event-data","text":"Now, it is time to use the event data and parameterize the Argo workflow trigger. We will extract the message from request payload and get the Argo workflow to print the message. Update the Webhook Sensor and add the dataKey in the parameter at index 0. kubectl - n argo - events apply - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / tutorials / 02 - parameterization / sensor - 02 . yaml Send a HTTP request to the gateway. curl - d '{\"message\":\"this is my first webhook\"}' - H \"Content-Type: application/json\" - X POST http : // localhost : 12000 / example Inspect the output of the Argo workflow that was created. argo logs name_of_the_workflow You will see the following output, __________________________ < this is my first webhook > -------------------------- \\ \\ \\ ## . ## ## ## == ## ## ## ## === / \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" ___ / === ~~~ { ~~ ~~~~ ~~~ ~~~~ ~~ ~ / ===- ~~~ \\ ______ o __ / \\ \\ __ / \\ ____ \\ ______ / Yay!! The Argo workflow printed the message. You can add however many number of parameters to update the trigger resource on the fly. Note : If you define both the contextKey and dataKey within a parameter, then the dataKey takes the precedence.","title":"Event Data"},{"location":"tutorials/02-parameterization/#default-values","text":"Each parameter comes with an option to configure the default value. This is specially important when the key you defined in the parameter doesn't exist in the event. Update the Webhook Sensor and add the value for the parameter at index 0. We will also update the dataKey to an unknown event key. kubectl - n argo - events apply - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / tutorials / 02 - parameterization / sensor - 03 . yaml Send a HTTP request to the gateway. curl - d '{\"message\":\"this is my first webhook\"}' - H \"Content-Type: application/json\" - X POST http : // localhost : 12000 / example Inspect the output of the Argo workflow that was created. argo logs name_of_the_workflow You will see the following output, _______________________ < wow ! a default value . > ----------------------- \\ \\ \\ ## . ## ## ## == ## ## ## ## === / \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" ___ / === ~~~ { ~~ ~~~~ ~~~ ~~~~ ~~ ~ / ===- ~~~ \\ ______ o __ / \\ \\ __ / \\ ____ \\ ______ /","title":"Default Values"},{"location":"tutorials/02-parameterization/#operations","text":"Sometimes you need the ability to append or prepend a parameter value to an existing value in trigger resource. This is where the operation field within a parameter comes handy. Update the Webhook Sensor and add the operation in the parameter at index 0. We will prepend the message to an existing value. kubectl - n argo - events apply - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / tutorials / 02 - parameterization / sensor - 04 . yaml Send a HTTP request to the gateway. curl - d '{\"message\":\"hey!!\"}' - H \"Content-Type: application/json\" - X POST http : // localhost : 12000 / example Inspect the output of the Argo workflow that was created. argo logs name_of_the_workflow You will see the following output, __________________ < hey !! hello world > ------------------ \\ \\ \\ ## . ## ## ## == ## ## ## ## === / \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" ___ / === ~~~ { ~~ ~~~~ ~~~ ~~~~ ~~ ~ / ===- ~~~ \\ ______ o __ / \\ \\ __ / \\ ____ \\ ______ /","title":"Operations"},{"location":"tutorials/02-parameterization/#trigger-template-parameterization","text":"The parameterization you saw above deals with the trigger resource, but sometimes you need to parameterize the trigger template itself. This comes handy when you have the trigger resource stored on some external source like S3, Git, etc. and you need to replace the url of the source on the fly in trigger template. Imagine a scenario where you want to parameterize the parameters of trigger to parameterize the trigger resource. What?... The sensor you have been using in this tutorial has one parameter defined in the trigger resource under k8s . We will parameterize that parameter by applying a parameter at the trigger template level. Update the Webhook Sensor and add parameters at trigger level. kubectl - n argo - events apply - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / tutorials / 02 - parameterization / sensor - 05 . yaml Send a HTTP request to the gateway. curl - d '{\"dependencyName\":\"test-dep\", \"dataKey\": \"body.message\", \"dest\": \"spec.arguments.parameters.0.value\", \"message\": \"amazing!!\"}' - H \"Content-Type: application/json\" - X POST http : // localhost : 12000 / example Inspect the output of the Argo workflow that was created. argo logs name_of_the_workflow You will see the following output, ___________ < amazing !! > ----------- \\ \\ \\ ## . ## ## ## == ## ## ## ## === / \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" ___ / === ~~~ { ~~ ~~~~ ~~~ ~~~~ ~~ ~ / ===- ~~~ \\ ______ o __ / \\ \\ __ / \\ ____ \\ ______ / Great!! You have now learned how to apply parameters at trigger resource and template level. Keep in mind that you can apply default values and operations like prepend and append for trigger template parameters as well.","title":"Trigger Template Parameterization"},{"location":"tutorials/03-trigger-sources/","text":"Trigger Sources \u00b6 A trigger source is the source of trigger resource. It can be either external source such as Git , S3 , K8s Configmap , File , any valid URL that hosts the resource or an internal resource which is defined in the sensor object itself like Inline or Resource . In the previous sections, you have been dealing with the Resource trigger source. In this tutorial, we will explore other trigger sources. Prerequisites \u00b6 The Webhook gateway is already set up. Git \u00b6 Git trigger source refers to K8s trigger refers to the K8s resource stored in Git. The specification for the Git source is available here . In order to fetch data from git, you need to set up the private SSH key in sensor. If you don't have ssh keys available, create them following this guide Create a K8s secret that holds the SSH keys kubectl - n argo - events create secret generic git - ssh --from-file=key=.ssh/<YOUR_SSH_KEY_FILE_NAME> Create a K8s secret that holds known hosts. kubectl - n argo - events create secret generic git - known - hosts --from-file=ssh_known_hosts=.ssh/known_hosts Create a sensor with the git trigger source and refer it to the hello world workflow stored on the Argo Git project kubectl - n argo - events apply - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / tutorials / 03 - trigger - sources / sensor - git . yaml Use either Curl or Postman to send a post request to the http://localhost:12000/example curl - d '{\"message\":\"ok\"}' - H \"Content-Type: application/json\" - X POST http : // localhost : 12000 / example Now, you should see an Argo workflow being created. kubectl - n argo - events get wf S3 \u00b6 You can refer to the K8s resource stored on S3 complaint store as the trigger source. For this tutorial, lets set up a minio server which is S3 compliant store. Create a K8s secret called artifacts-minio that holds your minio access key and secret key. The access key must be stored under accesskey key and secret key must be stored under secretkey . Follow steps described here to set up the minio server. Make sure a service is available to expose the minio server. Create a bucket called workflows and store a basic hello world Argo workflow with key name hello-world.yaml . Create the sensor with trigger source as S3. kubectl - n argo - events apply - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / tutorials / 03 - trigger - sources / sensor - minio . yaml Use either Curl or Postman to send a post request to the http://localhost:12000/example curl - d '{\"message\":\"ok\"}' - H \"Content-Type: application/json\" - X POST http : // localhost : 12000 / example Now, you should see an Argo workflow being created. kubectl - n argo - events get wf K8s Configmap \u00b6 K8s configmap can be treated as trigger source if needed. Lets create a configmap called trigger-store . kubectl - n argo - events apply - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / tutorials / 03 - trigger - sources / trigger - store . yaml Create a sensor with trigger source as configmap and refer it to the trigger-store . kubectl - n argo - events apply - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / tutorials / 03 - trigger - sources / sensor - cm . yaml Use either Curl or Postman to send a post request to the http://localhost:12000/example curl - d '{\"message\":\"ok\"}' - H \"Content-Type: application/json\" - X POST http : // localhost : 12000 / example Now, you should see an Argo workflow being created. kubectl - n argo - events get wf File & URL \u00b6 File and URL trigger sources are pretty self explanatory. The example sensors are available under examples/sensors folder.","title":"Trigger Sources"},{"location":"tutorials/03-trigger-sources/#trigger-sources","text":"A trigger source is the source of trigger resource. It can be either external source such as Git , S3 , K8s Configmap , File , any valid URL that hosts the resource or an internal resource which is defined in the sensor object itself like Inline or Resource . In the previous sections, you have been dealing with the Resource trigger source. In this tutorial, we will explore other trigger sources.","title":"Trigger Sources"},{"location":"tutorials/03-trigger-sources/#prerequisites","text":"The Webhook gateway is already set up.","title":"Prerequisites"},{"location":"tutorials/03-trigger-sources/#git","text":"Git trigger source refers to K8s trigger refers to the K8s resource stored in Git. The specification for the Git source is available here . In order to fetch data from git, you need to set up the private SSH key in sensor. If you don't have ssh keys available, create them following this guide Create a K8s secret that holds the SSH keys kubectl - n argo - events create secret generic git - ssh --from-file=key=.ssh/<YOUR_SSH_KEY_FILE_NAME> Create a K8s secret that holds known hosts. kubectl - n argo - events create secret generic git - known - hosts --from-file=ssh_known_hosts=.ssh/known_hosts Create a sensor with the git trigger source and refer it to the hello world workflow stored on the Argo Git project kubectl - n argo - events apply - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / tutorials / 03 - trigger - sources / sensor - git . yaml Use either Curl or Postman to send a post request to the http://localhost:12000/example curl - d '{\"message\":\"ok\"}' - H \"Content-Type: application/json\" - X POST http : // localhost : 12000 / example Now, you should see an Argo workflow being created. kubectl - n argo - events get wf","title":"Git"},{"location":"tutorials/03-trigger-sources/#s3","text":"You can refer to the K8s resource stored on S3 complaint store as the trigger source. For this tutorial, lets set up a minio server which is S3 compliant store. Create a K8s secret called artifacts-minio that holds your minio access key and secret key. The access key must be stored under accesskey key and secret key must be stored under secretkey . Follow steps described here to set up the minio server. Make sure a service is available to expose the minio server. Create a bucket called workflows and store a basic hello world Argo workflow with key name hello-world.yaml . Create the sensor with trigger source as S3. kubectl - n argo - events apply - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / tutorials / 03 - trigger - sources / sensor - minio . yaml Use either Curl or Postman to send a post request to the http://localhost:12000/example curl - d '{\"message\":\"ok\"}' - H \"Content-Type: application/json\" - X POST http : // localhost : 12000 / example Now, you should see an Argo workflow being created. kubectl - n argo - events get wf","title":"S3"},{"location":"tutorials/03-trigger-sources/#k8s-configmap","text":"K8s configmap can be treated as trigger source if needed. Lets create a configmap called trigger-store . kubectl - n argo - events apply - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / tutorials / 03 - trigger - sources / trigger - store . yaml Create a sensor with trigger source as configmap and refer it to the trigger-store . kubectl - n argo - events apply - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / tutorials / 03 - trigger - sources / sensor - cm . yaml Use either Curl or Postman to send a post request to the http://localhost:12000/example curl - d '{\"message\":\"ok\"}' - H \"Content-Type: application/json\" - X POST http : // localhost : 12000 / example Now, you should see an Argo workflow being created. kubectl - n argo - events get wf","title":"K8s Configmap"},{"location":"tutorials/03-trigger-sources/#file-url","text":"File and URL trigger sources are pretty self explanatory. The example sensors are available under examples/sensors folder.","title":"File &amp; URL"},{"location":"tutorials/04-standard-k8s-resources/","text":"Standard K8s Resources \u00b6 In the previous sections, you saw how to trigger the Argo workflows. In this tutorial, you will see how to trigger Pod and Deployment. Similarly you can trigger any standard Kubernetes resources. Having the ability to trigger standard Kubernetes resources is quite powerful as provides an avenue to set up pipelines for existing workloads. Prerequisites \u00b6 Make sure that argo-events-sa service account has necessary permissions to create the Kubernetes resource of your choice. The Webhook gateway is already set up. Pod \u00b6 Create a sensor with K8s trigger. Pay close attention to the group , version and kind keys within the trigger resource. These keys determine the type of kubernetes object. You will notice that the group key is empty, that means we want to use core group. For any other groups, you need to specify the group key. kubectl - n argo - events apply - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / tutorials / 03 - standard - k8s - resources / sensor - pod . yaml Use either Curl or Postman to send a post request to the http://localhost:12000/example curl - d '{\"message\":\"ok\"}' - H \"Content-Type: application/json\" - X POST http : // localhost : 12000 / example Now, you should see a pod being created. kubectl - n argo - events get po Output _________________________________________ / { \"context\" : { \"type\" : \"webhook\" , \"specVersi \\ | on\" : \"0.3\" , \"source\" : \"webhook-gateway\" , \"e | | ventID\" : \"30306463666539362d346666642d34 | | 3336332d383861312d336538363333613564313 | | 932\" , \"time\" : \"2020-01-11T21:23:07.682961 | | Z\" , \"dataContentType\" : \"application/json\" | | , \"subject\" : \"example\" } , \"data\" : \"eyJoZWFkZ | | XIiOnsiQWNjZXB0IjpbIiovKiJdLCJDb250ZW50 | | LUxlbmd0aCI6WyIxOSJdLCJDb250ZW50LVR5cGU | | iOlsiYXBwbGljYXRpb24vanNvbiJdLCJVc2VyLU | | FnZW50IjpbImN1cmwvNy41NC4wIl19LCJib2R5I | \\ jp7Im1lc3NhZ2UiOiJoZXkhISJ9fQ==\" } / ----------------------------------------- \\ \\ \\ ## . ## ## ## == ## ## ## ## === / \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" ___ / === ~~~ { ~~ ~~~~ ~~~ ~~~~ ~~ ~ / ===- ~~~ \\ ______ o __ / \\ \\ __ / \\ ____ \\ ______ / Deployment \u00b6 Lets create a sensor with a K8s deployment as trigger. kubectl - n argo - events apply - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / tutorials / 03 - standard - k8s - resources / sensor - deployment . yaml Use either Curl or Postman to send a post request to the http://localhost:12000/example curl - d '{\"message\":\"ok\"}' - H \"Content-Type: application/json\" - X POST http : // localhost : 12000 / example Now, you should see a deployment being created. Get the corresponding pod. kubectl - n argo - events get deployments Output _________________________________________ / { \"context\" : { \"type\" : \"webhook\" , \"specVersi \\ | on\" : \"0.3\" , \"source\" : \"webhook-gateway\" , \"e | | ventID\" : \"30306463666539362d346666642d34 | | 3336332d383861312d336538363333613564313 | | 932\" , \"time\" : \"2020-01-11T21:23:07.682961 | | Z\" , \"dataContentType\" : \"application/json\" | | , \"subject\" : \"example\" } , \"data\" : \"eyJoZWFkZ | | XIiOnsiQWNjZXB0IjpbIiovKiJdLCJDb250ZW50 | | LUxlbmd0aCI6WyIxOSJdLCJDb250ZW50LVR5cGU | | iOlsiYXBwbGljYXRpb24vanNvbiJdLCJVc2VyLU | | FnZW50IjpbImN1cmwvNy41NC4wIl19LCJib2R5I | \\ jp7Im1lc3NhZ2UiOiJoZXkhISJ9fQ==\" } / ----------------------------------------- \\ \\ \\ ## . ## ## ## == ## ## ## ## === / \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" ___ / === ~~~ { ~~ ~~~~ ~~~ ~~~~ ~~ ~ / ===- ~~~ \\ ______ o __ / \\ \\ __ / \\ ____ \\ ______ /","title":"Standard K8s Resources"},{"location":"tutorials/04-standard-k8s-resources/#standard-k8s-resources","text":"In the previous sections, you saw how to trigger the Argo workflows. In this tutorial, you will see how to trigger Pod and Deployment. Similarly you can trigger any standard Kubernetes resources. Having the ability to trigger standard Kubernetes resources is quite powerful as provides an avenue to set up pipelines for existing workloads.","title":"Standard K8s Resources"},{"location":"tutorials/04-standard-k8s-resources/#prerequisites","text":"Make sure that argo-events-sa service account has necessary permissions to create the Kubernetes resource of your choice. The Webhook gateway is already set up.","title":"Prerequisites"},{"location":"tutorials/04-standard-k8s-resources/#pod","text":"Create a sensor with K8s trigger. Pay close attention to the group , version and kind keys within the trigger resource. These keys determine the type of kubernetes object. You will notice that the group key is empty, that means we want to use core group. For any other groups, you need to specify the group key. kubectl - n argo - events apply - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / tutorials / 03 - standard - k8s - resources / sensor - pod . yaml Use either Curl or Postman to send a post request to the http://localhost:12000/example curl - d '{\"message\":\"ok\"}' - H \"Content-Type: application/json\" - X POST http : // localhost : 12000 / example Now, you should see a pod being created. kubectl - n argo - events get po Output _________________________________________ / { \"context\" : { \"type\" : \"webhook\" , \"specVersi \\ | on\" : \"0.3\" , \"source\" : \"webhook-gateway\" , \"e | | ventID\" : \"30306463666539362d346666642d34 | | 3336332d383861312d336538363333613564313 | | 932\" , \"time\" : \"2020-01-11T21:23:07.682961 | | Z\" , \"dataContentType\" : \"application/json\" | | , \"subject\" : \"example\" } , \"data\" : \"eyJoZWFkZ | | XIiOnsiQWNjZXB0IjpbIiovKiJdLCJDb250ZW50 | | LUxlbmd0aCI6WyIxOSJdLCJDb250ZW50LVR5cGU | | iOlsiYXBwbGljYXRpb24vanNvbiJdLCJVc2VyLU | | FnZW50IjpbImN1cmwvNy41NC4wIl19LCJib2R5I | \\ jp7Im1lc3NhZ2UiOiJoZXkhISJ9fQ==\" } / ----------------------------------------- \\ \\ \\ ## . ## ## ## == ## ## ## ## === / \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" ___ / === ~~~ { ~~ ~~~~ ~~~ ~~~~ ~~ ~ / ===- ~~~ \\ ______ o __ / \\ \\ __ / \\ ____ \\ ______ /","title":"Pod"},{"location":"tutorials/04-standard-k8s-resources/#deployment","text":"Lets create a sensor with a K8s deployment as trigger. kubectl - n argo - events apply - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / tutorials / 03 - standard - k8s - resources / sensor - deployment . yaml Use either Curl or Postman to send a post request to the http://localhost:12000/example curl - d '{\"message\":\"ok\"}' - H \"Content-Type: application/json\" - X POST http : // localhost : 12000 / example Now, you should see a deployment being created. Get the corresponding pod. kubectl - n argo - events get deployments Output _________________________________________ / { \"context\" : { \"type\" : \"webhook\" , \"specVersi \\ | on\" : \"0.3\" , \"source\" : \"webhook-gateway\" , \"e | | ventID\" : \"30306463666539362d346666642d34 | | 3336332d383861312d336538363333613564313 | | 932\" , \"time\" : \"2020-01-11T21:23:07.682961 | | Z\" , \"dataContentType\" : \"application/json\" | | , \"subject\" : \"example\" } , \"data\" : \"eyJoZWFkZ | | XIiOnsiQWNjZXB0IjpbIiovKiJdLCJDb250ZW50 | | LUxlbmd0aCI6WyIxOSJdLCJDb250ZW50LVR5cGU | | iOlsiYXBwbGljYXRpb24vanNvbiJdLCJVc2VyLU | | FnZW50IjpbImN1cmwvNy41NC4wIl19LCJib2R5I | \\ jp7Im1lc3NhZ2UiOiJoZXkhISJ9fQ==\" } / ----------------------------------------- \\ \\ \\ ## . ## ## ## == ## ## ## ## === / \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" ___ / === ~~~ { ~~ ~~~~ ~~~ ~~~~ ~~ ~ / ===- ~~~ \\ ______ o __ / \\ \\ __ / \\ ____ \\ ______ /","title":"Deployment"},{"location":"tutorials/05-trigger-custom-resources/","text":"Trigger Custom Resources \u00b6 Argo Events supports Argo workflows, standard K8s objects, Sensor and Gateway as K8s triggers. In order to add your custom resource as trigger to Argo Events, you will need to register the scheme of resource in Argo Events. If you feel Argo Events should support your custom resource out of box, create an issue on GitHub and provide the details. Steps to build sensor image with your CR \u00b6 Fork the Argo Events project. Go to store.go in store package. Import your custom resource api package. In init method, add the scheme to your custom resource api. Make sure there are no errors. Rebuild the sensor binary using make sensor To build the image, first change IMAGE_NAMESPACE in Makefile to your docker registry and then run make sensor-image.","title":"Trigger Custom Resources"},{"location":"tutorials/05-trigger-custom-resources/#trigger-custom-resources","text":"Argo Events supports Argo workflows, standard K8s objects, Sensor and Gateway as K8s triggers. In order to add your custom resource as trigger to Argo Events, you will need to register the scheme of resource in Argo Events. If you feel Argo Events should support your custom resource out of box, create an issue on GitHub and provide the details.","title":"Trigger Custom Resources"},{"location":"tutorials/05-trigger-custom-resources/#steps-to-build-sensor-image-with-your-cr","text":"Fork the Argo Events project. Go to store.go in store package. Import your custom resource api package. In init method, add the scheme to your custom resource api. Make sure there are no errors. Rebuild the sensor binary using make sensor To build the image, first change IMAGE_NAMESPACE in Makefile to your docker registry and then run make sensor-image.","title":"Steps to build sensor image with your CR"},{"location":"tutorials/06-circuit-and-switch/","text":"Circuit and Switch \u00b6 In previous sections, you have been dealing with just a single dependency. But in many cases, you want to wait for multiple events to occur and then trigger a resource which means you need a mechanism to determine which triggers to execute based on set of different event dependencies. This mechanism is supported through Circuit and Switch . Note : Whenever you define multiple dependencies in a sensor, the sensor applies a AND operation, meaning, it will wait for all dependencies to resolve before it executes triggers. Circuit and Switch can modify that behavior. Prerequisite \u00b6 Minio server must be set up in the argo-events namespace with a bucket called test and it should be available at minio-service.argo-events:9000 . Circuit \u00b6 A circuit is a boolean expression. To create a circuit, you just need to define event dependencies in groups and the sensor will apply the circuit logic on those groups. If the logic results in true value, the sensor will execute the triggers else it won't. Switch \u00b6 A switch is the conditional execution gate for a trigger. Consider a scenario where you have a Webhook and Minio gateway, and you want to trigger an Argo workflow if the sensor receives an event from the Webhook gateway, but, another workflow if it receives an event from the Minio gateway. Create the webhook event source and gateway. The gateway listens to HTTP requests on port 12000 kubectl - n argo - events apply - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / tutorials / 05 - circuit - and - switches / webhook - event - source . yaml kubectl - n argo - events apply - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / tutorials / 05 - circuit - and - switches / webhook - gateway . yaml Create the minio event source and gateway. The gateway listens to events of type PUT and DELETE for objects in bucket test . kubectl - n argo - events apply - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / tutorials / 05 - circuit - and - switches / minio - event - source . yaml kubectl - n argo - events apply - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / tutorials / 05 - circuit - and - switches / minio - gateway . yaml Make sure there are no errors in any of the gateways and all event sources are active. Lets create the sensor. If you take a closer look at the trigger templates, you will notice that it contains switch key with all condition, meaning, execute this trigger when every group defined in all is resolved. In the sensor definition, there is only one group under all in both trigger templates. So, as soon as the group is resolved, the corresponding trigger will be executed. kubectl - n argo - events apply - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / tutorials / 06 - circuit - and - switches / sensor - 01 . yaml Send a HTTP request to Webhook gateway, curl - d '{\"message\":\"this is my first webhook\"}' - H \"Content-Type: application/json\" - X POST http : // localhost : 12000 / example You will notice an Argo worklfow with name group-1-xxxx is created with following output, __________________________ < this is my first webhook > -------------------------- \\ \\ \\ ## . ## ## ## == ## ## ## ## === / \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" ___ / === ~~~ { ~~ ~~~~ ~~~ ~~~~ ~~ ~ / ===- ~~~ \\ ______ o __ / \\ \\ __ / \\ ____ \\ ______ / Now, lets generate a Minio event so that we can run group-2-xxxx workflow. Drop a file onto test bucket. The workflow that will get created will print the name of the bucket as follow, ______ < test > ------ \\ \\ \\ ## . ## ## ## == ## ## ## ## === / \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" ___ / === ~~~ { ~~ ~~~~ ~~~ ~~~~ ~~ ~ / ===- ~~~ \\ ______ o __ / \\ \\ __ / \\ ____ \\ ______ / Great!! You have now learned how to use a circuit and switch . Lets update the sensor with a trigger that waits for both groups to resolve. This is the normal sensor behavior if circuit is not defined. kubectl - n argo - events apply - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / tutorials / 06 - circuit - and - switches / sensor - 02 . yaml Send a HTTP request and perform a file drop on Minio bucket as done above. You should following output, _______________________________ < this is my first webhook test > ------------------------------- \\ \\ \\ ## . ## ## ## == ## ## ## ## === / \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" ___ / === ~~~ { ~~ ~~~~ ~~~ ~~~~ ~~ ~ / ===- ~~~ \\ ______ o __ / \\ \\ __ / \\ ____ \\ ______ /","title":"Circuit and Switch"},{"location":"tutorials/06-circuit-and-switch/#circuit-and-switch","text":"In previous sections, you have been dealing with just a single dependency. But in many cases, you want to wait for multiple events to occur and then trigger a resource which means you need a mechanism to determine which triggers to execute based on set of different event dependencies. This mechanism is supported through Circuit and Switch . Note : Whenever you define multiple dependencies in a sensor, the sensor applies a AND operation, meaning, it will wait for all dependencies to resolve before it executes triggers. Circuit and Switch can modify that behavior.","title":"Circuit and Switch"},{"location":"tutorials/06-circuit-and-switch/#prerequisite","text":"Minio server must be set up in the argo-events namespace with a bucket called test and it should be available at minio-service.argo-events:9000 .","title":"Prerequisite"},{"location":"tutorials/06-circuit-and-switch/#circuit","text":"A circuit is a boolean expression. To create a circuit, you just need to define event dependencies in groups and the sensor will apply the circuit logic on those groups. If the logic results in true value, the sensor will execute the triggers else it won't.","title":"Circuit"},{"location":"tutorials/06-circuit-and-switch/#switch","text":"A switch is the conditional execution gate for a trigger. Consider a scenario where you have a Webhook and Minio gateway, and you want to trigger an Argo workflow if the sensor receives an event from the Webhook gateway, but, another workflow if it receives an event from the Minio gateway. Create the webhook event source and gateway. The gateway listens to HTTP requests on port 12000 kubectl - n argo - events apply - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / tutorials / 05 - circuit - and - switches / webhook - event - source . yaml kubectl - n argo - events apply - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / tutorials / 05 - circuit - and - switches / webhook - gateway . yaml Create the minio event source and gateway. The gateway listens to events of type PUT and DELETE for objects in bucket test . kubectl - n argo - events apply - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / tutorials / 05 - circuit - and - switches / minio - event - source . yaml kubectl - n argo - events apply - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / tutorials / 05 - circuit - and - switches / minio - gateway . yaml Make sure there are no errors in any of the gateways and all event sources are active. Lets create the sensor. If you take a closer look at the trigger templates, you will notice that it contains switch key with all condition, meaning, execute this trigger when every group defined in all is resolved. In the sensor definition, there is only one group under all in both trigger templates. So, as soon as the group is resolved, the corresponding trigger will be executed. kubectl - n argo - events apply - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / tutorials / 06 - circuit - and - switches / sensor - 01 . yaml Send a HTTP request to Webhook gateway, curl - d '{\"message\":\"this is my first webhook\"}' - H \"Content-Type: application/json\" - X POST http : // localhost : 12000 / example You will notice an Argo worklfow with name group-1-xxxx is created with following output, __________________________ < this is my first webhook > -------------------------- \\ \\ \\ ## . ## ## ## == ## ## ## ## === / \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" ___ / === ~~~ { ~~ ~~~~ ~~~ ~~~~ ~~ ~ / ===- ~~~ \\ ______ o __ / \\ \\ __ / \\ ____ \\ ______ / Now, lets generate a Minio event so that we can run group-2-xxxx workflow. Drop a file onto test bucket. The workflow that will get created will print the name of the bucket as follow, ______ < test > ------ \\ \\ \\ ## . ## ## ## == ## ## ## ## === / \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" ___ / === ~~~ { ~~ ~~~~ ~~~ ~~~~ ~~ ~ / ===- ~~~ \\ ______ o __ / \\ \\ __ / \\ ____ \\ ______ / Great!! You have now learned how to use a circuit and switch . Lets update the sensor with a trigger that waits for both groups to resolve. This is the normal sensor behavior if circuit is not defined. kubectl - n argo - events apply - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / tutorials / 06 - circuit - and - switches / sensor - 02 . yaml Send a HTTP request and perform a file drop on Minio bucket as done above. You should following output, _______________________________ < this is my first webhook test > ------------------------------- \\ \\ \\ ## . ## ## ## == ## ## ## ## === / \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" ___ / === ~~~ { ~~ ~~~~ ~~~ ~~~~ ~~ ~ / ===- ~~~ \\ ______ o __ / \\ \\ __ / \\ ____ \\ ______ /","title":"Switch"},{"location":"tutorials/07-filters/","text":"Filters \u00b6 In previous sections, you have seen how to trigger an Argo workflow based on events. In this tutorial, you will learn how to apply filters on event data and context. Filters provide a powerful mechanism to apply constraints on the events in order to determine a validity. Argo Events offers 3 types of filters: Data Filter Context Filter Time Filter Prerequisite \u00b6 Webhook gateway must be set up. Data Filter \u00b6 Data filter as the name suggests are applied on the event data. A CloudEvent from Webhook gateway has payload structure as, { \"context\" : { \"type\" : \"type_of_gateway\" , \"specVersion\" : \"cloud_events_version\" , \"source\" : \"name_of_the_gateway\" , \"eventID\" : \"unique_event_id\" , \"time\" : \"event_time\" , \"dataContentType\" : \"type_of_data\" , \"subject\" : \"name_of_the_event_within_event_source\" } , \"data\" : { \"header\" : {} , \"body\" : {} , } } Data Filter are applied on data within the payload. We will make a simple HTTP request to webhook gateway with request data as {\"message\":\"this is my first webhook\"} and apply data filter on message . A data filter has following fields, data : - path : path_within_event_data type : types_of_the_data value : - list_of_possible_values Note : If data type is a string , then you can pass either an exact value or a regex. If data types is bool or float, then you need to pass the exact value. Lets create a webhook sensor with data filter. kubectl - n argo - events apply - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / tutorials / 07 - filters / sensor - data - filter . yaml Send a HTTP request to gateway curl - d '{\"message\":\"this is my first webhook\"}' - H \"Content-Type: application/json\" - X POST http : // localhost : 12000 / example You will notice that the sensor logs prints the event is invalid as the sensor expects for either hello or hey as the value of body.message . Send a valid HTTP request to gateway curl - d '{\"message\":\"hello\"}' - H \"Content-Type: application/json\" - X POST http : // localhost : 12000 / example Watch for a workflow with name data-workflow-xxxx . Context Filter \u00b6 Similar to the data filter, you can apply a filter on the context of the event. Change the subscriber in the webhook gateway to point it to context-filter sensor's URL. Lets create a webhook sensor with context filter. kubectl - n argo - events apply - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / tutorials / 07 - filters / sensor - context - filter . yaml Send a HTTP request to gateway curl - d '{\"message\":\"this is my first webhook\"}' - H \"Content-Type: application/json\" - X POST http : // localhost : 12000 / example You will notice that the sensor logs prints the event is invalid as the sensor expects for either custom-webhook as the value of the source . Time Filter \u00b6 Time filter is specially helpful when you need to make sure an event occurs between a certain time-frame. Time filter takes a start and stop time but you can also define just the start time, meaning, there is no end time constraint or just the stop time, meaning, there is no start time constraint. An example of time filter is available under examples/sensors .","title":"Filters"},{"location":"tutorials/07-filters/#filters","text":"In previous sections, you have seen how to trigger an Argo workflow based on events. In this tutorial, you will learn how to apply filters on event data and context. Filters provide a powerful mechanism to apply constraints on the events in order to determine a validity. Argo Events offers 3 types of filters: Data Filter Context Filter Time Filter","title":"Filters"},{"location":"tutorials/07-filters/#prerequisite","text":"Webhook gateway must be set up.","title":"Prerequisite"},{"location":"tutorials/07-filters/#data-filter","text":"Data filter as the name suggests are applied on the event data. A CloudEvent from Webhook gateway has payload structure as, { \"context\" : { \"type\" : \"type_of_gateway\" , \"specVersion\" : \"cloud_events_version\" , \"source\" : \"name_of_the_gateway\" , \"eventID\" : \"unique_event_id\" , \"time\" : \"event_time\" , \"dataContentType\" : \"type_of_data\" , \"subject\" : \"name_of_the_event_within_event_source\" } , \"data\" : { \"header\" : {} , \"body\" : {} , } } Data Filter are applied on data within the payload. We will make a simple HTTP request to webhook gateway with request data as {\"message\":\"this is my first webhook\"} and apply data filter on message . A data filter has following fields, data : - path : path_within_event_data type : types_of_the_data value : - list_of_possible_values Note : If data type is a string , then you can pass either an exact value or a regex. If data types is bool or float, then you need to pass the exact value. Lets create a webhook sensor with data filter. kubectl - n argo - events apply - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / tutorials / 07 - filters / sensor - data - filter . yaml Send a HTTP request to gateway curl - d '{\"message\":\"this is my first webhook\"}' - H \"Content-Type: application/json\" - X POST http : // localhost : 12000 / example You will notice that the sensor logs prints the event is invalid as the sensor expects for either hello or hey as the value of body.message . Send a valid HTTP request to gateway curl - d '{\"message\":\"hello\"}' - H \"Content-Type: application/json\" - X POST http : // localhost : 12000 / example Watch for a workflow with name data-workflow-xxxx .","title":"Data Filter"},{"location":"tutorials/07-filters/#context-filter","text":"Similar to the data filter, you can apply a filter on the context of the event. Change the subscriber in the webhook gateway to point it to context-filter sensor's URL. Lets create a webhook sensor with context filter. kubectl - n argo - events apply - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / tutorials / 07 - filters / sensor - context - filter . yaml Send a HTTP request to gateway curl - d '{\"message\":\"this is my first webhook\"}' - H \"Content-Type: application/json\" - X POST http : // localhost : 12000 / example You will notice that the sensor logs prints the event is invalid as the sensor expects for either custom-webhook as the value of the source .","title":"Context Filter"},{"location":"tutorials/07-filters/#time-filter","text":"Time filter is specially helpful when you need to make sure an event occurs between a certain time-frame. Time filter takes a start and stop time but you can also define just the start time, meaning, there is no end time constraint or just the stop time, meaning, there is no start time constraint. An example of time filter is available under examples/sensors .","title":"Time Filter"},{"location":"tutorials/08-policy/","text":"Policy \u00b6 A policy for a trigger determines whether the trigger resulted in success or failure. Currently, Argo Events supports 2 types of policies: Policy based on the K8s resource labels. Policy based on the response status for triggers like HTTP request, AWS Lambda, etc. Resource Labels Policy \u00b6 This type of policy determines whether trigger completed successfully based on the labels set on the trigger resource. Consider a sensor which has an Argo workflow as the trigger. When an Argo workflow completes successfully, the workflow controller sets a label on the resource as workflows.argoproj.io/completed: 'true' . So, in order for sensor to determine whether the trigger workflow completed successfully, you just need to set the policy labels as workflows.argoproj.io/completed: 'true' under trigger template. In addition to labels, you can also define a backoff and option to error out if sensor is unable to determine status of the trigger after the backoff completes. Check out the specification of resource labels policy here . Status Policy \u00b6 For triggers like HTTP request or AWS Lambda, you can apply the Status Policy to determine the trigger status. The Status Policy supports list of expected response statuses. If the status of the HTTP request or Lamda is within the statuses defined in the policy, then the trigger is considered successful. Complete specification is available here .","title":"Policy"},{"location":"tutorials/08-policy/#policy","text":"A policy for a trigger determines whether the trigger resulted in success or failure. Currently, Argo Events supports 2 types of policies: Policy based on the K8s resource labels. Policy based on the response status for triggers like HTTP request, AWS Lambda, etc.","title":"Policy"},{"location":"tutorials/08-policy/#resource-labels-policy","text":"This type of policy determines whether trigger completed successfully based on the labels set on the trigger resource. Consider a sensor which has an Argo workflow as the trigger. When an Argo workflow completes successfully, the workflow controller sets a label on the resource as workflows.argoproj.io/completed: 'true' . So, in order for sensor to determine whether the trigger workflow completed successfully, you just need to set the policy labels as workflows.argoproj.io/completed: 'true' under trigger template. In addition to labels, you can also define a backoff and option to error out if sensor is unable to determine status of the trigger after the backoff completes. Check out the specification of resource labels policy here .","title":"Resource Labels Policy"},{"location":"tutorials/08-policy/#status-policy","text":"For triggers like HTTP request or AWS Lambda, you can apply the Status Policy to determine the trigger status. The Status Policy supports list of expected response statuses. If the status of the HTTP request or Lamda is within the statuses defined in the policy, then the trigger is considered successful. Complete specification is available here .","title":"Status Policy"},{"location":"tutorials/09-http-trigger/","text":"HTTP Trigger \u00b6 Sometimes you face a situation where creating an Argo workflow on every event is not an ideal solution. This is where the HTTP trigger can help you. With this type of trigger, you can connect any old/new API server with 20+ event sources supported by Argo Events or invoke serveless fucntions without worrying about their respective event connector frameworks. Prerequisite \u00b6 Set up the Minio gateway and event source. The K8s manifests are available under examples/tutorials/09-http-trigger . API Server Integration \u00b6 We will set up a basic go http server and connect it with the minio events. Set up a simple http server that prints the request payload. kubectl - n argo - events apply - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / tutorials / 09 - http - trigger / http - server . yaml Create a service to expose the http server kubectl - n argo - events apply - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / tutorials / 09 - http - trigger / http - server - svc . yaml Either use Ingress, OpenShift Route or port-forwarding to expose the http server. We will use port-forwarding here. Create a sensor with HTTP trigger. We will discuss the trigger details in the following sections. kubectl - n argo - events apply - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / tutorials / 09 - http - trigger / sensor - 01 . yaml Now, drop a file onto test bucket in Minio server. The sensor has triggered a http request to out basic go http server. Take a look at the logs server is listening on 8090 { \"type\" : \"minio\" , \"bucket\" : \"test\" } Great!!! But how did the sensor constructed a payload for the http request? We will see that in next section. Note : HTTP trigger must have a payload, otherwise it is pretty useless to send a request without event data. Payload Construction \u00b6 The http trigger payload has the following structure, payload : - src : dependencyName : test - dep dataKey : s3 . bucket . name dest : bucket - src : dependencyName : test - dep contextKey : type dest : type This looks very similar to the parameter structure you have seen in previous sections for trigger parameterization. The src is the source of event. It contains, dependencyName : name of the event dependency to extract the event from. dataKey : to extract a particular key-value from event's data. contextKey : to extract a particular key-value from event' context. The dest is the destination key within the result payload. So, the above trigger payload will generate a request payload as, { \"bucket\" : \"value_of_the_bucket_name_extracted_from_event_data\" , \"type\" : \"value_of_the_event_type_extracted_from_event_context\" } Note : If you define both the contextKey and dataKey within a payload item, then the dataKey takes the precedence. You can create any payload structure you want. To get more info on how to generate complex event payloads, take a look at this library . The complete specification of HTTP trigger is available here . Serverless Workload Integration \u00b6 HTTP trigger provides an easy integration into 20+ event sources for the existing serverless workloads. In this section, we will look at how to invoke a Kubeless function. Prerequisite \u00b6 Kubeless must be installed :). You can follow the installation here . Make sure to deploy the test function. Invoke Function \u00b6 First, lets create an http trigger for kubeless function, kubeless trigger http create hello --function-name hello Update the sensor and update the serverURL to point to your Kubeless function URL. Drop a file onto test bucket. You will see the hello function getting invoked with following output, { 'event-time' : None , 'extensions' : { 'request' : < LocalRequest : POST http : //< URL > : 8080 /> } , 'event-type' : None , 'event-namespace' : None , 'data' : '{\"type\":\"minio\",\"bucket\":\"test\"}' , 'event-id' : None } Note: The output was taken from Kubeless deployed on GCP. Policy \u00b6 To determine whether the request was successful or not, HTTP trigger provides a Status policy. The Status holds a list of response statuses that are considered valid. Lets update the sensor with acceptable response statuses as [ 200 , 300 ] and point the http trigger to an invalid server url. kubectl - n argo - events apply - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / tutorials / 09 - http - trigger / sensor - 02 . yaml Drop a file onto test bucket. Inspect the sensor logs, you will see the trigger resulted in failure. ERRO [ 2020 - 01 - 13 21 : 27 : 04 ] failed to operate on the event notification error = \"policy application resulted in failure. http response status 404 is not allowed\" Parameterization \u00b6 You can either use parameters within http trigger or parameter under the template to parameterize the trigger on fly.","title":"HTTP Trigger"},{"location":"tutorials/09-http-trigger/#http-trigger","text":"Sometimes you face a situation where creating an Argo workflow on every event is not an ideal solution. This is where the HTTP trigger can help you. With this type of trigger, you can connect any old/new API server with 20+ event sources supported by Argo Events or invoke serveless fucntions without worrying about their respective event connector frameworks.","title":"HTTP Trigger"},{"location":"tutorials/09-http-trigger/#prerequisite","text":"Set up the Minio gateway and event source. The K8s manifests are available under examples/tutorials/09-http-trigger .","title":"Prerequisite"},{"location":"tutorials/09-http-trigger/#api-server-integration","text":"We will set up a basic go http server and connect it with the minio events. Set up a simple http server that prints the request payload. kubectl - n argo - events apply - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / tutorials / 09 - http - trigger / http - server . yaml Create a service to expose the http server kubectl - n argo - events apply - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / tutorials / 09 - http - trigger / http - server - svc . yaml Either use Ingress, OpenShift Route or port-forwarding to expose the http server. We will use port-forwarding here. Create a sensor with HTTP trigger. We will discuss the trigger details in the following sections. kubectl - n argo - events apply - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / tutorials / 09 - http - trigger / sensor - 01 . yaml Now, drop a file onto test bucket in Minio server. The sensor has triggered a http request to out basic go http server. Take a look at the logs server is listening on 8090 { \"type\" : \"minio\" , \"bucket\" : \"test\" } Great!!! But how did the sensor constructed a payload for the http request? We will see that in next section. Note : HTTP trigger must have a payload, otherwise it is pretty useless to send a request without event data.","title":"API Server Integration"},{"location":"tutorials/09-http-trigger/#payload-construction","text":"The http trigger payload has the following structure, payload : - src : dependencyName : test - dep dataKey : s3 . bucket . name dest : bucket - src : dependencyName : test - dep contextKey : type dest : type This looks very similar to the parameter structure you have seen in previous sections for trigger parameterization. The src is the source of event. It contains, dependencyName : name of the event dependency to extract the event from. dataKey : to extract a particular key-value from event's data. contextKey : to extract a particular key-value from event' context. The dest is the destination key within the result payload. So, the above trigger payload will generate a request payload as, { \"bucket\" : \"value_of_the_bucket_name_extracted_from_event_data\" , \"type\" : \"value_of_the_event_type_extracted_from_event_context\" } Note : If you define both the contextKey and dataKey within a payload item, then the dataKey takes the precedence. You can create any payload structure you want. To get more info on how to generate complex event payloads, take a look at this library . The complete specification of HTTP trigger is available here .","title":"Payload Construction"},{"location":"tutorials/09-http-trigger/#serverless-workload-integration","text":"HTTP trigger provides an easy integration into 20+ event sources for the existing serverless workloads. In this section, we will look at how to invoke a Kubeless function.","title":"Serverless Workload Integration"},{"location":"tutorials/09-http-trigger/#prerequisite_1","text":"Kubeless must be installed :). You can follow the installation here . Make sure to deploy the test function.","title":"Prerequisite"},{"location":"tutorials/09-http-trigger/#invoke-function","text":"First, lets create an http trigger for kubeless function, kubeless trigger http create hello --function-name hello Update the sensor and update the serverURL to point to your Kubeless function URL. Drop a file onto test bucket. You will see the hello function getting invoked with following output, { 'event-time' : None , 'extensions' : { 'request' : < LocalRequest : POST http : //< URL > : 8080 /> } , 'event-type' : None , 'event-namespace' : None , 'data' : '{\"type\":\"minio\",\"bucket\":\"test\"}' , 'event-id' : None } Note: The output was taken from Kubeless deployed on GCP.","title":"Invoke Function"},{"location":"tutorials/09-http-trigger/#policy","text":"To determine whether the request was successful or not, HTTP trigger provides a Status policy. The Status holds a list of response statuses that are considered valid. Lets update the sensor with acceptable response statuses as [ 200 , 300 ] and point the http trigger to an invalid server url. kubectl - n argo - events apply - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / tutorials / 09 - http - trigger / sensor - 02 . yaml Drop a file onto test bucket. Inspect the sensor logs, you will see the trigger resulted in failure. ERRO [ 2020 - 01 - 13 21 : 27 : 04 ] failed to operate on the event notification error = \"policy application resulted in failure. http response status 404 is not allowed\"","title":"Policy"},{"location":"tutorials/09-http-trigger/#parameterization","text":"You can either use parameters within http trigger or parameter under the template to parameterize the trigger on fly.","title":"Parameterization"},{"location":"tutorials/10-aws-lambda-and-openfaas-trigger/","text":"AWS Lambda Trigger \u00b6 AWS Lambda provides a tremendous value but the event driven lambda invocation is limited to SNS, SQS and few other event sources. Argo Events makes it easy to integrate lambda with event sources that are not native to AWS. Prerequisite \u00b6 Set up the webhook gateway and event source. Create a K8s secret that holds your access and secret key. Make sure to store the base64 encoded keys in the secret. Create a basic lambda function that can parse following payload, { \"name\" : \"foo\" } Lambda Trigger \u00b6 The trigger specification is available here Note : You must declare the payload for the lambda trigger. Check out the HTTP trigger tutorial to understand how to construct the payload. Lets create a sensor with a lambda trigger kubectl - n argo - events apply - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / tutorials / 10 - aws - lambda - trigger / sensor . yaml Send a http request to webhook gateway, curl - d '{\"name\":\"foo\"}' - H \"Content-Type: application/json\" - X POST http : // localhost : 12000 / example You should see the lambda execution in CloudWatch logs. Policy \u00b6 To determine whether the function was successful or not, Lambda trigger provides a Status policy. The Status holds a list of response statuses that are considered valid. Parameterization \u00b6 Similar to HTTP trigger, the Lambda trigger provides parameters at both trigger resource and trigger template level. OpenFaas Trigger \u00b6 Similar to AWS lambda, you can trigger a OpenFaas function. The trigger specification is available here","title":"AWS Lambda Trigger"},{"location":"tutorials/10-aws-lambda-and-openfaas-trigger/#aws-lambda-trigger","text":"AWS Lambda provides a tremendous value but the event driven lambda invocation is limited to SNS, SQS and few other event sources. Argo Events makes it easy to integrate lambda with event sources that are not native to AWS.","title":"AWS Lambda Trigger"},{"location":"tutorials/10-aws-lambda-and-openfaas-trigger/#prerequisite","text":"Set up the webhook gateway and event source. Create a K8s secret that holds your access and secret key. Make sure to store the base64 encoded keys in the secret. Create a basic lambda function that can parse following payload, { \"name\" : \"foo\" }","title":"Prerequisite"},{"location":"tutorials/10-aws-lambda-and-openfaas-trigger/#lambda-trigger","text":"The trigger specification is available here Note : You must declare the payload for the lambda trigger. Check out the HTTP trigger tutorial to understand how to construct the payload. Lets create a sensor with a lambda trigger kubectl - n argo - events apply - f https : // raw . githubusercontent . com / argoproj / argo - events / master / examples / tutorials / 10 - aws - lambda - trigger / sensor . yaml Send a http request to webhook gateway, curl - d '{\"name\":\"foo\"}' - H \"Content-Type: application/json\" - X POST http : // localhost : 12000 / example You should see the lambda execution in CloudWatch logs.","title":"Lambda Trigger"},{"location":"tutorials/10-aws-lambda-and-openfaas-trigger/#policy","text":"To determine whether the function was successful or not, Lambda trigger provides a Status policy. The Status holds a list of response statuses that are considered valid.","title":"Policy"},{"location":"tutorials/10-aws-lambda-and-openfaas-trigger/#parameterization","text":"Similar to HTTP trigger, the Lambda trigger provides parameters at both trigger resource and trigger template level.","title":"Parameterization"},{"location":"tutorials/10-aws-lambda-and-openfaas-trigger/#openfaas-trigger","text":"Similar to AWS lambda, you can trigger a OpenFaas function. The trigger specification is available here","title":"OpenFaas Trigger"},{"location":"tutorials/11-special-argo-workflow-trigger/","text":"Special Argo Workflow Trigger \u00b6 Although you can trigger an Argo workflow using a standard K8s trigger, the functionality provided by argo cli can't be leveraged in a standard K8s trigger. The special argo workflow trigger supports following operations for a workflow, Submit Resubmit Resume Retry Suspend The trigger specification is available here and the example is located under examples/sensors .","title":"Special Argo Workflow Trigger"},{"location":"tutorials/11-special-argo-workflow-trigger/#special-argo-workflow-trigger","text":"Although you can trigger an Argo workflow using a standard K8s trigger, the functionality provided by argo cli can't be leveraged in a standard K8s trigger. The special argo workflow trigger supports following operations for a workflow, Submit Resubmit Resume Retry Suspend The trigger specification is available here and the example is located under examples/sensors .","title":"Special Argo Workflow Trigger"}]}